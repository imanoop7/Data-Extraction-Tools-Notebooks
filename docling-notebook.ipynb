{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\Data-Extraction-Tools-Notebooks\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]d:\\GitHub\\Data-Extraction-Tools-Notebooks\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\anoop\\.cache\\huggingface\\hub\\models--ds4sd--docling-models. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 9 files: 100%|██████████| 9/9 [00:01<00:00,  8.56it/s]\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete## An Introduction to Convolutional Neural Networks\n",
      "\n",
      "Keiron O'Shea 1 and Ryan Nash 2\n",
      "\n",
      "- 1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB keo7@aber.ac.uk\n",
      "- 2 School of Computing and Communications, Lancaster University, Lancashire, LA1 4YW\n",
      "\n",
      "nashrd@live.lancs.ac.uk\n",
      "\n",
      "Abstract. The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs.\n",
      "\n",
      "This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.\n",
      "\n",
      "Keywords: Pattern recognition, artificial neural networks, machine learning, image analysis\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Artificial Neural Networks (ANNs) are computational processing systems of which are heavily inspired by way biological nervous systems (such as the human brain) operate. ANNs are mainly comprised of a high number of interconnected computational nodes (referred to as neurons), of which work entwine in a distributed fashion to collectively learn from the input in order to optimise its final output.\n",
      "\n",
      "The basic structure of a ANN can be modelled as shown in Figure 1. We would load the input, usually in the form of a multidimensional vector to the input layer of which will distribute it to the hidden layers. The hidden layers will then make decisions from the previous layer and weigh up how a stochastic change within itself detriments or improves the final output, and this is referred to as the process of learning. Having multiple hidden layers stacked upon each-other is commonly called deep learning.\n",
      "\n",
      "Fig. 1: A simple three layered feedforward neural network (FNN), comprised of a input layer, a hidden layer and an output layer. This structure is the basis of a number of common ANN architectures, included but not limited to Feedforward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and Recurrent Neural Networks (RNNs).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "The two key learning paradigms in image processing tasks are supervised and unsupervised learning. Supervised learning is learning through pre-labelled inputs, which act as targets. For each training example there will be a set of input values (vectors) and one or more associated designated output values. The goal of this form of training is to reduce the models overall classification error, through correct calculation of the output value of training example by training.\n",
      "\n",
      "Unsupervised learning differs in that the training set does not include any labels. Success is usually determined by whether the network is able to reduce or increase an associated cost function. However, it is important to note that most image-focused pattern-recognition tasks usually depend on classification using supervised learning.\n",
      "\n",
      "Convolutional Neural Networks (CNNs) are analogous to traditional ANNs in that they are comprised of neurons that self-optimise through learning. Each neuron will still receive an input and perform a operation (such as a scalar product followed by a non-linear function) - the basis of countless ANNs. From the input raw image vectors to the final output of the class score, the entire of the network will still express a single perceptive score function (the weight). The last layer will contain loss functions associated with the classes, and all of the regular tips and tricks developed for traditional ANNs still apply.\n",
      "\n",
      "The only notable difference between CNNs and traditional ANNs is that CNNs are primarily used in the field of pattern recognition within images. This allows us to encode image-specific features into the architecture, making the network\n",
      "\n",
      "more suited for image-focused tasks - whilst further reducing the parameters required to set up the model.\n",
      "\n",
      "One of the largest limitations of traditional forms of ANN is that they tend to struggle with the computational complexity required to compute image data. Common machine learning benchmarking datasets such as the MNIST database of handwritten digits are suitable for most forms of ANN, due to its relatively small image dimensionality of just 28 × 28 . With this dataset a single neuron in the first hidden layer will contain 784 weights ( 28 × 28 × 1 where 1 bare in mind that MNIST is normalised to just black and white values), which is manageable for most forms of ANN.\n",
      "\n",
      "If you consider a more substantial coloured image input of 64 × 64 , the number of weights on just a single neuron of the first layer increases substantially to 12 , 288 . Also take into account that to deal with this scale of input, the network will also need to be a lot larger than one used to classify colour-normalised MNIST digits, then you will understand the drawbacks of using such models.\n",
      "\n",
      "## 1.1 Overfitting\n",
      "\n",
      "But why does it matter? Surely we could just increase the number of hidden layers in our network, and perhaps increase the number of neurons within them? The simple answer to this question is no. This is down to two reasons, one being the simple problem of not having unlimited computational power and time to train these huge ANNs.\n",
      "\n",
      "The second reason is stopping or reducing the effects of overfitting. Overfitting is basically when a network is unable to learn effectively due to a number of reasons. It is an important concept of most, if not all machine learning algorithms and it is important that every precaution is taken as to reduce its effects. If our models were to exhibit signs of overfitting then we may see a reduced ability to pinpoint generalised features for not only our training dataset, but also our test and prediction sets.\n",
      "\n",
      "This is the main reason behind reducing the complexity of our ANNs. The less parameters required to train, the less likely the network will overfit - and of course, improve the predictive performance of the model.\n",
      "\n",
      "## 2 CNN architecture\n",
      "\n",
      "As noted earlier, CNNs primarily focus on the basis that the input will be comprised of images. This focuses the architecture to be set up in way to best suit the need for dealing with the specific type of data.\n",
      "\n",
      "Keiron O'Shea et al.\n",
      "\n",
      "One of the key differences is that the neurons that the layers within the CNN are comprised of neurons organised into three dimensions, the spatial dimensionality of the input ( height and the width ) and the depth . The depth does not refer to the total number of layers within the ANN, but the third dimension of a activation volume. Unlike standard ANNS, the neurons within any given layer will only connect to a small region of the layer preceding it.\n",
      "\n",
      "In practice this would mean that for the example given earlier, the input 'volume' will have a dimensionality of 64 × 64 × 3 (height, width and depth), leading to a final output layer comprised of a dimensionality of 1 × 1 × n (where n represents the possible number of classes) as we would have condensed the full input dimensionality into a smaller volume of class scores filed across the depth dimension.\n",
      "\n",
      "## 2.1 Overall architecture\n",
      "\n",
      "CNNs are comprised of three types of layers. These are convolutional layers, pooling layers and fully-connected layers . When these layers are stacked, a CNN architecture has been formed. A simplified CNN architecture for MNIST classification is illustrated in Figure 2.\n",
      "\n",
      "Fig. 2: An simple CNN architecture, comprised of just five layers\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "The basic functionality of the example CNN above can be broken down into four key areas.\n",
      "\n",
      "- 1. As found in other forms of ANN, the input layer will hold the pixel values of the image.\n",
      "- 2. The convolutional layer will determine the output of neurons of which are connected to local regions of the input through the calculation of the scalar product between their weights and the region connected to the input volume. The rectified linear unit (commonly shortened to ReLu) aims to apply\n",
      "\n",
      "- an 'elementwise' activation function such as sigmoid to the output of the activation produced by the previous layer.\n",
      "- 3. The pooling layer will then simply perform downsampling along the spatial dimensionality of the given input, further reducing the number of parameters within that activation.\n",
      "- 4. The fully-connected layers will then perform the same duties found in standard ANNs and attempt to produce class scores from the activations, to be used for classification. It is also suggested that ReLu may be used between these layers, as to improve performance.\n",
      "\n",
      "Through this simple method of transformation, CNNs are able to transform the original input layer by layer using convolutional and downsampling techniques to produce class scores for classification and regression purposes.\n",
      "\n",
      "Fig. 3: Activations taken from the first convolutional layer of a simplistic deep CNN, after training on the MNIST database of handwritten digits. If you look carefully, you can see that the network has successfully picked up on characteristics unique to specific numeric digits.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "However, it is important to note that simply understanding the overall architecture of a CNN architecture will not suffice. The creation and optimisation of these models can take quite some time, and can be quite confusing. We will now explore in detail the individual layers, detailing their hyperparameters and connectivities.\n",
      "\n",
      "## 2.2 Convolutional layer\n",
      "\n",
      "As the name implies, the convolutional layer plays a vital role in how CNNs operate. The layers parameters focus around the use of learnable kernels .\n",
      "\n",
      "These kernels are usually small in spatial dimensionality, but spreads along the entirety of the depth of the input. When the data hits a convolutional layer, the layer convolves each filter across the spatial dimensionality of the input to produce a 2D activation map. These activation maps can be visualised, as seen in Figure 3.\n",
      "\n",
      "As we glide through the input, the scalar product is calculated for each value in that kernel. (Figure 4) From this the network will learn kernels that 'fire' when they see a specific feature at a given spatial position of the input. These are commonly known as activations .\n",
      "\n",
      "Fig. 4: A visual representation of a convolutional layer. The centre element of the kernel is placed over the input vector, of which is then calculated and replaced with a weighted sum of itself and any nearby pixels.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Every kernel will have a corresponding activation map, of which will be stacked along the depth dimension to form the full output volume from the convolutional layer.\n",
      "\n",
      "As we alluded to earlier, training ANNs on inputs such as images results in models of which are too big to train effectively. This comes down to the fullyconnected manner of standard ANN neurons, so to mitigate against this every neuron in a convolutional layer is only connected to small region of the input volume. The dimensionality of this region is commonly referred to as the receptive field size of the neuron. The magnitude of the connectivity through the depth is nearly always equal to the depth of the input.\n",
      "\n",
      "For example, if the input to the network is an image of size 64 × 64 × 3 (a RGBcoloured image with a dimensionality of 64 × 64 ) and we set the receptive field size as 6 × 6 , we would have a total of 108 weights on each neuron within the convolutional layer. ( 6 × 6 × 3 where 3 is the magnitude of connectivity across the depth of the volume) To put this into perspective, a standard neuron seen in other forms of ANN would contain 12 , 288 weights each.\n",
      "\n",
      "Convolutional layers are also able to significantly reduce the complexity of the model through the optimisation of its output. These are optimised through three hyperparameters, the depth , the stride and setting zero-padding .\n",
      "\n",
      "The depth of the output volume produced by the convolutional layers can be manually set through the number of neurons within the layer to a the same region of the input. This can be seen with other forms of ANNs, where the all of the neurons in the hidden layer are directly connected to every single neuron beforehand. Reducing this hyperparameter can significantly minimise the total number of neurons of the network, but it can also significantly reduce the pattern recognition capabilities of the model.\n",
      "\n",
      "We are also able to define the stride in which we set the depth around the spatial dimensionality of the input in order to place the receptive field. For example if we were to set a stride as 1, then we would have a heavily overlapped receptive field producing extremely large activations. Alternatively, setting the stride to a greater number will reduce the amount of overlapping and produce an output of lower spatial dimensions.\n",
      "\n",
      "Zero-padding is the simple process of padding the border of the input, and is an effective method to give further control as to the dimensionality of the output volumes.\n",
      "\n",
      "It is important to understand that through using these techniques, we will alter the spatial dimensionality of the convolutional layers output. To calculate this, you can make use of the following formula:\n",
      "\n",
      "( V - R ) + 2 Z S + 1\n",
      "\n",
      "Where V represents the input volume size ( height × width × depth ), R represents the receptive field size, Z is the amount of zero padding set and S referring to the stride. If the calculated result from this equation is not equal to a whole integer then the stride has been incorrectly set, as the neurons will be unable to fit neatly across the given input.\n",
      "\n",
      "Despite our best efforts so far we will still find that our models are still enormous if we use an image input of any real dimensionality. However, methods have been developed as to greatly curtail the overall number of parameters within the convolutional layer.\n",
      "\n",
      "Parameter sharing works on the assumption that if one region feature is useful to compute at a set spatial region, then it is likely to be useful in another region. If we constrain each individual activation map within the output volume to the same weights and bias, then we will see a massive reduction in the number of parameters being produced by the convolutional layer.\n",
      "\n",
      "As a result of this as the backpropagation stage occurs, each neuron in the output will represent the overall gradient of which can be totalled across the depth - thus only updating a single set of weights, as opposed to every single one.\n",
      "\n",
      "## 2.3 Pooling layer\n",
      "\n",
      "Pooling layers aim to gradually reduce the dimensionality of the representation, and thus further reduce the number of parameters and the computational complexity of the model.\n",
      "\n",
      "The pooling layer operates over each activation map in the input, and scales its dimensionality using the \"MAX\" function. In most CNNs, these come in the form of max-pooling layers with kernels of a dimensionality of 2 × 2 applied with a stride of 2 along the spatial dimensions of the input. This scales the activation map down to 25% of the original size - whilst maintaining the depth volume to its standard size.\n",
      "\n",
      "Due to the destructive nature of the pooling layer, there are only two generally observed methods of max-pooling. Usually, the stride and filters of the pooling layers are both set to 2 × 2 , which will allow the layer to extend through the entirety of the spatial dimensionality of the input. Furthermore overlapping pooling may be utilised, where the stride is set to 2 with a kernel size set to 3 . Due to the destructive nature of pooling, having a kernel size above 3 will usually greatly decrease the performance of the model.\n",
      "\n",
      "It is also important to understand that beyond max-pooling, CNN architectures may contain general-pooling. General pooling layers are comprised of pooling neurons that are able to perform a multitude of common operations including L1/L2-normalisation, and average pooling. However, this tutorial will primarily focus on the use of max-pooling.\n",
      "\n",
      "## 2.4 Fully-connected layer\n",
      "\n",
      "The fully-connected layer contains neurons of which are directly connected to the neurons in the two adjacent layers, without being connected to any layers within them. This is analogous to way that neurons are arranged in traditional forms of ANN. (Figure 1)\n",
      "\n",
      "## 3 Recipes\n",
      "\n",
      "Despite the relatively small number of layers required to form a CNN, there is no set way of formulating a CNN architecture. That being said, it would be idiotic to simply throw a few of layers together and expect it to work. Through reading of related literature it is obvious that much like other forms of ANNs, CNNs tend to follow a common architecture. This common architecture is illustrated in Figure 2, where convolutional layers are stacked, followed by pooling layers in a repeated manner before feeding forward to fully-connected layers.\n",
      "\n",
      "Another common CNN architecture is to stack two convolutional layers before each pooling layer, as illustrated in Figure 5. This is strongly recommended as stacking multiple convolutional layers allows for more complex features of the input vector to be selected.\n",
      "\n",
      "Fig. 5: A common form of CNN architecture in which convolutional layers are stacked between ReLus continuously before being passed through the pooling layer, before going between one or many fully connected ReLus.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "It is also advised to split large convolutional layers up into many smaller sized convolutional layers. This is to reduce the amount of computational complexity within a given convolutional layer. For example, if you were to stack three convolutional layers on top of each other with a receptive field of 3 × 3 . Each neuron of the first convolutional layer will have a 3 × 3 view of the input vector. A neuron on the second convolutional layer will then have a 5 × 5 view of the input vector. A neuron on the third convolutional layer will then have a 7 × 7 view of the input vector. As these stacks feature non-linearities which in turn allows us to express stronger features of the input with fewer parameters. However, it is important to understand that this does come with a distinct memory allocation problem - especially when making use of the backpropagation algorithm.\n",
      "\n",
      "The input layer should be recursively divisible by two. Common numbers include 32 × 32 , 64 × 64 , 96 × 96 , 128 × 128 and 224 × 224 .\n",
      "\n",
      "Whilst using small filters, set stride to one and make use of zero-padding as to ensure that the convolutional layers do not reconfigure any of the dimensionality of the input. The amount of zero-padding to be used should be calculated by taking one away from the receptive field size and dividing by two.activation\n",
      "\n",
      "CNNs are extremely powerful machine learning algorithms, however they can be horrendously resource-heavy. An example of this problem could be in filtering a large image (anything over 128 × 128 could be considered large), so if the input is 227 × 227 (as seen with ImageNet) and we're filtering with 64 kernels each with a zero padding of then the result will be three activation vectors of size 227 × 227 × 64 - which calculates to roughly 10 million activations - or an enormous 70 megabytes of memory per image. In this case you have two options. Firstly, you can reduce the spatial dimensionality of the input images by\n",
      "\n",
      "resizing the raw images to something a little less heavy. Alternatively, you can go against everything we stated earlier in this document and opt for larger filter sizes with a larger stride (2, as opposed to 1).\n",
      "\n",
      "In addition to the few rules-of-thumb outlined above, it is also important to acknowledge a few 'tricks' about generalised ANN training techniques. The authors suggest a read of Geoffrey Hinton's excellent \"Practical Guide to Training Restricted Boltzmann Machines\".\n",
      "\n",
      "## 4 Conclusion\n",
      "\n",
      "Convolutional Neural Networks differ to other forms of Artifical Neural Network in that instead of focusing on the entirety of the problem domain, knowledge about the specific type of input is exploited. This in turn allows for a much simpler network architecture to be set up.\n",
      "\n",
      "This paper has outlined the basic concepts of Convolutional Neural Networks, explaining the layers required to build one and detailing how best to structure the network in most image analysis tasks.\n",
      "\n",
      "Research in the field of image analysis using neural networks has somewhat slowed in recent times. This is partly due to the incorrect belief surrounding the level of complexity and knowledge required to begin modelling these superbly powerful machine learning algorithms. The authors hope that this paper has in some way reduced this confusion, and made the field more accessible to beginners.\n",
      "\n",
      "## Acknowledgements\n",
      "\n",
      "The authors would like to thank Dr. Chuan Lu and Nicholas Dimonaco for useful discussion and suggestions.\n",
      "\n",
      "## References\n",
      "\n",
      "- 1. Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for image classification. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. pp. 3642-3649. IEEE (2012)\n",
      "- 2. Cires¸an, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in breast cancer histology images with deep neural networks. In: Medical Image Computing and Computer-Assisted Intervention-MICCAI 2013, pp. 411-418. Springer (2013)\n",
      "- 3. Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible, high performance convolutional neural networks for image classification. In: IJCAI Proceedings-International Joint Conference on Artificial Intelligence. vol. 22, p. 1237 (2011)\n",
      "\n",
      "- 4. Cires¸an, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural network committees for handwritten character classification. In: Document Analysis and Recognition (ICDAR), 2011 International Conference on. pp. 1135-1139. IEEE (2011)\n",
      "- 5. Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural networksa review. Pattern recognition 35(10), 2279-2301 (2002)\n",
      "- 6. Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware accelerated convolutional neural networks for synthetic vision systems. In: Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp. 257-260. IEEE (2010)\n",
      "- 7. Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum 9(1), 926 (2010)\n",
      "- 8. Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 (2012)\n",
      "- 9. Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1), 221-231 (2013)\n",
      "- 10. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Largescale video classification with convolutional neural networks. In: Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725-1732. IEEE (2014)\n",
      "- 11. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems. pp. 1097-1105 (2012)\n",
      "- 12. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural computation 1(4), 541-551 (1989)\n",
      "- 13. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998)\n",
      "- 14. Nebauer, C.: Evaluation of convolutional neural networks for visual recognition. Neural Networks, IEEE Transactions on 9(4), 685-696 (1998)\n",
      "- 15. Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural networks applied to visual document analysis. In: null. p. 958. IEEE (2003)\n",
      "- 16. Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of Toronto (2013)\n",
      "- 17. Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with convolutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings. IEEE. pp. 224-229. IEEE (2005)\n",
      "- 18. Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In: Advances in Neural Information Processing Systems. pp. 2553-2561 (2013)\n",
      "- 19. Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks (siconnets) and their application of face detection. In: Neural Networks, 2003. Proceedings of the International Joint Conference on. vol. 3, pp. 2157-2162. IEEE (2003)\n",
      "- 20. Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional neural networks. arXiv preprint arXiv:1301.3557 (2013)\n",
      "- 21. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In: Computer Vision-ECCV 2014, pp. 818-833. Springer (2014)\n"
     ]
    }
   ],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "source = \"https://arxiv.org/pdf/1511.08458\"  # document per local path or URL\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(source)\n",
    "print(result.document.export_to_markdown())  # output: \"## Docling Technical Report[...]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "\n",
    "# Initialize DocumentConverter for specific formats\n",
    "doc_converter = DocumentConverter(\n",
    "    allowed_formats=[InputFormat.PDF, InputFormat.IMAGE, InputFormat.DOCX]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_result = doc_converter.convert(\"1511.08458v2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input=InputDocument(file=WindowsPath('1511.08458v2.pdf'), document_hash='44e7ccf8647632980dfd327188b3f7c27bf959667a3430e04eee07405d74c48f', valid=True, limits=DocumentLimits(max_num_pages=9223372036854775807, max_file_size=9223372036854775807), format=<InputFormat.PDF: 'pdf'>, filesize=228188, page_count=11) status=<ConversionStatus.SUCCESS: '4'> errors=[] pages=[Page(page_no=0, size=Size(width=595.2760009765625, height=841.8900146484375), cells=[Cell(id=0, text='An Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=138.53101022726298, t=116.98291064591297, r=476.82874078224734, b=130.95611072306508, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=1, text='Keiron O’Shea', bbox=BoundingBox(l=236.21001038750742, t=161.63281089244435, r=299.9806504921245, b=171.565490947287, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=2, text='1', bbox=BoundingBox(l=299.981020492125, t=160.30120088509193, r=303.9526104986405, b=166.49395091928477, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='and Ryan Nash', bbox=BoundingBox(l=306.94101050354305, t=161.63281089244435, r=374.67673061466496, b=171.565490947287, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='2', bbox=BoundingBox(l=374.6760306146638, t=160.30120088509193, r=378.6476106211792, b=166.49395091928477, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='1', bbox=BoundingBox(l=141.3270302318499, t=181.6796210031315, r=144.97993023784258, b=186.9877310324398, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB', bbox=BoundingBox(l=149.96103024601416, t=183.22613101167042, r=474.033690777662, b=192.1655810610289, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='keo7@aber.ac.uk', bbox=BoundingBox(l=267.3290404385588, t=194.99108107662983, r=348.0266405709449, b=201.9131411148494, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='2', bbox=BoundingBox(l=139.82004022937767, t=203.5966811241451, r=143.47295023537032, b=208.9047811534533, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='School of Computing and Communications, Lancaster University, Lancashire, LA1', bbox=BoundingBox(l=148.45503024354352, t=205.14318113268382, r=475.54037078013374, b=214.0826411820425, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='4YW', bbox=BoundingBox(l=297.96301048881446, t=216.10217119319316, r=317.39319052069004, b=225.04162124255174, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='nashrd@live.lancs.ac.uk', bbox=BoundingBox(l=245.8100104032564, t=227.86810125815794, r=369.5463306062484, b=234.79016129637762, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='Abstract.', bbox=BoundingBox(l=163.11102026758698, t=254.98889140790345, r=199.71187032763143, b=263.72216145612356, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='The field of machine learning has taken a dramatic twist in re-', bbox=BoundingBox(l=204.6930203358031, t=254.926141407557, r=452.2463107419194, b=263.86560145691567, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='cent times, with the rise of the Artificial Neural Network (ANN). These', bbox=BoundingBox(l=163.11102026758698, t=265.88415146806096, r=452.2416707419117, b=274.82360151741955, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='biologically inspired computational models are able to far exceed the per-', bbox=BoundingBox(l=163.11102026758698, t=276.8431315285702, r=452.2416107419117, b=285.78259157792877, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='formance of previous forms of artificial intelligence in common machine', bbox=BoundingBox(l=163.11102026758698, t=287.80212158907943, r=452.2415207419116, b=296.7415716384379, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='learning tasks. One of the most impressive forms of ANN architecture is', bbox=BoundingBox(l=163.11102026758698, t=298.7611016495888, r=452.2415207419116, b=307.70056169894735, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='that of the Convolutional Neural Network (CNN). CNNs are primarily', bbox=BoundingBox(l=163.11102026758698, t=309.720091710098, r=452.2415807419116, b=318.6595417594566, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='used to solve difficult image-driven pattern recognition tasks and with', bbox=BoundingBox(l=163.11102026758698, t=320.67907177060727, r=452.2415207419116, b=329.6185318199658, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='their precise yet simple architecture, offers a simplified method of getting', bbox=BoundingBox(l=163.11102026758698, t=331.6380318311164, r=452.2415507419116, b=340.5775418804753, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='started with ANNs.', bbox=BoundingBox(l=163.11102026758698, t=342.5970118916257, r=241.19044039567788, b=351.5365219409846, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='This document provides a brief introduction to CNNs, discussing recently', bbox=BoundingBox(l=163.11102026758698, t=360.5400019906967, r=452.2414607419114, b=369.47952204005566, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='published papers and newly formed techniques in developing these bril-', bbox=BoundingBox(l=163.11102026758698, t=371.49899205120596, r=452.2415507419116, b=380.43850210056485, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='liantly fantastic image recognition models. This introduction assumes you', bbox=BoundingBox(l=163.11102026758698, t=382.45700211170987, r=452.2415507419116, b=391.3965121610688, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='are familiar with the fundamentals of ANNs and machine learning.', bbox=BoundingBox(l=163.11102026758698, t=393.41598217221906, r=429.83463070515256, b=402.355492221578, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='Keywords:', bbox=BoundingBox(l=163.11102026758698, t=422.3807623321461, r=207.19881033991393, b=431.1140423803664, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='Pattern recognition, artificial neural networks, machine learn-', bbox=BoundingBox(l=211.6810303472671, t=422.31799233179953, r=452.4108907421894, b=431.2575023811584, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='ing, image analysis', bbox=BoundingBox(l=163.11102026758698, t=433.27700239230904, r=238.93089039197108, b=442.21652244166796, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='1', bbox=BoundingBox(l=134.76501022108476, t=472.10070260667135, r=140.74261023089116, b=483.745082670965, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='Introduction', bbox=BoundingBox(l=152.6978102505039, t=472.10070260667135, r=221.76300036380678, b=483.745082670965, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='Artificial Neural Networks', bbox=BoundingBox(l=134.76501022108476, t=506.41946279616, r=257.86292042302944, b=516.1230428497377, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='(ANNs) are computational processing systems of', bbox=BoundingBox(l=261.0410204282432, t=506.349732795775, r=480.58688078841266, b=516.2824428506178, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='which are heavily inspired by way biological nervous systems (such as the hu-', bbox=BoundingBox(l=134.76501022108476, t=518.3047128617836, r=480.58670078841243, b=528.2374229166264, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='man brain) operate. ANNs are mainly comprised of a high number of intercon-', bbox=BoundingBox(l=134.76501022108476, t=530.2597029277922, r=480.58676078841245, b=540.192412982635, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='nected computational nodes (referred to as neurons), of which work entwine in', bbox=BoundingBox(l=134.76501022108476, t=542.2146929938009, r=480.58682078841264, b=552.1474030486437, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='a distributed fashion to collectively learn from the input in order to optimise its', bbox=BoundingBox(l=134.76501022108476, t=554.1696730598095, r=480.58679078841254, b=564.1023831146523, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='final output.', bbox=BoundingBox(l=134.76501022108476, t=566.1256731258238, r=189.3899503106981, b=576.0583831806665, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='The basic structure of a ANN can be modelled as shown in Figure 1. We would', bbox=BoundingBox(l=134.76501022108476, t=585.064663230394, r=480.5867307884124, b=594.9973732852368, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='load the input, usually in the form of a multidimensional vector to the input', bbox=BoundingBox(l=134.76501022108476, t=597.0196632964028, r=480.5867307884124, b=606.9523733512456, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='layer of which will distribute it to the hidden layers. The hidden layers will then', bbox=BoundingBox(l=134.76501022108476, t=608.9746733624115, r=480.5867307884124, b=618.9073734172542, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='make decisions from the previous layer and weigh up how a stochastic change', bbox=BoundingBox(l=134.76501022108476, t=620.9296734284203, r=480.58679078841254, b=630.8623834832631, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='within itself detriments or improves the final output, and this is referred to as', bbox=BoundingBox(l=134.76501022108476, t=632.884673494429, r=480.5867307884124, b=642.8173835492717, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='the process of learning. Having multiple hidden layers stacked upon each-other', bbox=BoundingBox(l=134.76501022108476, t=644.8406635604431, r=480.5867307884124, b=654.7733736152859, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='is commonly called deep learning.', bbox=BoundingBox(l=134.76501022108476, t=656.7956636264519, r=285.7780804688248, b=666.7283736812946, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='arXiv:1511.08458v2 [cs.NE] 2 Dec 2015', bbox=BoundingBox(l=18.3402140300875, t=264.95001146290315, r=36.33979405961618, b=604.8900433398585, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))], predictions=PagePredictions(layout=LayoutPrediction(clusters=[Cluster(id=0, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=137.64463806152344, t=116.22373962402344, r=476.82874078224734, b=130.95611072306508, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8532029986381531, cells=[Cell(id=0, text='An Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=138.53101022726298, t=116.98291064591297, r=476.82874078224734, b=130.95611072306508, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=1, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=235.75559997558594, t=160.30120088509193, r=378.7718811035156, b=171.6171875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7410858869552612, cells=[Cell(id=1, text='Keiron O’Shea', bbox=BoundingBox(l=236.21001038750742, t=161.63281089244435, r=299.9806504921245, b=171.565490947287, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=2, text='1', bbox=BoundingBox(l=299.981020492125, t=160.30120088509193, r=303.9526104986405, b=166.49395091928477, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='and Ryan Nash', bbox=BoundingBox(l=306.94101050354305, t=161.63281089244435, r=374.67673061466496, b=171.565490947287, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='2', bbox=BoundingBox(l=374.6760306146638, t=160.30120088509193, r=378.6476106211792, b=166.49395091928477, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=2, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=140.86863708496094, t=181.55941772460938, r=474.033690777662, b=202.1148223876953, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.6404754519462585, cells=[Cell(id=5, text='1', bbox=BoundingBox(l=141.3270302318499, t=181.6796210031315, r=144.97993023784258, b=186.9877310324398, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB', bbox=BoundingBox(l=149.96103024601416, t=183.22613101167042, r=474.033690777662, b=192.1655810610289, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='keo7@aber.ac.uk', bbox=BoundingBox(l=267.3290404385588, t=194.99108107662983, r=348.0266405709449, b=201.9131411148494, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=3, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=139.36328125, t=203.46005249023438, r=475.54037078013374, b=225.04162124255174, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.6642215251922607, cells=[Cell(id=8, text='2', bbox=BoundingBox(l=139.82004022937767, t=203.5966811241451, r=143.47295023537032, b=208.9047811534533, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='School of Computing and Communications, Lancaster University, Lancashire, LA1', bbox=BoundingBox(l=148.45503024354352, t=205.14318113268382, r=475.54037078013374, b=214.0826411820425, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='4YW', bbox=BoundingBox(l=297.96301048881446, t=216.10217119319316, r=317.39319052069004, b=225.04162124255174, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=245.39329528808594, t=226.80148315429688, r=369.5463306062484, b=234.79016129637762, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7631615996360779, cells=[Cell(id=11, text='nashrd@live.lancs.ac.uk', bbox=BoundingBox(l=245.8100104032564, t=227.86810125815794, r=369.5463306062484, b=234.79016129637762, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=162.13299560546875, t=253.8007049560547, r=452.33795166015625, b=351.5365219409846, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9657419919967651, cells=[Cell(id=12, text='Abstract.', bbox=BoundingBox(l=163.11102026758698, t=254.98889140790345, r=199.71187032763143, b=263.72216145612356, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='The field of machine learning has taken a dramatic twist in re-', bbox=BoundingBox(l=204.6930203358031, t=254.926141407557, r=452.2463107419194, b=263.86560145691567, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='cent times, with the rise of the Artificial Neural Network (ANN). These', bbox=BoundingBox(l=163.11102026758698, t=265.88415146806096, r=452.2416707419117, b=274.82360151741955, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='biologically inspired computational models are able to far exceed the per-', bbox=BoundingBox(l=163.11102026758698, t=276.8431315285702, r=452.2416107419117, b=285.78259157792877, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='formance of previous forms of artificial intelligence in common machine', bbox=BoundingBox(l=163.11102026758698, t=287.80212158907943, r=452.2415207419116, b=296.7415716384379, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='learning tasks. One of the most impressive forms of ANN architecture is', bbox=BoundingBox(l=163.11102026758698, t=298.7611016495888, r=452.2415207419116, b=307.70056169894735, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='that of the Convolutional Neural Network (CNN). CNNs are primarily', bbox=BoundingBox(l=163.11102026758698, t=309.720091710098, r=452.2415807419116, b=318.6595417594566, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='used to solve difficult image-driven pattern recognition tasks and with', bbox=BoundingBox(l=163.11102026758698, t=320.67907177060727, r=452.2415207419116, b=329.6185318199658, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='their precise yet simple architecture, offers a simplified method of getting', bbox=BoundingBox(l=163.11102026758698, t=331.6380318311164, r=452.2415507419116, b=340.5775418804753, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='started with ANNs.', bbox=BoundingBox(l=163.11102026758698, t=342.5970118916257, r=241.19044039567788, b=351.5365219409846, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=162.31292724609375, t=359.31500244140625, r=452.2415507419116, b=402.4580993652344, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9703543782234192, cells=[Cell(id=22, text='This document provides a brief introduction to CNNs, discussing recently', bbox=BoundingBox(l=163.11102026758698, t=360.5400019906967, r=452.2414607419114, b=369.47952204005566, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='published papers and newly formed techniques in developing these bril-', bbox=BoundingBox(l=163.11102026758698, t=371.49899205120596, r=452.2415507419116, b=380.43850210056485, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='liantly fantastic image recognition models. This introduction assumes you', bbox=BoundingBox(l=163.11102026758698, t=382.45700211170987, r=452.2415507419116, b=391.3965121610688, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='are familiar with the fundamentals of ANNs and machine learning.', bbox=BoundingBox(l=163.11102026758698, t=393.41598217221906, r=429.83463070515256, b=402.355492221578, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=162.44024658203125, t=421.299072265625, r=452.4108907421894, b=442.73138427734375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9514586925506592, cells=[Cell(id=26, text='Keywords:', bbox=BoundingBox(l=163.11102026758698, t=422.3807623321461, r=207.19881033991393, b=431.1140423803664, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='Pattern recognition, artificial neural networks, machine learn-', bbox=BoundingBox(l=211.6810303472671, t=422.31799233179953, r=452.4108907421894, b=431.2575023811584, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='ing, image analysis', bbox=BoundingBox(l=163.11102026758698, t=433.27700239230904, r=238.93089039197108, b=442.21652244166796, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=8, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.2177734375, t=471.4940490722656, r=221.76300036380678, b=483.745082670965, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9292116761207581, cells=[Cell(id=29, text='1', bbox=BoundingBox(l=134.76501022108476, t=472.10070260667135, r=140.74261023089116, b=483.745082670965, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='Introduction', bbox=BoundingBox(l=152.6978102505039, t=472.10070260667135, r=221.76300036380678, b=483.745082670965, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.661865234375, t=505.0313415527344, r=480.72076416015625, b=576.1156616210938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9834169149398804, cells=[Cell(id=31, text='Artificial Neural Networks', bbox=BoundingBox(l=134.76501022108476, t=506.41946279616, r=257.86292042302944, b=516.1230428497377, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='(ANNs) are computational processing systems of', bbox=BoundingBox(l=261.0410204282432, t=506.349732795775, r=480.58688078841266, b=516.2824428506178, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='which are heavily inspired by way biological nervous systems (such as the hu-', bbox=BoundingBox(l=134.76501022108476, t=518.3047128617836, r=480.58670078841243, b=528.2374229166264, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='man brain) operate. ANNs are mainly comprised of a high number of intercon-', bbox=BoundingBox(l=134.76501022108476, t=530.2597029277922, r=480.58676078841245, b=540.192412982635, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='nected computational nodes (referred to as neurons), of which work entwine in', bbox=BoundingBox(l=134.76501022108476, t=542.2146929938009, r=480.58682078841264, b=552.1474030486437, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='a distributed fashion to collectively learn from the input in order to optimise its', bbox=BoundingBox(l=134.76501022108476, t=554.1696730598095, r=480.58679078841254, b=564.1023831146523, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='final output.', bbox=BoundingBox(l=134.76501022108476, t=566.1256731258238, r=189.3899503106981, b=576.0583831806665, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=10, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.71791076660156, t=583.977294921875, r=480.8759460449219, b=667.2293701171875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9865064024925232, cells=[Cell(id=38, text='The basic structure of a ANN can be modelled as shown in Figure 1. We would', bbox=BoundingBox(l=134.76501022108476, t=585.064663230394, r=480.5867307884124, b=594.9973732852368, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='load the input, usually in the form of a multidimensional vector to the input', bbox=BoundingBox(l=134.76501022108476, t=597.0196632964028, r=480.5867307884124, b=606.9523733512456, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='layer of which will distribute it to the hidden layers. The hidden layers will then', bbox=BoundingBox(l=134.76501022108476, t=608.9746733624115, r=480.5867307884124, b=618.9073734172542, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='make decisions from the previous layer and weigh up how a stochastic change', bbox=BoundingBox(l=134.76501022108476, t=620.9296734284203, r=480.58679078841254, b=630.8623834832631, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='within itself detriments or improves the final output, and this is referred to as', bbox=BoundingBox(l=134.76501022108476, t=632.884673494429, r=480.5867307884124, b=642.8173835492717, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='the process of learning. Having multiple hidden layers stacked upon each-other', bbox=BoundingBox(l=134.76501022108476, t=644.8406635604431, r=480.5867307884124, b=654.7733736152859, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='is commonly called deep learning.', bbox=BoundingBox(l=134.76501022108476, t=656.7956636264519, r=285.7780804688248, b=666.7283736812946, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=11, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=17.202451705932617, t=264.95001146290315, r=36.33979405961618, b=604.8900433398585, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8815839290618896, cells=[Cell(id=45, text='arXiv:1511.08458v2 [cs.NE] 2 Dec 2015', bbox=BoundingBox(l=18.3402140300875, t=264.95001146290315, r=36.33979405961618, b=604.8900433398585, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))])]), tablestructure=TableStructurePrediction(table_map={}), figures_classification=None, equations_prediction=None), assembled=AssembledUnit(elements=[TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=0, page_no=0, cluster=Cluster(id=0, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=137.64463806152344, t=116.22373962402344, r=476.82874078224734, b=130.95611072306508, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8532029986381531, cells=[Cell(id=0, text='An Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=138.53101022726298, t=116.98291064591297, r=476.82874078224734, b=130.95611072306508, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='An Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=1, page_no=0, cluster=Cluster(id=1, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=235.75559997558594, t=160.30120088509193, r=378.7718811035156, b=171.6171875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7410858869552612, cells=[Cell(id=1, text='Keiron O’Shea', bbox=BoundingBox(l=236.21001038750742, t=161.63281089244435, r=299.9806504921245, b=171.565490947287, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=2, text='1', bbox=BoundingBox(l=299.981020492125, t=160.30120088509193, r=303.9526104986405, b=166.49395091928477, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='and Ryan Nash', bbox=BoundingBox(l=306.94101050354305, t=161.63281089244435, r=374.67673061466496, b=171.565490947287, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='2', bbox=BoundingBox(l=374.6760306146638, t=160.30120088509193, r=378.6476106211792, b=166.49395091928477, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea 1 and Ryan Nash 2'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=2, page_no=0, cluster=Cluster(id=2, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=140.86863708496094, t=181.55941772460938, r=474.033690777662, b=202.1148223876953, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.6404754519462585, cells=[Cell(id=5, text='1', bbox=BoundingBox(l=141.3270302318499, t=181.6796210031315, r=144.97993023784258, b=186.9877310324398, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB', bbox=BoundingBox(l=149.96103024601416, t=183.22613101167042, r=474.033690777662, b=192.1655810610289, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='keo7@aber.ac.uk', bbox=BoundingBox(l=267.3290404385588, t=194.99108107662983, r=348.0266405709449, b=201.9131411148494, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB keo7@aber.ac.uk'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=3, page_no=0, cluster=Cluster(id=3, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=139.36328125, t=203.46005249023438, r=475.54037078013374, b=225.04162124255174, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.6642215251922607, cells=[Cell(id=8, text='2', bbox=BoundingBox(l=139.82004022937767, t=203.5966811241451, r=143.47295023537032, b=208.9047811534533, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='School of Computing and Communications, Lancaster University, Lancashire, LA1', bbox=BoundingBox(l=148.45503024354352, t=205.14318113268382, r=475.54037078013374, b=214.0826411820425, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='4YW', bbox=BoundingBox(l=297.96301048881446, t=216.10217119319316, r=317.39319052069004, b=225.04162124255174, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2 School of Computing and Communications, Lancaster University, Lancashire, LA1 4YW'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=4, page_no=0, cluster=Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=245.39329528808594, t=226.80148315429688, r=369.5463306062484, b=234.79016129637762, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7631615996360779, cells=[Cell(id=11, text='nashrd@live.lancs.ac.uk', bbox=BoundingBox(l=245.8100104032564, t=227.86810125815794, r=369.5463306062484, b=234.79016129637762, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='nashrd@live.lancs.ac.uk'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=0, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=162.13299560546875, t=253.8007049560547, r=452.33795166015625, b=351.5365219409846, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9657419919967651, cells=[Cell(id=12, text='Abstract.', bbox=BoundingBox(l=163.11102026758698, t=254.98889140790345, r=199.71187032763143, b=263.72216145612356, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='The field of machine learning has taken a dramatic twist in re-', bbox=BoundingBox(l=204.6930203358031, t=254.926141407557, r=452.2463107419194, b=263.86560145691567, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='cent times, with the rise of the Artificial Neural Network (ANN). These', bbox=BoundingBox(l=163.11102026758698, t=265.88415146806096, r=452.2416707419117, b=274.82360151741955, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='biologically inspired computational models are able to far exceed the per-', bbox=BoundingBox(l=163.11102026758698, t=276.8431315285702, r=452.2416107419117, b=285.78259157792877, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='formance of previous forms of artificial intelligence in common machine', bbox=BoundingBox(l=163.11102026758698, t=287.80212158907943, r=452.2415207419116, b=296.7415716384379, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='learning tasks. One of the most impressive forms of ANN architecture is', bbox=BoundingBox(l=163.11102026758698, t=298.7611016495888, r=452.2415207419116, b=307.70056169894735, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='that of the Convolutional Neural Network (CNN). CNNs are primarily', bbox=BoundingBox(l=163.11102026758698, t=309.720091710098, r=452.2415807419116, b=318.6595417594566, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='used to solve difficult image-driven pattern recognition tasks and with', bbox=BoundingBox(l=163.11102026758698, t=320.67907177060727, r=452.2415207419116, b=329.6185318199658, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='their precise yet simple architecture, offers a simplified method of getting', bbox=BoundingBox(l=163.11102026758698, t=331.6380318311164, r=452.2415507419116, b=340.5775418804753, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='started with ANNs.', bbox=BoundingBox(l=163.11102026758698, t=342.5970118916257, r=241.19044039567788, b=351.5365219409846, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Abstract. The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=0, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=162.31292724609375, t=359.31500244140625, r=452.2415507419116, b=402.4580993652344, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9703543782234192, cells=[Cell(id=22, text='This document provides a brief introduction to CNNs, discussing recently', bbox=BoundingBox(l=163.11102026758698, t=360.5400019906967, r=452.2414607419114, b=369.47952204005566, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='published papers and newly formed techniques in developing these bril-', bbox=BoundingBox(l=163.11102026758698, t=371.49899205120596, r=452.2415507419116, b=380.43850210056485, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='liantly fantastic image recognition models. This introduction assumes you', bbox=BoundingBox(l=163.11102026758698, t=382.45700211170987, r=452.2415507419116, b=391.3965121610688, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='are familiar with the fundamentals of ANNs and machine learning.', bbox=BoundingBox(l=163.11102026758698, t=393.41598217221906, r=429.83463070515256, b=402.355492221578, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=0, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=162.44024658203125, t=421.299072265625, r=452.4108907421894, b=442.73138427734375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9514586925506592, cells=[Cell(id=26, text='Keywords:', bbox=BoundingBox(l=163.11102026758698, t=422.3807623321461, r=207.19881033991393, b=431.1140423803664, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='Pattern recognition, artificial neural networks, machine learn-', bbox=BoundingBox(l=211.6810303472671, t=422.31799233179953, r=452.4108907421894, b=431.2575023811584, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='ing, image analysis', bbox=BoundingBox(l=163.11102026758698, t=433.27700239230904, r=238.93089039197108, b=442.21652244166796, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keywords: Pattern recognition, artificial neural networks, machine learning, image analysis'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=8, page_no=0, cluster=Cluster(id=8, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.2177734375, t=471.4940490722656, r=221.76300036380678, b=483.745082670965, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9292116761207581, cells=[Cell(id=29, text='1', bbox=BoundingBox(l=134.76501022108476, t=472.10070260667135, r=140.74261023089116, b=483.745082670965, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='Introduction', bbox=BoundingBox(l=152.6978102505039, t=472.10070260667135, r=221.76300036380678, b=483.745082670965, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='1 Introduction'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=9, page_no=0, cluster=Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.661865234375, t=505.0313415527344, r=480.72076416015625, b=576.1156616210938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9834169149398804, cells=[Cell(id=31, text='Artificial Neural Networks', bbox=BoundingBox(l=134.76501022108476, t=506.41946279616, r=257.86292042302944, b=516.1230428497377, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='(ANNs) are computational processing systems of', bbox=BoundingBox(l=261.0410204282432, t=506.349732795775, r=480.58688078841266, b=516.2824428506178, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='which are heavily inspired by way biological nervous systems (such as the hu-', bbox=BoundingBox(l=134.76501022108476, t=518.3047128617836, r=480.58670078841243, b=528.2374229166264, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='man brain) operate. ANNs are mainly comprised of a high number of intercon-', bbox=BoundingBox(l=134.76501022108476, t=530.2597029277922, r=480.58676078841245, b=540.192412982635, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='nected computational nodes (referred to as neurons), of which work entwine in', bbox=BoundingBox(l=134.76501022108476, t=542.2146929938009, r=480.58682078841264, b=552.1474030486437, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='a distributed fashion to collectively learn from the input in order to optimise its', bbox=BoundingBox(l=134.76501022108476, t=554.1696730598095, r=480.58679078841254, b=564.1023831146523, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='final output.', bbox=BoundingBox(l=134.76501022108476, t=566.1256731258238, r=189.3899503106981, b=576.0583831806665, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Artificial Neural Networks (ANNs) are computational processing systems of which are heavily inspired by way biological nervous systems (such as the human brain) operate. ANNs are mainly comprised of a high number of interconnected computational nodes (referred to as neurons), of which work entwine in a distributed fashion to collectively learn from the input in order to optimise its final output.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=10, page_no=0, cluster=Cluster(id=10, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.71791076660156, t=583.977294921875, r=480.8759460449219, b=667.2293701171875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9865064024925232, cells=[Cell(id=38, text='The basic structure of a ANN can be modelled as shown in Figure 1. We would', bbox=BoundingBox(l=134.76501022108476, t=585.064663230394, r=480.5867307884124, b=594.9973732852368, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='load the input, usually in the form of a multidimensional vector to the input', bbox=BoundingBox(l=134.76501022108476, t=597.0196632964028, r=480.5867307884124, b=606.9523733512456, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='layer of which will distribute it to the hidden layers. The hidden layers will then', bbox=BoundingBox(l=134.76501022108476, t=608.9746733624115, r=480.5867307884124, b=618.9073734172542, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='make decisions from the previous layer and weigh up how a stochastic change', bbox=BoundingBox(l=134.76501022108476, t=620.9296734284203, r=480.58679078841254, b=630.8623834832631, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='within itself detriments or improves the final output, and this is referred to as', bbox=BoundingBox(l=134.76501022108476, t=632.884673494429, r=480.5867307884124, b=642.8173835492717, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='the process of learning. Having multiple hidden layers stacked upon each-other', bbox=BoundingBox(l=134.76501022108476, t=644.8406635604431, r=480.5867307884124, b=654.7733736152859, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='is commonly called deep learning.', bbox=BoundingBox(l=134.76501022108476, t=656.7956636264519, r=285.7780804688248, b=666.7283736812946, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The basic structure of a ANN can be modelled as shown in Figure 1. We would load the input, usually in the form of a multidimensional vector to the input layer of which will distribute it to the hidden layers. The hidden layers will then make decisions from the previous layer and weigh up how a stochastic change within itself detriments or improves the final output, and this is referred to as the process of learning. Having multiple hidden layers stacked upon each-other is commonly called deep learning.'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=11, page_no=0, cluster=Cluster(id=11, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=17.202451705932617, t=264.95001146290315, r=36.33979405961618, b=604.8900433398585, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8815839290618896, cells=[Cell(id=45, text='arXiv:1511.08458v2 [cs.NE] 2 Dec 2015', bbox=BoundingBox(l=18.3402140300875, t=264.95001146290315, r=36.33979405961618, b=604.8900433398585, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='arXiv:1511.08458v2 [cs.NE] 2 Dec 2015')], body=[TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=0, page_no=0, cluster=Cluster(id=0, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=137.64463806152344, t=116.22373962402344, r=476.82874078224734, b=130.95611072306508, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8532029986381531, cells=[Cell(id=0, text='An Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=138.53101022726298, t=116.98291064591297, r=476.82874078224734, b=130.95611072306508, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='An Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=1, page_no=0, cluster=Cluster(id=1, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=235.75559997558594, t=160.30120088509193, r=378.7718811035156, b=171.6171875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7410858869552612, cells=[Cell(id=1, text='Keiron O’Shea', bbox=BoundingBox(l=236.21001038750742, t=161.63281089244435, r=299.9806504921245, b=171.565490947287, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=2, text='1', bbox=BoundingBox(l=299.981020492125, t=160.30120088509193, r=303.9526104986405, b=166.49395091928477, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='and Ryan Nash', bbox=BoundingBox(l=306.94101050354305, t=161.63281089244435, r=374.67673061466496, b=171.565490947287, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='2', bbox=BoundingBox(l=374.6760306146638, t=160.30120088509193, r=378.6476106211792, b=166.49395091928477, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea 1 and Ryan Nash 2'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=2, page_no=0, cluster=Cluster(id=2, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=140.86863708496094, t=181.55941772460938, r=474.033690777662, b=202.1148223876953, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.6404754519462585, cells=[Cell(id=5, text='1', bbox=BoundingBox(l=141.3270302318499, t=181.6796210031315, r=144.97993023784258, b=186.9877310324398, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB', bbox=BoundingBox(l=149.96103024601416, t=183.22613101167042, r=474.033690777662, b=192.1655810610289, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='keo7@aber.ac.uk', bbox=BoundingBox(l=267.3290404385588, t=194.99108107662983, r=348.0266405709449, b=201.9131411148494, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB keo7@aber.ac.uk'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=3, page_no=0, cluster=Cluster(id=3, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=139.36328125, t=203.46005249023438, r=475.54037078013374, b=225.04162124255174, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.6642215251922607, cells=[Cell(id=8, text='2', bbox=BoundingBox(l=139.82004022937767, t=203.5966811241451, r=143.47295023537032, b=208.9047811534533, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='School of Computing and Communications, Lancaster University, Lancashire, LA1', bbox=BoundingBox(l=148.45503024354352, t=205.14318113268382, r=475.54037078013374, b=214.0826411820425, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='4YW', bbox=BoundingBox(l=297.96301048881446, t=216.10217119319316, r=317.39319052069004, b=225.04162124255174, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2 School of Computing and Communications, Lancaster University, Lancashire, LA1 4YW'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=4, page_no=0, cluster=Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=245.39329528808594, t=226.80148315429688, r=369.5463306062484, b=234.79016129637762, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7631615996360779, cells=[Cell(id=11, text='nashrd@live.lancs.ac.uk', bbox=BoundingBox(l=245.8100104032564, t=227.86810125815794, r=369.5463306062484, b=234.79016129637762, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='nashrd@live.lancs.ac.uk'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=0, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=162.13299560546875, t=253.8007049560547, r=452.33795166015625, b=351.5365219409846, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9657419919967651, cells=[Cell(id=12, text='Abstract.', bbox=BoundingBox(l=163.11102026758698, t=254.98889140790345, r=199.71187032763143, b=263.72216145612356, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='The field of machine learning has taken a dramatic twist in re-', bbox=BoundingBox(l=204.6930203358031, t=254.926141407557, r=452.2463107419194, b=263.86560145691567, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='cent times, with the rise of the Artificial Neural Network (ANN). These', bbox=BoundingBox(l=163.11102026758698, t=265.88415146806096, r=452.2416707419117, b=274.82360151741955, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='biologically inspired computational models are able to far exceed the per-', bbox=BoundingBox(l=163.11102026758698, t=276.8431315285702, r=452.2416107419117, b=285.78259157792877, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='formance of previous forms of artificial intelligence in common machine', bbox=BoundingBox(l=163.11102026758698, t=287.80212158907943, r=452.2415207419116, b=296.7415716384379, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='learning tasks. One of the most impressive forms of ANN architecture is', bbox=BoundingBox(l=163.11102026758698, t=298.7611016495888, r=452.2415207419116, b=307.70056169894735, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='that of the Convolutional Neural Network (CNN). CNNs are primarily', bbox=BoundingBox(l=163.11102026758698, t=309.720091710098, r=452.2415807419116, b=318.6595417594566, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='used to solve difficult image-driven pattern recognition tasks and with', bbox=BoundingBox(l=163.11102026758698, t=320.67907177060727, r=452.2415207419116, b=329.6185318199658, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='their precise yet simple architecture, offers a simplified method of getting', bbox=BoundingBox(l=163.11102026758698, t=331.6380318311164, r=452.2415507419116, b=340.5775418804753, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='started with ANNs.', bbox=BoundingBox(l=163.11102026758698, t=342.5970118916257, r=241.19044039567788, b=351.5365219409846, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Abstract. The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=0, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=162.31292724609375, t=359.31500244140625, r=452.2415507419116, b=402.4580993652344, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9703543782234192, cells=[Cell(id=22, text='This document provides a brief introduction to CNNs, discussing recently', bbox=BoundingBox(l=163.11102026758698, t=360.5400019906967, r=452.2414607419114, b=369.47952204005566, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='published papers and newly formed techniques in developing these bril-', bbox=BoundingBox(l=163.11102026758698, t=371.49899205120596, r=452.2415507419116, b=380.43850210056485, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='liantly fantastic image recognition models. This introduction assumes you', bbox=BoundingBox(l=163.11102026758698, t=382.45700211170987, r=452.2415507419116, b=391.3965121610688, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='are familiar with the fundamentals of ANNs and machine learning.', bbox=BoundingBox(l=163.11102026758698, t=393.41598217221906, r=429.83463070515256, b=402.355492221578, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=0, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=162.44024658203125, t=421.299072265625, r=452.4108907421894, b=442.73138427734375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9514586925506592, cells=[Cell(id=26, text='Keywords:', bbox=BoundingBox(l=163.11102026758698, t=422.3807623321461, r=207.19881033991393, b=431.1140423803664, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='Pattern recognition, artificial neural networks, machine learn-', bbox=BoundingBox(l=211.6810303472671, t=422.31799233179953, r=452.4108907421894, b=431.2575023811584, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='ing, image analysis', bbox=BoundingBox(l=163.11102026758698, t=433.27700239230904, r=238.93089039197108, b=442.21652244166796, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keywords: Pattern recognition, artificial neural networks, machine learning, image analysis'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=8, page_no=0, cluster=Cluster(id=8, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.2177734375, t=471.4940490722656, r=221.76300036380678, b=483.745082670965, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9292116761207581, cells=[Cell(id=29, text='1', bbox=BoundingBox(l=134.76501022108476, t=472.10070260667135, r=140.74261023089116, b=483.745082670965, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='Introduction', bbox=BoundingBox(l=152.6978102505039, t=472.10070260667135, r=221.76300036380678, b=483.745082670965, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='1 Introduction'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=9, page_no=0, cluster=Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.661865234375, t=505.0313415527344, r=480.72076416015625, b=576.1156616210938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9834169149398804, cells=[Cell(id=31, text='Artificial Neural Networks', bbox=BoundingBox(l=134.76501022108476, t=506.41946279616, r=257.86292042302944, b=516.1230428497377, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='(ANNs) are computational processing systems of', bbox=BoundingBox(l=261.0410204282432, t=506.349732795775, r=480.58688078841266, b=516.2824428506178, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='which are heavily inspired by way biological nervous systems (such as the hu-', bbox=BoundingBox(l=134.76501022108476, t=518.3047128617836, r=480.58670078841243, b=528.2374229166264, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='man brain) operate. ANNs are mainly comprised of a high number of intercon-', bbox=BoundingBox(l=134.76501022108476, t=530.2597029277922, r=480.58676078841245, b=540.192412982635, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='nected computational nodes (referred to as neurons), of which work entwine in', bbox=BoundingBox(l=134.76501022108476, t=542.2146929938009, r=480.58682078841264, b=552.1474030486437, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='a distributed fashion to collectively learn from the input in order to optimise its', bbox=BoundingBox(l=134.76501022108476, t=554.1696730598095, r=480.58679078841254, b=564.1023831146523, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='final output.', bbox=BoundingBox(l=134.76501022108476, t=566.1256731258238, r=189.3899503106981, b=576.0583831806665, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Artificial Neural Networks (ANNs) are computational processing systems of which are heavily inspired by way biological nervous systems (such as the human brain) operate. ANNs are mainly comprised of a high number of interconnected computational nodes (referred to as neurons), of which work entwine in a distributed fashion to collectively learn from the input in order to optimise its final output.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=10, page_no=0, cluster=Cluster(id=10, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.71791076660156, t=583.977294921875, r=480.8759460449219, b=667.2293701171875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9865064024925232, cells=[Cell(id=38, text='The basic structure of a ANN can be modelled as shown in Figure 1. We would', bbox=BoundingBox(l=134.76501022108476, t=585.064663230394, r=480.5867307884124, b=594.9973732852368, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='load the input, usually in the form of a multidimensional vector to the input', bbox=BoundingBox(l=134.76501022108476, t=597.0196632964028, r=480.5867307884124, b=606.9523733512456, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='layer of which will distribute it to the hidden layers. The hidden layers will then', bbox=BoundingBox(l=134.76501022108476, t=608.9746733624115, r=480.5867307884124, b=618.9073734172542, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='make decisions from the previous layer and weigh up how a stochastic change', bbox=BoundingBox(l=134.76501022108476, t=620.9296734284203, r=480.58679078841254, b=630.8623834832631, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='within itself detriments or improves the final output, and this is referred to as', bbox=BoundingBox(l=134.76501022108476, t=632.884673494429, r=480.5867307884124, b=642.8173835492717, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='the process of learning. Having multiple hidden layers stacked upon each-other', bbox=BoundingBox(l=134.76501022108476, t=644.8406635604431, r=480.5867307884124, b=654.7733736152859, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='is commonly called deep learning.', bbox=BoundingBox(l=134.76501022108476, t=656.7956636264519, r=285.7780804688248, b=666.7283736812946, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The basic structure of a ANN can be modelled as shown in Figure 1. We would load the input, usually in the form of a multidimensional vector to the input layer of which will distribute it to the hidden layers. The hidden layers will then make decisions from the previous layer and weigh up how a stochastic change within itself detriments or improves the final output, and this is referred to as the process of learning. Having multiple hidden layers stacked upon each-other is commonly called deep learning.')], headers=[TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=11, page_no=0, cluster=Cluster(id=11, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=17.202451705932617, t=264.95001146290315, r=36.33979405961618, b=604.8900433398585, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8815839290618896, cells=[Cell(id=45, text='arXiv:1511.08458v2 [cs.NE] 2 Dec 2015', bbox=BoundingBox(l=18.3402140300875, t=264.95001146290315, r=36.33979405961618, b=604.8900433398585, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='arXiv:1511.08458v2 [cs.NE] 2 Dec 2015')])), Page(page_no=1, size=Size(width=595.2760009765625, height=841.8900146484375), cells=[Cell(id=0, text='2', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=2, text='Fig. 1: A simple three layered feedforward neural network (FNN), comprised', bbox=BoundingBox(l=134.76500022108476, t=278.50677153775587, r=480.58679078841254, b=288.4394515925984, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='of a input layer, a hidden layer and an output layer. This structure is the basis', bbox=BoundingBox(l=134.76500022108476, t=290.4617916037647, r=480.5866407884123, b=300.39447165860736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='of a number of common ANN architectures, included but not limited to Feed-', bbox=BoundingBox(l=134.76500022108476, t=302.41680166977346, r=480.58676078841245, b=312.349481724616, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='forward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and', bbox=BoundingBox(l=134.76500022108476, t=314.3718217357823, r=480.58676078841245, b=324.30450179062484, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='Recurrent Neural Networks (RNNs).', bbox=BoundingBox(l=134.76500022108476, t=326.32678180179073, r=296.7867704868848, b=336.2594918566335, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='The two key learning paradigms in image processing tasks are supervised and', bbox=BoundingBox(l=134.76500022108476, t=365.8777720201687, r=480.58679078841254, b=375.8104820750116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='unsupervised learning.', bbox=BoundingBox(l=134.76500022108476, t=377.8327620861774, r=237.7583603900475, b=387.76547214102015, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='Supervised learning', bbox=BoundingBox(l=241.2870003958363, t=377.90249208656246, r=334.47714054871665, b=387.6060721401401, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='is learning through pre-labelled', bbox=BoundingBox(l=338.00800055450907, t=377.8327620861774, r=480.59271078842227, b=387.76547214102015, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='inputs, which act as targets. For each training example there will be a set of', bbox=BoundingBox(l=134.76500022108476, t=389.78775215218604, r=480.58676078841245, b=399.72045220702887, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='input values (vectors) and one or more associated designated output values.', bbox=BoundingBox(l=134.76500022108476, t=401.7427322181947, r=480.58679078841254, b=411.6754422730375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='The goal of this form of training is to reduce the models overall classification', bbox=BoundingBox(l=134.76500022108476, t=413.69772228420334, r=480.58679078841254, b=423.6304323390461, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='error, through correct calculation of the output value of training example by', bbox=BoundingBox(l=134.76500022108476, t=425.65271235021197, r=480.58676078841245, b=435.58541240505474, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='training.', bbox=BoundingBox(l=134.76500022108476, t=437.6087024162262, r=172.35388028275008, b=447.54141247106895, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='Unsupervised learning', bbox=BoundingBox(l=134.76500022108476, t=458.9344425339748, r=239.35237039266252, b=468.63803258755246, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='differs in that the training set does not include any la-', bbox=BoundingBox(l=242.10600039717988, t=458.8647125335898, r=480.59070078841904, b=468.79742258843254, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='bels. Success is usually determined by whether the network is able to reduce or', bbox=BoundingBox(l=134.76500022108476, t=470.81970259959843, r=480.5867307884124, b=480.7524126544412, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='increase an associated cost function. However, it is important to note that most', bbox=BoundingBox(l=134.76500022108476, t=482.77569266561255, r=480.58679078841254, b=492.70840272045535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='image-focused pattern-recognition tasks usually depend on classification using', bbox=BoundingBox(l=134.76500022108476, t=494.7306827316212, r=480.5867307884124, b=504.663392786464, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='supervised learning.', bbox=BoundingBox(l=134.76500022108476, t=506.6856627976299, r=224.9065903689639, b=516.6183728524726, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='Convolutional Neural Networks', bbox=BoundingBox(l=134.76500022108476, t=528.012392915384, r=283.65607046534365, b=537.7159729689615, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='(CNNs) are analogous to traditional ANNs', bbox=BoundingBox(l=286.989990470813, t=527.9426529149989, r=480.593200788423, b=537.8753629698416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='in that they are comprised of neurons that self-optimise through learning. Each', bbox=BoundingBox(l=134.76498022108473, t=539.8976429810075, r=480.5867307884124, b=549.8303530358503, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='neuron will still receive an input and perform a operation (such as a scalar', bbox=BoundingBox(l=134.76498022108473, t=551.8526330470162, r=480.58667078841233, b=561.7853331018589, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='product followed by a non-linear function) - the basis of countless ANNs. From', bbox=BoundingBox(l=134.76498022108473, t=563.8076131130247, r=480.5866407884123, b=573.7403231678675, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='the input raw image vectors to the final output of the class score, the entire of', bbox=BoundingBox(l=134.76498022108473, t=575.7626031790335, r=480.5866407884123, b=585.695313233876, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='the network will still express a single perceptive score function (the weight).', bbox=BoundingBox(l=134.76498022108473, t=587.7185932450475, r=480.58688078841266, b=597.6513032998903, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='The last layer will contain loss functions associated with the classes, and all of', bbox=BoundingBox(l=134.76498022108473, t=599.6735933110563, r=480.58682078841264, b=609.6063033658991, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='the regular tips and tricks developed for traditional ANNs still apply.', bbox=BoundingBox(l=134.76498022108473, t=611.628603377065, r=439.90955072168066, b=621.5613134319078, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='The only notable difference between CNNs and traditional ANNs is that CNNs', bbox=BoundingBox(l=134.76498022108473, t=632.885603494434, r=480.5868507884127, b=642.8183135492768, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='are primarily used in the field of pattern recognition within images. This allows', bbox=BoundingBox(l=134.76498022108473, t=644.8406035604428, r=480.58679078841254, b=654.7733136152856, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='us to encode image-specific features into the architecture, making the network', bbox=BoundingBox(l=134.76498022108473, t=656.7956036264516, r=480.58682078841264, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))], predictions=PagePredictions(layout=LayoutPrediction(clusters=[Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=133.6807861328125, t=93.64701080322266, r=139.58908081054688, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7250283360481262, cells=[Cell(id=0, text='2', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=167.17018127441406, t=93.5260238647461, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.6552881002426147, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=2, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=133.7849884033203, t=278.09375, r=480.58679078841254, b=336.2594918566335, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9529556035995483, cells=[Cell(id=2, text='Fig. 1: A simple three layered feedforward neural network (FNN), comprised', bbox=BoundingBox(l=134.76500022108476, t=278.50677153775587, r=480.58679078841254, b=288.4394515925984, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='of a input layer, a hidden layer and an output layer. This structure is the basis', bbox=BoundingBox(l=134.76500022108476, t=290.4617916037647, r=480.5866407884123, b=300.39447165860736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='of a number of common ANN architectures, included but not limited to Feed-', bbox=BoundingBox(l=134.76500022108476, t=302.41680166977346, r=480.58676078841245, b=312.349481724616, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='forward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and', bbox=BoundingBox(l=134.76500022108476, t=314.3718217357823, r=480.58676078841245, b=324.30450179062484, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='Recurrent Neural Networks (RNNs).', bbox=BoundingBox(l=134.76500022108476, t=326.32678180179073, r=296.7867704868848, b=336.2594918566335, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.74380493164062, t=365.6249084472656, r=480.59271078842227, b=447.6732482910156, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9868213534355164, cells=[Cell(id=7, text='The two key learning paradigms in image processing tasks are supervised and', bbox=BoundingBox(l=134.76500022108476, t=365.8777720201687, r=480.58679078841254, b=375.8104820750116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='unsupervised learning.', bbox=BoundingBox(l=134.76500022108476, t=377.8327620861774, r=237.7583603900475, b=387.76547214102015, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='Supervised learning', bbox=BoundingBox(l=241.2870003958363, t=377.90249208656246, r=334.47714054871665, b=387.6060721401401, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='is learning through pre-labelled', bbox=BoundingBox(l=338.00800055450907, t=377.8327620861774, r=480.59271078842227, b=387.76547214102015, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='inputs, which act as targets. For each training example there will be a set of', bbox=BoundingBox(l=134.76500022108476, t=389.78775215218604, r=480.58676078841245, b=399.72045220702887, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='input values (vectors) and one or more associated designated output values.', bbox=BoundingBox(l=134.76500022108476, t=401.7427322181947, r=480.58679078841254, b=411.6754422730375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='The goal of this form of training is to reduce the models overall classification', bbox=BoundingBox(l=134.76500022108476, t=413.69772228420334, r=480.58679078841254, b=423.6304323390461, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='error, through correct calculation of the output value of training example by', bbox=BoundingBox(l=134.76500022108476, t=425.65271235021197, r=480.58676078841245, b=435.58541240505474, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='training.', bbox=BoundingBox(l=134.76500022108476, t=437.6087024162262, r=172.35388028275008, b=447.54141247106895, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.91444396972656, t=458.1381530761719, r=480.59070078841904, b=517.1390380859375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9838359951972961, cells=[Cell(id=16, text='Unsupervised learning', bbox=BoundingBox(l=134.76500022108476, t=458.9344425339748, r=239.35237039266252, b=468.63803258755246, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='differs in that the training set does not include any la-', bbox=BoundingBox(l=242.10600039717988, t=458.8647125335898, r=480.59070078841904, b=468.79742258843254, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='bels. Success is usually determined by whether the network is able to reduce or', bbox=BoundingBox(l=134.76500022108476, t=470.81970259959843, r=480.5867307884124, b=480.7524126544412, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='increase an associated cost function. However, it is important to note that most', bbox=BoundingBox(l=134.76500022108476, t=482.77569266561255, r=480.58679078841254, b=492.70840272045535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='image-focused pattern-recognition tasks usually depend on classification using', bbox=BoundingBox(l=134.76500022108476, t=494.7306827316212, r=480.5867307884124, b=504.663392786464, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='supervised learning.', bbox=BoundingBox(l=134.76500022108476, t=506.6856627976299, r=224.9065903689639, b=516.6183728524726, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.6765899658203, t=527.3146362304688, r=480.7476806640625, b=621.6336669921875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9860644340515137, cells=[Cell(id=22, text='Convolutional Neural Networks', bbox=BoundingBox(l=134.76500022108476, t=528.012392915384, r=283.65607046534365, b=537.7159729689615, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='(CNNs) are analogous to traditional ANNs', bbox=BoundingBox(l=286.989990470813, t=527.9426529149989, r=480.593200788423, b=537.8753629698416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='in that they are comprised of neurons that self-optimise through learning. Each', bbox=BoundingBox(l=134.76498022108473, t=539.8976429810075, r=480.5867307884124, b=549.8303530358503, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='neuron will still receive an input and perform a operation (such as a scalar', bbox=BoundingBox(l=134.76498022108473, t=551.8526330470162, r=480.58667078841233, b=561.7853331018589, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='product followed by a non-linear function) - the basis of countless ANNs. From', bbox=BoundingBox(l=134.76498022108473, t=563.8076131130247, r=480.5866407884123, b=573.7403231678675, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='the input raw image vectors to the final output of the class score, the entire of', bbox=BoundingBox(l=134.76498022108473, t=575.7626031790335, r=480.5866407884123, b=585.695313233876, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='the network will still express a single perceptive score function (the weight).', bbox=BoundingBox(l=134.76498022108473, t=587.7185932450475, r=480.58688078841266, b=597.6513032998903, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='The last layer will contain loss functions associated with the classes, and all of', bbox=BoundingBox(l=134.76498022108473, t=599.6735933110563, r=480.58682078841264, b=609.6063033658991, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='the regular tips and tricks developed for traditional ANNs still apply.', bbox=BoundingBox(l=134.76498022108473, t=611.628603377065, r=439.90955072168066, b=621.5613134319078, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.68446350097656, t=631.9942626953125, r=480.5868507884127, b=666.78271484375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9788795113563538, cells=[Cell(id=31, text='The only notable difference between CNNs and traditional ANNs is that CNNs', bbox=BoundingBox(l=134.76498022108473, t=632.885603494434, r=480.5868507884127, b=642.8183135492768, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='are primarily used in the field of pattern recognition within images. This allows', bbox=BoundingBox(l=134.76498022108473, t=644.8406035604428, r=480.58679078841254, b=654.7733136152856, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='us to encode image-specific features into the architecture, making the network', bbox=BoundingBox(l=134.76498022108473, t=656.7956036264516, r=480.58682078841264, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=7, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=193.8206024169922, t=115.9693374633789, r=420.3355407714844, b=267.38494873046875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9722045063972473, cells=[])]), tablestructure=TableStructurePrediction(table_map={}), figures_classification=None, equations_prediction=None), assembled=AssembledUnit(elements=[TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=1, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=133.6807861328125, t=93.64701080322266, r=139.58908081054688, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7250283360481262, cells=[Cell(id=0, text='2', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=1, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=167.17018127441406, t=93.5260238647461, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.6552881002426147, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea et al.'), TextElement(label=<DocItemLabel.CAPTION: 'caption'>, id=2, page_no=1, cluster=Cluster(id=2, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=133.7849884033203, t=278.09375, r=480.58679078841254, b=336.2594918566335, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9529556035995483, cells=[Cell(id=2, text='Fig. 1: A simple three layered feedforward neural network (FNN), comprised', bbox=BoundingBox(l=134.76500022108476, t=278.50677153775587, r=480.58679078841254, b=288.4394515925984, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='of a input layer, a hidden layer and an output layer. This structure is the basis', bbox=BoundingBox(l=134.76500022108476, t=290.4617916037647, r=480.5866407884123, b=300.39447165860736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='of a number of common ANN architectures, included but not limited to Feed-', bbox=BoundingBox(l=134.76500022108476, t=302.41680166977346, r=480.58676078841245, b=312.349481724616, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='forward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and', bbox=BoundingBox(l=134.76500022108476, t=314.3718217357823, r=480.58676078841245, b=324.30450179062484, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='Recurrent Neural Networks (RNNs).', bbox=BoundingBox(l=134.76500022108476, t=326.32678180179073, r=296.7867704868848, b=336.2594918566335, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Fig. 1: A simple three layered feedforward neural network (FNN), comprised of a input layer, a hidden layer and an output layer. This structure is the basis of a number of common ANN architectures, included but not limited to Feedforward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and Recurrent Neural Networks (RNNs).'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=1, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.74380493164062, t=365.6249084472656, r=480.59271078842227, b=447.6732482910156, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9868213534355164, cells=[Cell(id=7, text='The two key learning paradigms in image processing tasks are supervised and', bbox=BoundingBox(l=134.76500022108476, t=365.8777720201687, r=480.58679078841254, b=375.8104820750116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='unsupervised learning.', bbox=BoundingBox(l=134.76500022108476, t=377.8327620861774, r=237.7583603900475, b=387.76547214102015, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='Supervised learning', bbox=BoundingBox(l=241.2870003958363, t=377.90249208656246, r=334.47714054871665, b=387.6060721401401, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='is learning through pre-labelled', bbox=BoundingBox(l=338.00800055450907, t=377.8327620861774, r=480.59271078842227, b=387.76547214102015, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='inputs, which act as targets. For each training example there will be a set of', bbox=BoundingBox(l=134.76500022108476, t=389.78775215218604, r=480.58676078841245, b=399.72045220702887, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='input values (vectors) and one or more associated designated output values.', bbox=BoundingBox(l=134.76500022108476, t=401.7427322181947, r=480.58679078841254, b=411.6754422730375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='The goal of this form of training is to reduce the models overall classification', bbox=BoundingBox(l=134.76500022108476, t=413.69772228420334, r=480.58679078841254, b=423.6304323390461, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='error, through correct calculation of the output value of training example by', bbox=BoundingBox(l=134.76500022108476, t=425.65271235021197, r=480.58676078841245, b=435.58541240505474, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='training.', bbox=BoundingBox(l=134.76500022108476, t=437.6087024162262, r=172.35388028275008, b=447.54141247106895, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The two key learning paradigms in image processing tasks are supervised and unsupervised learning. Supervised learning is learning through pre-labelled inputs, which act as targets. For each training example there will be a set of input values (vectors) and one or more associated designated output values. The goal of this form of training is to reduce the models overall classification error, through correct calculation of the output value of training example by training.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=4, page_no=1, cluster=Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.91444396972656, t=458.1381530761719, r=480.59070078841904, b=517.1390380859375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9838359951972961, cells=[Cell(id=16, text='Unsupervised learning', bbox=BoundingBox(l=134.76500022108476, t=458.9344425339748, r=239.35237039266252, b=468.63803258755246, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='differs in that the training set does not include any la-', bbox=BoundingBox(l=242.10600039717988, t=458.8647125335898, r=480.59070078841904, b=468.79742258843254, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='bels. Success is usually determined by whether the network is able to reduce or', bbox=BoundingBox(l=134.76500022108476, t=470.81970259959843, r=480.5867307884124, b=480.7524126544412, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='increase an associated cost function. However, it is important to note that most', bbox=BoundingBox(l=134.76500022108476, t=482.77569266561255, r=480.58679078841254, b=492.70840272045535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='image-focused pattern-recognition tasks usually depend on classification using', bbox=BoundingBox(l=134.76500022108476, t=494.7306827316212, r=480.5867307884124, b=504.663392786464, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='supervised learning.', bbox=BoundingBox(l=134.76500022108476, t=506.6856627976299, r=224.9065903689639, b=516.6183728524726, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Unsupervised learning differs in that the training set does not include any labels. Success is usually determined by whether the network is able to reduce or increase an associated cost function. However, it is important to note that most image-focused pattern-recognition tasks usually depend on classification using supervised learning.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=1, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.6765899658203, t=527.3146362304688, r=480.7476806640625, b=621.6336669921875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9860644340515137, cells=[Cell(id=22, text='Convolutional Neural Networks', bbox=BoundingBox(l=134.76500022108476, t=528.012392915384, r=283.65607046534365, b=537.7159729689615, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='(CNNs) are analogous to traditional ANNs', bbox=BoundingBox(l=286.989990470813, t=527.9426529149989, r=480.593200788423, b=537.8753629698416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='in that they are comprised of neurons that self-optimise through learning. Each', bbox=BoundingBox(l=134.76498022108473, t=539.8976429810075, r=480.5867307884124, b=549.8303530358503, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='neuron will still receive an input and perform a operation (such as a scalar', bbox=BoundingBox(l=134.76498022108473, t=551.8526330470162, r=480.58667078841233, b=561.7853331018589, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='product followed by a non-linear function) - the basis of countless ANNs. From', bbox=BoundingBox(l=134.76498022108473, t=563.8076131130247, r=480.5866407884123, b=573.7403231678675, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='the input raw image vectors to the final output of the class score, the entire of', bbox=BoundingBox(l=134.76498022108473, t=575.7626031790335, r=480.5866407884123, b=585.695313233876, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='the network will still express a single perceptive score function (the weight).', bbox=BoundingBox(l=134.76498022108473, t=587.7185932450475, r=480.58688078841266, b=597.6513032998903, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='The last layer will contain loss functions associated with the classes, and all of', bbox=BoundingBox(l=134.76498022108473, t=599.6735933110563, r=480.58682078841264, b=609.6063033658991, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='the regular tips and tricks developed for traditional ANNs still apply.', bbox=BoundingBox(l=134.76498022108473, t=611.628603377065, r=439.90955072168066, b=621.5613134319078, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Convolutional Neural Networks (CNNs) are analogous to traditional ANNs in that they are comprised of neurons that self-optimise through learning. Each neuron will still receive an input and perform a operation (such as a scalar product followed by a non-linear function) - the basis of countless ANNs. From the input raw image vectors to the final output of the class score, the entire of the network will still express a single perceptive score function (the weight). The last layer will contain loss functions associated with the classes, and all of the regular tips and tricks developed for traditional ANNs still apply.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=1, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.68446350097656, t=631.9942626953125, r=480.5868507884127, b=666.78271484375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9788795113563538, cells=[Cell(id=31, text='The only notable difference between CNNs and traditional ANNs is that CNNs', bbox=BoundingBox(l=134.76498022108473, t=632.885603494434, r=480.5868507884127, b=642.8183135492768, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='are primarily used in the field of pattern recognition within images. This allows', bbox=BoundingBox(l=134.76498022108473, t=644.8406035604428, r=480.58679078841254, b=654.7733136152856, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='us to encode image-specific features into the architecture, making the network', bbox=BoundingBox(l=134.76498022108473, t=656.7956036264516, r=480.58682078841264, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The only notable difference between CNNs and traditional ANNs is that CNNs are primarily used in the field of pattern recognition within images. This allows us to encode image-specific features into the architecture, making the network'), FigureElement(label=<DocItemLabel.PICTURE: 'picture'>, id=7, page_no=1, cluster=Cluster(id=7, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=193.8206024169922, t=115.9693374633789, r=420.3355407714844, b=267.38494873046875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9722045063972473, cells=[]), text='', annotations=[], provenance=None, predicted_class=None, confidence=None)], body=[TextElement(label=<DocItemLabel.CAPTION: 'caption'>, id=2, page_no=1, cluster=Cluster(id=2, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=133.7849884033203, t=278.09375, r=480.58679078841254, b=336.2594918566335, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9529556035995483, cells=[Cell(id=2, text='Fig. 1: A simple three layered feedforward neural network (FNN), comprised', bbox=BoundingBox(l=134.76500022108476, t=278.50677153775587, r=480.58679078841254, b=288.4394515925984, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='of a input layer, a hidden layer and an output layer. This structure is the basis', bbox=BoundingBox(l=134.76500022108476, t=290.4617916037647, r=480.5866407884123, b=300.39447165860736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='of a number of common ANN architectures, included but not limited to Feed-', bbox=BoundingBox(l=134.76500022108476, t=302.41680166977346, r=480.58676078841245, b=312.349481724616, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='forward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and', bbox=BoundingBox(l=134.76500022108476, t=314.3718217357823, r=480.58676078841245, b=324.30450179062484, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='Recurrent Neural Networks (RNNs).', bbox=BoundingBox(l=134.76500022108476, t=326.32678180179073, r=296.7867704868848, b=336.2594918566335, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Fig. 1: A simple three layered feedforward neural network (FNN), comprised of a input layer, a hidden layer and an output layer. This structure is the basis of a number of common ANN architectures, included but not limited to Feedforward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and Recurrent Neural Networks (RNNs).'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=1, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.74380493164062, t=365.6249084472656, r=480.59271078842227, b=447.6732482910156, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9868213534355164, cells=[Cell(id=7, text='The two key learning paradigms in image processing tasks are supervised and', bbox=BoundingBox(l=134.76500022108476, t=365.8777720201687, r=480.58679078841254, b=375.8104820750116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='unsupervised learning.', bbox=BoundingBox(l=134.76500022108476, t=377.8327620861774, r=237.7583603900475, b=387.76547214102015, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='Supervised learning', bbox=BoundingBox(l=241.2870003958363, t=377.90249208656246, r=334.47714054871665, b=387.6060721401401, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='is learning through pre-labelled', bbox=BoundingBox(l=338.00800055450907, t=377.8327620861774, r=480.59271078842227, b=387.76547214102015, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='inputs, which act as targets. For each training example there will be a set of', bbox=BoundingBox(l=134.76500022108476, t=389.78775215218604, r=480.58676078841245, b=399.72045220702887, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='input values (vectors) and one or more associated designated output values.', bbox=BoundingBox(l=134.76500022108476, t=401.7427322181947, r=480.58679078841254, b=411.6754422730375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='The goal of this form of training is to reduce the models overall classification', bbox=BoundingBox(l=134.76500022108476, t=413.69772228420334, r=480.58679078841254, b=423.6304323390461, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='error, through correct calculation of the output value of training example by', bbox=BoundingBox(l=134.76500022108476, t=425.65271235021197, r=480.58676078841245, b=435.58541240505474, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='training.', bbox=BoundingBox(l=134.76500022108476, t=437.6087024162262, r=172.35388028275008, b=447.54141247106895, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The two key learning paradigms in image processing tasks are supervised and unsupervised learning. Supervised learning is learning through pre-labelled inputs, which act as targets. For each training example there will be a set of input values (vectors) and one or more associated designated output values. The goal of this form of training is to reduce the models overall classification error, through correct calculation of the output value of training example by training.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=4, page_no=1, cluster=Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.91444396972656, t=458.1381530761719, r=480.59070078841904, b=517.1390380859375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9838359951972961, cells=[Cell(id=16, text='Unsupervised learning', bbox=BoundingBox(l=134.76500022108476, t=458.9344425339748, r=239.35237039266252, b=468.63803258755246, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='differs in that the training set does not include any la-', bbox=BoundingBox(l=242.10600039717988, t=458.8647125335898, r=480.59070078841904, b=468.79742258843254, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='bels. Success is usually determined by whether the network is able to reduce or', bbox=BoundingBox(l=134.76500022108476, t=470.81970259959843, r=480.5867307884124, b=480.7524126544412, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='increase an associated cost function. However, it is important to note that most', bbox=BoundingBox(l=134.76500022108476, t=482.77569266561255, r=480.58679078841254, b=492.70840272045535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='image-focused pattern-recognition tasks usually depend on classification using', bbox=BoundingBox(l=134.76500022108476, t=494.7306827316212, r=480.5867307884124, b=504.663392786464, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='supervised learning.', bbox=BoundingBox(l=134.76500022108476, t=506.6856627976299, r=224.9065903689639, b=516.6183728524726, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Unsupervised learning differs in that the training set does not include any labels. Success is usually determined by whether the network is able to reduce or increase an associated cost function. However, it is important to note that most image-focused pattern-recognition tasks usually depend on classification using supervised learning.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=1, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.6765899658203, t=527.3146362304688, r=480.7476806640625, b=621.6336669921875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9860644340515137, cells=[Cell(id=22, text='Convolutional Neural Networks', bbox=BoundingBox(l=134.76500022108476, t=528.012392915384, r=283.65607046534365, b=537.7159729689615, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='(CNNs) are analogous to traditional ANNs', bbox=BoundingBox(l=286.989990470813, t=527.9426529149989, r=480.593200788423, b=537.8753629698416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='in that they are comprised of neurons that self-optimise through learning. Each', bbox=BoundingBox(l=134.76498022108473, t=539.8976429810075, r=480.5867307884124, b=549.8303530358503, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='neuron will still receive an input and perform a operation (such as a scalar', bbox=BoundingBox(l=134.76498022108473, t=551.8526330470162, r=480.58667078841233, b=561.7853331018589, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='product followed by a non-linear function) - the basis of countless ANNs. From', bbox=BoundingBox(l=134.76498022108473, t=563.8076131130247, r=480.5866407884123, b=573.7403231678675, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='the input raw image vectors to the final output of the class score, the entire of', bbox=BoundingBox(l=134.76498022108473, t=575.7626031790335, r=480.5866407884123, b=585.695313233876, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='the network will still express a single perceptive score function (the weight).', bbox=BoundingBox(l=134.76498022108473, t=587.7185932450475, r=480.58688078841266, b=597.6513032998903, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='The last layer will contain loss functions associated with the classes, and all of', bbox=BoundingBox(l=134.76498022108473, t=599.6735933110563, r=480.58682078841264, b=609.6063033658991, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='the regular tips and tricks developed for traditional ANNs still apply.', bbox=BoundingBox(l=134.76498022108473, t=611.628603377065, r=439.90955072168066, b=621.5613134319078, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Convolutional Neural Networks (CNNs) are analogous to traditional ANNs in that they are comprised of neurons that self-optimise through learning. Each neuron will still receive an input and perform a operation (such as a scalar product followed by a non-linear function) - the basis of countless ANNs. From the input raw image vectors to the final output of the class score, the entire of the network will still express a single perceptive score function (the weight). The last layer will contain loss functions associated with the classes, and all of the regular tips and tricks developed for traditional ANNs still apply.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=1, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.68446350097656, t=631.9942626953125, r=480.5868507884127, b=666.78271484375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9788795113563538, cells=[Cell(id=31, text='The only notable difference between CNNs and traditional ANNs is that CNNs', bbox=BoundingBox(l=134.76498022108473, t=632.885603494434, r=480.5868507884127, b=642.8183135492768, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='are primarily used in the field of pattern recognition within images. This allows', bbox=BoundingBox(l=134.76498022108473, t=644.8406035604428, r=480.58679078841254, b=654.7733136152856, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='us to encode image-specific features into the architecture, making the network', bbox=BoundingBox(l=134.76498022108473, t=656.7956036264516, r=480.58682078841264, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The only notable difference between CNNs and traditional ANNs is that CNNs are primarily used in the field of pattern recognition within images. This allows us to encode image-specific features into the architecture, making the network'), FigureElement(label=<DocItemLabel.PICTURE: 'picture'>, id=7, page_no=1, cluster=Cluster(id=7, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=193.8206024169922, t=115.9693374633789, r=420.3355407714844, b=267.38494873046875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9722045063972473, cells=[]), text='', annotations=[], provenance=None, predicted_class=None, confidence=None)], headers=[TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=1, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=133.6807861328125, t=93.64701080322266, r=139.58908081054688, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7250283360481262, cells=[Cell(id=0, text='2', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=1, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=167.17018127441406, t=93.5260238647461, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.6552881002426147, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea et al.')])), Page(page_no=2, size=Size(width=595.2760009765625, height=841.8900146484375), cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=1, text='3', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=2, text='more suited for image-focused tasks - whilst further reducing the parameters', bbox=BoundingBox(l=134.76501022108476, t=119.67181066075966, r=480.58670078841243, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='required to set up the model.', bbox=BoundingBox(l=134.76501022108476, t=131.6268307267684, r=262.07709042994287, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='One of the largest limitations of traditional forms of ANN is that they tend to', bbox=BoundingBox(l=134.76501022108476, t=151.12481083442503, r=480.5867307884124, b=161.0574908892678, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='struggle with the computational complexity required to compute image data.', bbox=BoundingBox(l=134.76501022108476, t=163.079830900434, r=480.58679078841254, b=173.01251095527653, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='Common machine learning benchmarking datasets such as the MNIST database', bbox=BoundingBox(l=134.76501022108476, t=175.03485096644295, r=481.443480789818, b=184.9675210212854, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='of handwritten digits are suitable for most forms of ANN, due to its relatively', bbox=BoundingBox(l=134.76501022108476, t=186.98986103245147, r=480.58670078841243, b=196.92254108729412, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='small image dimensionality of just', bbox=BoundingBox(l=134.76501022108476, t=198.94586109846568, r=286.20648046952766, b=208.87854115330833, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='28', bbox=BoundingBox(l=288.66400047355927, t=199.15502109962063, r=298.6265904899031, b=208.00183114846766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='×', bbox=BoundingBox(l=300.6990104933029, t=198.5971610965405, r=308.44791050601515, b=208.00183114846766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='28', bbox=BoundingBox(l=310.5200205094145, t=199.15502109962063, r=320.4826005257583, b=208.00183114846766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='. With this dataset a single neuron in', bbox=BoundingBox(l=320.48203052575735, t=198.94586109846568, r=480.59103078841946, b=208.87854115330833, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='the first hidden layer will contain', bbox=BoundingBox(l=134.7650302210848, t=210.90087116447455, r=280.3784504599666, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='784', bbox=BoundingBox(l=282.62103046364564, t=211.11004116562947, r=297.56494048816137, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='weights (', bbox=BoundingBox(l=299.80502049183633, t=210.90087116447455, r=340.1535305580289, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='28', bbox=BoundingBox(l=340.15204055802644, t=211.11004116562947, r=350.11462057437024, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='×', bbox=BoundingBox(l=351.40204057648225, t=210.55218116254923, r=359.15094058919453, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='28', bbox=BoundingBox(l=360.438050591306, t=211.11004116562947, r=370.4006306076499, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='×', bbox=BoundingBox(l=371.68704060976023, t=210.55218116254923, r=379.4359406224725, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='1', bbox=BoundingBox(l=380.723050624584, t=211.11004116562947, r=385.7043506327559, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='where', bbox=BoundingBox(l=387.9440606364302, t=210.90087116447455, r=415.35120068139224, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='1', bbox=BoundingBox(l=417.5920706850684, t=211.11004116562947, r=422.5733606932403, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='bare in mind', bbox=BoundingBox(l=424.81409069691625, t=210.90087116447455, r=480.59470078842554, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='that MNIST is normalised to just black and white values), which is manageable', bbox=BoundingBox(l=134.7650802210849, t=222.85589123048328, r=480.58676078841245, b=232.78857128532593, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='for most forms of ANN.', bbox=BoundingBox(l=134.7650802210849, t=234.81091129649212, r=240.3985103943787, b=244.74359135133477, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='If you consider a more substantial coloured image input of', bbox=BoundingBox(l=134.7650802210849, t=254.30889140414888, r=391.39169064208613, b=264.2415714589914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='64', bbox=BoundingBox(l=393.7400806459387, t=254.5180614053039, r=403.70267066228257, b=263.36486145415074, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='×', bbox=BoundingBox(l=405.381070665036, t=253.96020140222367, r=413.12997067774825, b=263.36486145415074, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='64', bbox=BoundingBox(l=414.80908068050286, t=254.5180614053039, r=424.7716706968467, b=263.36486145415074, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text=', the number', bbox=BoundingBox(l=424.7710906968457, t=254.30889140414888, r=480.5915207884203, b=264.2415714589914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='of weights on just a single neuron of the first layer increases substantially to', bbox=BoundingBox(l=134.7650802210849, t=266.26391147015784, r=480.58682078841264, b=276.1965915250004, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='12', bbox=BoundingBox(l=134.7650802210849, t=278.42907153732676, r=144.72768023742873, b=287.2758715861738, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text=',', bbox=BoundingBox(l=144.72708023742774, t=278.42907153732676, r=147.49469024196807, b=287.2758715861738, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='288', bbox=BoundingBox(l=149.155080244692, t=278.42907153732676, r=164.09897026920777, b=287.2758715861738, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='. Also take into account that to deal with this scale of input, the network', bbox=BoundingBox(l=164.09908026920792, t=278.21991153617194, r=480.5909407884193, b=288.1525815910145, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='will also need to be a lot larger than one used to classify colour-normalised', bbox=BoundingBox(l=134.7650802210849, t=290.1749216021807, r=480.5868507884127, b=300.10760165702334, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='MNIST digits, then you will understand the drawbacks of using such models.', bbox=BoundingBox(l=134.7650802210849, t=302.12994166818953, r=476.6915007820223, b=312.0626217230322, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='1.1', bbox=BoundingBox(l=134.7650802210849, t=342.7606518925292, r=147.2183202415147, b=352.4642319461069, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='Overfitting', bbox=BoundingBox(l=157.1809202578585, t=342.7606518925292, r=207.5418703404767, b=352.4642319461069, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='But why does it matter? Surely we could just increase the number of hidden lay-', bbox=BoundingBox(l=134.7650802210849, t=373.28991206109447, r=480.58679078841254, b=383.22262211593716, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='ers in our network, and perhaps increase the number of neurons within them?', bbox=BoundingBox(l=134.7650802210849, t=385.2449021271031, r=480.58682078841264, b=395.1776121819458, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='The simple answer to this question is no. This is down to two reasons, one be-', bbox=BoundingBox(l=134.7650802210849, t=397.1998921931118, r=480.58682078841264, b=407.13259224795445, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='ing the simple problem of not having unlimited computational power and time', bbox=BoundingBox(l=134.7650802210849, t=409.15487225912034, r=480.5868507884127, b=419.0875823139631, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='to train these huge ANNs.', bbox=BoundingBox(l=134.7650802210849, t=421.11087232513455, r=250.23160041051008, b=431.04357237997726, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='The second reason is stopping or reducing the effects of overfitting.', bbox=BoundingBox(l=134.7650802210849, t=440.6088524327913, r=427.9843107021171, b=450.54156248763405, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='Overfitting', bbox=BoundingBox(l=430.231080705803, t=440.6785824331763, r=480.5920107884211, b=450.382172486754, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='is basically when a network is unable to learn effectively due to a number of', bbox=BoundingBox(l=134.7650802210849, t=452.5638424988, r=480.5868507884127, b=462.49655255364274, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='reasons. It is an important concept of most, if not all machine learning algo-', bbox=BoundingBox(l=134.7650802210849, t=464.51882256480854, r=480.58682078841264, b=474.4515326196513, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='rithms and it is important that every precaution is taken as to reduce its effects.', bbox=BoundingBox(l=134.7650802210849, t=476.4738126308172, r=480.58691078841275, b=486.4065226856599, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='If our models were to exhibit signs of overfitting then we may see a reduced', bbox=BoundingBox(l=134.7650802210849, t=488.4288026968258, r=480.58691078841275, b=498.3615127516686, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='ability to pinpoint generalised features for not only our training dataset, but', bbox=BoundingBox(l=134.7650802210849, t=500.38479276284, r=480.58691078841275, b=510.3175028176828, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='also our test and prediction sets.', bbox=BoundingBox(l=134.7650802210849, t=512.3397828288487, r=276.7719704540501, b=522.2724928836915, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='This is the main reason behind reducing the complexity of our ANNs. The less', bbox=BoundingBox(l=134.7650802210849, t=531.8377629365054, r=480.58688078841266, b=541.7704729913482, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='parameters required to train, the less likely the network will overfit - and of', bbox=BoundingBox(l=134.7650802210849, t=543.792753002514, r=480.58679078841254, b=553.7254630573568, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='course, improve the predictive performance of the model.', bbox=BoundingBox(l=134.7650802210849, t=555.7477430685227, r=388.2833906369869, b=565.6804531233654, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='2', bbox=BoundingBox(l=134.7650802210849, t=596.9607232960773, r=140.74268023089127, b=608.6050833603708, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='CNN architecture', bbox=BoundingBox(l=152.697880250504, t=596.9607232960773, r=248.63835040789635, b=608.6050833603708, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='As noted earlier, CNNs primarily focus on the basis that the input will be com-', bbox=BoundingBox(l=134.7650802210849, t=632.8857434944348, r=480.58682078841264, b=642.8184535492776, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='prised of images. This focuses the architecture to be set up in way to best suit', bbox=BoundingBox(l=134.7650802210849, t=644.8407435604436, r=480.58679078841254, b=654.7734536152864, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='the need for dealing with the specific type of data.', bbox=BoundingBox(l=134.7650802210849, t=656.7957436264522, r=355.0182505824148, b=666.728453681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))], predictions=PagePredictions(layout=LayoutPrediction(clusters=[Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=255.9642333984375, t=93.53244018554688, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9313411116600037, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=475.16705322265625, t=93.53336334228516, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.873039722442627, cells=[Cell(id=1, text='3', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=134.2552032470703, t=118.86754608154297, r=480.58670078841243, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.974609911441803, cells=[Cell(id=2, text='more suited for image-focused tasks - whilst further reducing the parameters', bbox=BoundingBox(l=134.76501022108476, t=119.67181066075966, r=480.58670078841243, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='required to set up the model.', bbox=BoundingBox(l=134.76501022108476, t=131.6268307267684, r=262.07709042994287, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.69602966308594, t=150.32054138183594, r=481.443480789818, b=244.74359135133477, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9850017428398132, cells=[Cell(id=4, text='One of the largest limitations of traditional forms of ANN is that they tend to', bbox=BoundingBox(l=134.76501022108476, t=151.12481083442503, r=480.5867307884124, b=161.0574908892678, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='struggle with the computational complexity required to compute image data.', bbox=BoundingBox(l=134.76501022108476, t=163.079830900434, r=480.58679078841254, b=173.01251095527653, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='Common machine learning benchmarking datasets such as the MNIST database', bbox=BoundingBox(l=134.76501022108476, t=175.03485096644295, r=481.443480789818, b=184.9675210212854, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='of handwritten digits are suitable for most forms of ANN, due to its relatively', bbox=BoundingBox(l=134.76501022108476, t=186.98986103245147, r=480.58670078841243, b=196.92254108729412, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='small image dimensionality of just', bbox=BoundingBox(l=134.76501022108476, t=198.94586109846568, r=286.20648046952766, b=208.87854115330833, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='28', bbox=BoundingBox(l=288.66400047355927, t=199.15502109962063, r=298.6265904899031, b=208.00183114846766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='×', bbox=BoundingBox(l=300.6990104933029, t=198.5971610965405, r=308.44791050601515, b=208.00183114846766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='28', bbox=BoundingBox(l=310.5200205094145, t=199.15502109962063, r=320.4826005257583, b=208.00183114846766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='. With this dataset a single neuron in', bbox=BoundingBox(l=320.48203052575735, t=198.94586109846568, r=480.59103078841946, b=208.87854115330833, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='the first hidden layer will contain', bbox=BoundingBox(l=134.7650302210848, t=210.90087116447455, r=280.3784504599666, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='784', bbox=BoundingBox(l=282.62103046364564, t=211.11004116562947, r=297.56494048816137, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='weights (', bbox=BoundingBox(l=299.80502049183633, t=210.90087116447455, r=340.1535305580289, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='28', bbox=BoundingBox(l=340.15204055802644, t=211.11004116562947, r=350.11462057437024, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='×', bbox=BoundingBox(l=351.40204057648225, t=210.55218116254923, r=359.15094058919453, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='28', bbox=BoundingBox(l=360.438050591306, t=211.11004116562947, r=370.4006306076499, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='×', bbox=BoundingBox(l=371.68704060976023, t=210.55218116254923, r=379.4359406224725, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='1', bbox=BoundingBox(l=380.723050624584, t=211.11004116562947, r=385.7043506327559, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='where', bbox=BoundingBox(l=387.9440606364302, t=210.90087116447455, r=415.35120068139224, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='1', bbox=BoundingBox(l=417.5920706850684, t=211.11004116562947, r=422.5733606932403, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='bare in mind', bbox=BoundingBox(l=424.81409069691625, t=210.90087116447455, r=480.59470078842554, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='that MNIST is normalised to just black and white values), which is manageable', bbox=BoundingBox(l=134.7650802210849, t=222.85589123048328, r=480.58676078841245, b=232.78857128532593, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='for most forms of ANN.', bbox=BoundingBox(l=134.7650802210849, t=234.81091129649212, r=240.3985103943787, b=244.74359135133477, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.93231201171875, t=253.4578094482422, r=480.5915207884203, b=312.1922912597656, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9856253266334534, cells=[Cell(id=26, text='If you consider a more substantial coloured image input of', bbox=BoundingBox(l=134.7650802210849, t=254.30889140414888, r=391.39169064208613, b=264.2415714589914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='64', bbox=BoundingBox(l=393.7400806459387, t=254.5180614053039, r=403.70267066228257, b=263.36486145415074, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='×', bbox=BoundingBox(l=405.381070665036, t=253.96020140222367, r=413.12997067774825, b=263.36486145415074, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='64', bbox=BoundingBox(l=414.80908068050286, t=254.5180614053039, r=424.7716706968467, b=263.36486145415074, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text=', the number', bbox=BoundingBox(l=424.7710906968457, t=254.30889140414888, r=480.5915207884203, b=264.2415714589914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='of weights on just a single neuron of the first layer increases substantially to', bbox=BoundingBox(l=134.7650802210849, t=266.26391147015784, r=480.58682078841264, b=276.1965915250004, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='12', bbox=BoundingBox(l=134.7650802210849, t=278.42907153732676, r=144.72768023742873, b=287.2758715861738, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text=',', bbox=BoundingBox(l=144.72708023742774, t=278.42907153732676, r=147.49469024196807, b=287.2758715861738, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='288', bbox=BoundingBox(l=149.155080244692, t=278.42907153732676, r=164.09897026920777, b=287.2758715861738, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='. Also take into account that to deal with this scale of input, the network', bbox=BoundingBox(l=164.09908026920792, t=278.21991153617194, r=480.5909407884193, b=288.1525815910145, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='will also need to be a lot larger than one used to classify colour-normalised', bbox=BoundingBox(l=134.7650802210849, t=290.1749216021807, r=480.5868507884127, b=300.10760165702334, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='MNIST digits, then you will understand the drawbacks of using such models.', bbox=BoundingBox(l=134.7650802210849, t=302.12994166818953, r=476.6915007820223, b=312.0626217230322, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=5, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.107177734375, t=342.1416320800781, r=207.76548767089844, b=352.77398681640625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9495469331741333, cells=[Cell(id=38, text='1.1', bbox=BoundingBox(l=134.7650802210849, t=342.7606518925292, r=147.2183202415147, b=352.4642319461069, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='Overfitting', bbox=BoundingBox(l=157.1809202578585, t=342.7606518925292, r=207.5418703404767, b=352.4642319461069, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87643432617188, t=372.7414855957031, r=480.5868507884127, b=431.04357237997726, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9863153100013733, cells=[Cell(id=40, text='But why does it matter? Surely we could just increase the number of hidden lay-', bbox=BoundingBox(l=134.7650802210849, t=373.28991206109447, r=480.58679078841254, b=383.22262211593716, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='ers in our network, and perhaps increase the number of neurons within them?', bbox=BoundingBox(l=134.7650802210849, t=385.2449021271031, r=480.58682078841264, b=395.1776121819458, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='The simple answer to this question is no. This is down to two reasons, one be-', bbox=BoundingBox(l=134.7650802210849, t=397.1998921931118, r=480.58682078841264, b=407.13259224795445, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='ing the simple problem of not having unlimited computational power and time', bbox=BoundingBox(l=134.7650802210849, t=409.15487225912034, r=480.5868507884127, b=419.0875823139631, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='to train these huge ANNs.', bbox=BoundingBox(l=134.7650802210849, t=421.11087232513455, r=250.23160041051008, b=431.04357237997726, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.83572387695312, t=440.1903076171875, r=480.64300537109375, b=522.4312133789062, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9872767329216003, cells=[Cell(id=45, text='The second reason is stopping or reducing the effects of overfitting.', bbox=BoundingBox(l=134.7650802210849, t=440.6088524327913, r=427.9843107021171, b=450.54156248763405, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='Overfitting', bbox=BoundingBox(l=430.231080705803, t=440.6785824331763, r=480.5920107884211, b=450.382172486754, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='is basically when a network is unable to learn effectively due to a number of', bbox=BoundingBox(l=134.7650802210849, t=452.5638424988, r=480.5868507884127, b=462.49655255364274, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='reasons. It is an important concept of most, if not all machine learning algo-', bbox=BoundingBox(l=134.7650802210849, t=464.51882256480854, r=480.58682078841264, b=474.4515326196513, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='rithms and it is important that every precaution is taken as to reduce its effects.', bbox=BoundingBox(l=134.7650802210849, t=476.4738126308172, r=480.58691078841275, b=486.4065226856599, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='If our models were to exhibit signs of overfitting then we may see a reduced', bbox=BoundingBox(l=134.7650802210849, t=488.4288026968258, r=480.58691078841275, b=498.3615127516686, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='ability to pinpoint generalised features for not only our training dataset, but', bbox=BoundingBox(l=134.7650802210849, t=500.38479276284, r=480.58691078841275, b=510.3175028176828, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='also our test and prediction sets.', bbox=BoundingBox(l=134.7650802210849, t=512.3397828288487, r=276.7719704540501, b=522.2724928836915, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8554229736328, t=531.4302368164062, r=480.58688078841266, b=565.6804531233654, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9846621751785278, cells=[Cell(id=53, text='This is the main reason behind reducing the complexity of our ANNs. The less', bbox=BoundingBox(l=134.7650802210849, t=531.8377629365054, r=480.58688078841266, b=541.7704729913482, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='parameters required to train, the less likely the network will overfit - and of', bbox=BoundingBox(l=134.7650802210849, t=543.792753002514, r=480.58679078841254, b=553.7254630573568, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='course, improve the predictive performance of the model.', bbox=BoundingBox(l=134.7650802210849, t=555.7477430685227, r=388.2833906369869, b=565.6804531233654, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=9, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.10931396484375, t=596.1439819335938, r=248.64112854003906, b=608.6050833603708, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9415203332901001, cells=[Cell(id=56, text='2', bbox=BoundingBox(l=134.7650802210849, t=596.9607232960773, r=140.74268023089127, b=608.6050833603708, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='CNN architecture', bbox=BoundingBox(l=152.697880250504, t=596.9607232960773, r=248.63835040789635, b=608.6050833603708, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=10, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.93942260742188, t=631.9437255859375, r=480.8302001953125, b=666.7557373046875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9827980399131775, cells=[Cell(id=58, text='As noted earlier, CNNs primarily focus on the basis that the input will be com-', bbox=BoundingBox(l=134.7650802210849, t=632.8857434944348, r=480.58682078841264, b=642.8184535492776, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='prised of images. This focuses the architecture to be set up in way to best suit', bbox=BoundingBox(l=134.7650802210849, t=644.8407435604436, r=480.58679078841254, b=654.7734536152864, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='the need for dealing with the specific type of data.', bbox=BoundingBox(l=134.7650802210849, t=656.7957436264522, r=355.0182505824148, b=666.728453681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))])]), tablestructure=TableStructurePrediction(table_map={}), figures_classification=None, equations_prediction=None), assembled=AssembledUnit(elements=[TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=2, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=255.9642333984375, t=93.53244018554688, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9313411116600037, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=2, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=475.16705322265625, t=93.53336334228516, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.873039722442627, cells=[Cell(id=1, text='3', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='3'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=2, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=134.2552032470703, t=118.86754608154297, r=480.58670078841243, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.974609911441803, cells=[Cell(id=2, text='more suited for image-focused tasks - whilst further reducing the parameters', bbox=BoundingBox(l=134.76501022108476, t=119.67181066075966, r=480.58670078841243, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='required to set up the model.', bbox=BoundingBox(l=134.76501022108476, t=131.6268307267684, r=262.07709042994287, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='more suited for image-focused tasks - whilst further reducing the parameters required to set up the model.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=2, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.69602966308594, t=150.32054138183594, r=481.443480789818, b=244.74359135133477, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9850017428398132, cells=[Cell(id=4, text='One of the largest limitations of traditional forms of ANN is that they tend to', bbox=BoundingBox(l=134.76501022108476, t=151.12481083442503, r=480.5867307884124, b=161.0574908892678, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='struggle with the computational complexity required to compute image data.', bbox=BoundingBox(l=134.76501022108476, t=163.079830900434, r=480.58679078841254, b=173.01251095527653, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='Common machine learning benchmarking datasets such as the MNIST database', bbox=BoundingBox(l=134.76501022108476, t=175.03485096644295, r=481.443480789818, b=184.9675210212854, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='of handwritten digits are suitable for most forms of ANN, due to its relatively', bbox=BoundingBox(l=134.76501022108476, t=186.98986103245147, r=480.58670078841243, b=196.92254108729412, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='small image dimensionality of just', bbox=BoundingBox(l=134.76501022108476, t=198.94586109846568, r=286.20648046952766, b=208.87854115330833, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='28', bbox=BoundingBox(l=288.66400047355927, t=199.15502109962063, r=298.6265904899031, b=208.00183114846766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='×', bbox=BoundingBox(l=300.6990104933029, t=198.5971610965405, r=308.44791050601515, b=208.00183114846766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='28', bbox=BoundingBox(l=310.5200205094145, t=199.15502109962063, r=320.4826005257583, b=208.00183114846766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='. With this dataset a single neuron in', bbox=BoundingBox(l=320.48203052575735, t=198.94586109846568, r=480.59103078841946, b=208.87854115330833, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='the first hidden layer will contain', bbox=BoundingBox(l=134.7650302210848, t=210.90087116447455, r=280.3784504599666, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='784', bbox=BoundingBox(l=282.62103046364564, t=211.11004116562947, r=297.56494048816137, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='weights (', bbox=BoundingBox(l=299.80502049183633, t=210.90087116447455, r=340.1535305580289, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='28', bbox=BoundingBox(l=340.15204055802644, t=211.11004116562947, r=350.11462057437024, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='×', bbox=BoundingBox(l=351.40204057648225, t=210.55218116254923, r=359.15094058919453, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='28', bbox=BoundingBox(l=360.438050591306, t=211.11004116562947, r=370.4006306076499, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='×', bbox=BoundingBox(l=371.68704060976023, t=210.55218116254923, r=379.4359406224725, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='1', bbox=BoundingBox(l=380.723050624584, t=211.11004116562947, r=385.7043506327559, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='where', bbox=BoundingBox(l=387.9440606364302, t=210.90087116447455, r=415.35120068139224, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='1', bbox=BoundingBox(l=417.5920706850684, t=211.11004116562947, r=422.5733606932403, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='bare in mind', bbox=BoundingBox(l=424.81409069691625, t=210.90087116447455, r=480.59470078842554, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='that MNIST is normalised to just black and white values), which is manageable', bbox=BoundingBox(l=134.7650802210849, t=222.85589123048328, r=480.58676078841245, b=232.78857128532593, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='for most forms of ANN.', bbox=BoundingBox(l=134.7650802210849, t=234.81091129649212, r=240.3985103943787, b=244.74359135133477, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='One of the largest limitations of traditional forms of ANN is that they tend to struggle with the computational complexity required to compute image data. Common machine learning benchmarking datasets such as the MNIST database of handwritten digits are suitable for most forms of ANN, due to its relatively small image dimensionality of just 28 × 28 . With this dataset a single neuron in the first hidden layer will contain 784 weights ( 28 × 28 × 1 where 1 bare in mind that MNIST is normalised to just black and white values), which is manageable for most forms of ANN.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=4, page_no=2, cluster=Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.93231201171875, t=253.4578094482422, r=480.5915207884203, b=312.1922912597656, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9856253266334534, cells=[Cell(id=26, text='If you consider a more substantial coloured image input of', bbox=BoundingBox(l=134.7650802210849, t=254.30889140414888, r=391.39169064208613, b=264.2415714589914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='64', bbox=BoundingBox(l=393.7400806459387, t=254.5180614053039, r=403.70267066228257, b=263.36486145415074, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='×', bbox=BoundingBox(l=405.381070665036, t=253.96020140222367, r=413.12997067774825, b=263.36486145415074, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='64', bbox=BoundingBox(l=414.80908068050286, t=254.5180614053039, r=424.7716706968467, b=263.36486145415074, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text=', the number', bbox=BoundingBox(l=424.7710906968457, t=254.30889140414888, r=480.5915207884203, b=264.2415714589914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='of weights on just a single neuron of the first layer increases substantially to', bbox=BoundingBox(l=134.7650802210849, t=266.26391147015784, r=480.58682078841264, b=276.1965915250004, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='12', bbox=BoundingBox(l=134.7650802210849, t=278.42907153732676, r=144.72768023742873, b=287.2758715861738, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text=',', bbox=BoundingBox(l=144.72708023742774, t=278.42907153732676, r=147.49469024196807, b=287.2758715861738, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='288', bbox=BoundingBox(l=149.155080244692, t=278.42907153732676, r=164.09897026920777, b=287.2758715861738, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='. Also take into account that to deal with this scale of input, the network', bbox=BoundingBox(l=164.09908026920792, t=278.21991153617194, r=480.5909407884193, b=288.1525815910145, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='will also need to be a lot larger than one used to classify colour-normalised', bbox=BoundingBox(l=134.7650802210849, t=290.1749216021807, r=480.5868507884127, b=300.10760165702334, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='MNIST digits, then you will understand the drawbacks of using such models.', bbox=BoundingBox(l=134.7650802210849, t=302.12994166818953, r=476.6915007820223, b=312.0626217230322, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='If you consider a more substantial coloured image input of 64 × 64 , the number of weights on just a single neuron of the first layer increases substantially to 12 , 288 . Also take into account that to deal with this scale of input, the network will also need to be a lot larger than one used to classify colour-normalised MNIST digits, then you will understand the drawbacks of using such models.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=5, page_no=2, cluster=Cluster(id=5, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.107177734375, t=342.1416320800781, r=207.76548767089844, b=352.77398681640625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9495469331741333, cells=[Cell(id=38, text='1.1', bbox=BoundingBox(l=134.7650802210849, t=342.7606518925292, r=147.2183202415147, b=352.4642319461069, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='Overfitting', bbox=BoundingBox(l=157.1809202578585, t=342.7606518925292, r=207.5418703404767, b=352.4642319461069, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='1.1 Overfitting'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=2, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87643432617188, t=372.7414855957031, r=480.5868507884127, b=431.04357237997726, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9863153100013733, cells=[Cell(id=40, text='But why does it matter? Surely we could just increase the number of hidden lay-', bbox=BoundingBox(l=134.7650802210849, t=373.28991206109447, r=480.58679078841254, b=383.22262211593716, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='ers in our network, and perhaps increase the number of neurons within them?', bbox=BoundingBox(l=134.7650802210849, t=385.2449021271031, r=480.58682078841264, b=395.1776121819458, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='The simple answer to this question is no. This is down to two reasons, one be-', bbox=BoundingBox(l=134.7650802210849, t=397.1998921931118, r=480.58682078841264, b=407.13259224795445, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='ing the simple problem of not having unlimited computational power and time', bbox=BoundingBox(l=134.7650802210849, t=409.15487225912034, r=480.5868507884127, b=419.0875823139631, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='to train these huge ANNs.', bbox=BoundingBox(l=134.7650802210849, t=421.11087232513455, r=250.23160041051008, b=431.04357237997726, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='But why does it matter? Surely we could just increase the number of hidden layers in our network, and perhaps increase the number of neurons within them? The simple answer to this question is no. This is down to two reasons, one being the simple problem of not having unlimited computational power and time to train these huge ANNs.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=2, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.83572387695312, t=440.1903076171875, r=480.64300537109375, b=522.4312133789062, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9872767329216003, cells=[Cell(id=45, text='The second reason is stopping or reducing the effects of overfitting.', bbox=BoundingBox(l=134.7650802210849, t=440.6088524327913, r=427.9843107021171, b=450.54156248763405, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='Overfitting', bbox=BoundingBox(l=430.231080705803, t=440.6785824331763, r=480.5920107884211, b=450.382172486754, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='is basically when a network is unable to learn effectively due to a number of', bbox=BoundingBox(l=134.7650802210849, t=452.5638424988, r=480.5868507884127, b=462.49655255364274, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='reasons. It is an important concept of most, if not all machine learning algo-', bbox=BoundingBox(l=134.7650802210849, t=464.51882256480854, r=480.58682078841264, b=474.4515326196513, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='rithms and it is important that every precaution is taken as to reduce its effects.', bbox=BoundingBox(l=134.7650802210849, t=476.4738126308172, r=480.58691078841275, b=486.4065226856599, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='If our models were to exhibit signs of overfitting then we may see a reduced', bbox=BoundingBox(l=134.7650802210849, t=488.4288026968258, r=480.58691078841275, b=498.3615127516686, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='ability to pinpoint generalised features for not only our training dataset, but', bbox=BoundingBox(l=134.7650802210849, t=500.38479276284, r=480.58691078841275, b=510.3175028176828, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='also our test and prediction sets.', bbox=BoundingBox(l=134.7650802210849, t=512.3397828288487, r=276.7719704540501, b=522.2724928836915, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The second reason is stopping or reducing the effects of overfitting. Overfitting is basically when a network is unable to learn effectively due to a number of reasons. It is an important concept of most, if not all machine learning algorithms and it is important that every precaution is taken as to reduce its effects. If our models were to exhibit signs of overfitting then we may see a reduced ability to pinpoint generalised features for not only our training dataset, but also our test and prediction sets.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=2, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8554229736328, t=531.4302368164062, r=480.58688078841266, b=565.6804531233654, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9846621751785278, cells=[Cell(id=53, text='This is the main reason behind reducing the complexity of our ANNs. The less', bbox=BoundingBox(l=134.7650802210849, t=531.8377629365054, r=480.58688078841266, b=541.7704729913482, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='parameters required to train, the less likely the network will overfit - and of', bbox=BoundingBox(l=134.7650802210849, t=543.792753002514, r=480.58679078841254, b=553.7254630573568, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='course, improve the predictive performance of the model.', bbox=BoundingBox(l=134.7650802210849, t=555.7477430685227, r=388.2833906369869, b=565.6804531233654, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='This is the main reason behind reducing the complexity of our ANNs. The less parameters required to train, the less likely the network will overfit - and of course, improve the predictive performance of the model.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=9, page_no=2, cluster=Cluster(id=9, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.10931396484375, t=596.1439819335938, r=248.64112854003906, b=608.6050833603708, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9415203332901001, cells=[Cell(id=56, text='2', bbox=BoundingBox(l=134.7650802210849, t=596.9607232960773, r=140.74268023089127, b=608.6050833603708, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='CNN architecture', bbox=BoundingBox(l=152.697880250504, t=596.9607232960773, r=248.63835040789635, b=608.6050833603708, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2 CNN architecture'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=10, page_no=2, cluster=Cluster(id=10, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.93942260742188, t=631.9437255859375, r=480.8302001953125, b=666.7557373046875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9827980399131775, cells=[Cell(id=58, text='As noted earlier, CNNs primarily focus on the basis that the input will be com-', bbox=BoundingBox(l=134.7650802210849, t=632.8857434944348, r=480.58682078841264, b=642.8184535492776, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='prised of images. This focuses the architecture to be set up in way to best suit', bbox=BoundingBox(l=134.7650802210849, t=644.8407435604436, r=480.58679078841254, b=654.7734536152864, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='the need for dealing with the specific type of data.', bbox=BoundingBox(l=134.7650802210849, t=656.7957436264522, r=355.0182505824148, b=666.728453681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='As noted earlier, CNNs primarily focus on the basis that the input will be comprised of images. This focuses the architecture to be set up in way to best suit the need for dealing with the specific type of data.')], body=[TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=2, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=134.2552032470703, t=118.86754608154297, r=480.58670078841243, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.974609911441803, cells=[Cell(id=2, text='more suited for image-focused tasks - whilst further reducing the parameters', bbox=BoundingBox(l=134.76501022108476, t=119.67181066075966, r=480.58670078841243, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='required to set up the model.', bbox=BoundingBox(l=134.76501022108476, t=131.6268307267684, r=262.07709042994287, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='more suited for image-focused tasks - whilst further reducing the parameters required to set up the model.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=2, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.69602966308594, t=150.32054138183594, r=481.443480789818, b=244.74359135133477, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9850017428398132, cells=[Cell(id=4, text='One of the largest limitations of traditional forms of ANN is that they tend to', bbox=BoundingBox(l=134.76501022108476, t=151.12481083442503, r=480.5867307884124, b=161.0574908892678, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='struggle with the computational complexity required to compute image data.', bbox=BoundingBox(l=134.76501022108476, t=163.079830900434, r=480.58679078841254, b=173.01251095527653, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='Common machine learning benchmarking datasets such as the MNIST database', bbox=BoundingBox(l=134.76501022108476, t=175.03485096644295, r=481.443480789818, b=184.9675210212854, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='of handwritten digits are suitable for most forms of ANN, due to its relatively', bbox=BoundingBox(l=134.76501022108476, t=186.98986103245147, r=480.58670078841243, b=196.92254108729412, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='small image dimensionality of just', bbox=BoundingBox(l=134.76501022108476, t=198.94586109846568, r=286.20648046952766, b=208.87854115330833, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='28', bbox=BoundingBox(l=288.66400047355927, t=199.15502109962063, r=298.6265904899031, b=208.00183114846766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='×', bbox=BoundingBox(l=300.6990104933029, t=198.5971610965405, r=308.44791050601515, b=208.00183114846766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='28', bbox=BoundingBox(l=310.5200205094145, t=199.15502109962063, r=320.4826005257583, b=208.00183114846766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='. With this dataset a single neuron in', bbox=BoundingBox(l=320.48203052575735, t=198.94586109846568, r=480.59103078841946, b=208.87854115330833, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='the first hidden layer will contain', bbox=BoundingBox(l=134.7650302210848, t=210.90087116447455, r=280.3784504599666, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='784', bbox=BoundingBox(l=282.62103046364564, t=211.11004116562947, r=297.56494048816137, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='weights (', bbox=BoundingBox(l=299.80502049183633, t=210.90087116447455, r=340.1535305580289, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='28', bbox=BoundingBox(l=340.15204055802644, t=211.11004116562947, r=350.11462057437024, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='×', bbox=BoundingBox(l=351.40204057648225, t=210.55218116254923, r=359.15094058919453, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='28', bbox=BoundingBox(l=360.438050591306, t=211.11004116562947, r=370.4006306076499, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='×', bbox=BoundingBox(l=371.68704060976023, t=210.55218116254923, r=379.4359406224725, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='1', bbox=BoundingBox(l=380.723050624584, t=211.11004116562947, r=385.7043506327559, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='where', bbox=BoundingBox(l=387.9440606364302, t=210.90087116447455, r=415.35120068139224, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='1', bbox=BoundingBox(l=417.5920706850684, t=211.11004116562947, r=422.5733606932403, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='bare in mind', bbox=BoundingBox(l=424.81409069691625, t=210.90087116447455, r=480.59470078842554, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='that MNIST is normalised to just black and white values), which is manageable', bbox=BoundingBox(l=134.7650802210849, t=222.85589123048328, r=480.58676078841245, b=232.78857128532593, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='for most forms of ANN.', bbox=BoundingBox(l=134.7650802210849, t=234.81091129649212, r=240.3985103943787, b=244.74359135133477, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='One of the largest limitations of traditional forms of ANN is that they tend to struggle with the computational complexity required to compute image data. Common machine learning benchmarking datasets such as the MNIST database of handwritten digits are suitable for most forms of ANN, due to its relatively small image dimensionality of just 28 × 28 . With this dataset a single neuron in the first hidden layer will contain 784 weights ( 28 × 28 × 1 where 1 bare in mind that MNIST is normalised to just black and white values), which is manageable for most forms of ANN.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=4, page_no=2, cluster=Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.93231201171875, t=253.4578094482422, r=480.5915207884203, b=312.1922912597656, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9856253266334534, cells=[Cell(id=26, text='If you consider a more substantial coloured image input of', bbox=BoundingBox(l=134.7650802210849, t=254.30889140414888, r=391.39169064208613, b=264.2415714589914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='64', bbox=BoundingBox(l=393.7400806459387, t=254.5180614053039, r=403.70267066228257, b=263.36486145415074, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='×', bbox=BoundingBox(l=405.381070665036, t=253.96020140222367, r=413.12997067774825, b=263.36486145415074, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='64', bbox=BoundingBox(l=414.80908068050286, t=254.5180614053039, r=424.7716706968467, b=263.36486145415074, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text=', the number', bbox=BoundingBox(l=424.7710906968457, t=254.30889140414888, r=480.5915207884203, b=264.2415714589914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='of weights on just a single neuron of the first layer increases substantially to', bbox=BoundingBox(l=134.7650802210849, t=266.26391147015784, r=480.58682078841264, b=276.1965915250004, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='12', bbox=BoundingBox(l=134.7650802210849, t=278.42907153732676, r=144.72768023742873, b=287.2758715861738, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text=',', bbox=BoundingBox(l=144.72708023742774, t=278.42907153732676, r=147.49469024196807, b=287.2758715861738, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='288', bbox=BoundingBox(l=149.155080244692, t=278.42907153732676, r=164.09897026920777, b=287.2758715861738, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='. Also take into account that to deal with this scale of input, the network', bbox=BoundingBox(l=164.09908026920792, t=278.21991153617194, r=480.5909407884193, b=288.1525815910145, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='will also need to be a lot larger than one used to classify colour-normalised', bbox=BoundingBox(l=134.7650802210849, t=290.1749216021807, r=480.5868507884127, b=300.10760165702334, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='MNIST digits, then you will understand the drawbacks of using such models.', bbox=BoundingBox(l=134.7650802210849, t=302.12994166818953, r=476.6915007820223, b=312.0626217230322, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='If you consider a more substantial coloured image input of 64 × 64 , the number of weights on just a single neuron of the first layer increases substantially to 12 , 288 . Also take into account that to deal with this scale of input, the network will also need to be a lot larger than one used to classify colour-normalised MNIST digits, then you will understand the drawbacks of using such models.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=5, page_no=2, cluster=Cluster(id=5, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.107177734375, t=342.1416320800781, r=207.76548767089844, b=352.77398681640625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9495469331741333, cells=[Cell(id=38, text='1.1', bbox=BoundingBox(l=134.7650802210849, t=342.7606518925292, r=147.2183202415147, b=352.4642319461069, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='Overfitting', bbox=BoundingBox(l=157.1809202578585, t=342.7606518925292, r=207.5418703404767, b=352.4642319461069, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='1.1 Overfitting'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=2, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87643432617188, t=372.7414855957031, r=480.5868507884127, b=431.04357237997726, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9863153100013733, cells=[Cell(id=40, text='But why does it matter? Surely we could just increase the number of hidden lay-', bbox=BoundingBox(l=134.7650802210849, t=373.28991206109447, r=480.58679078841254, b=383.22262211593716, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='ers in our network, and perhaps increase the number of neurons within them?', bbox=BoundingBox(l=134.7650802210849, t=385.2449021271031, r=480.58682078841264, b=395.1776121819458, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='The simple answer to this question is no. This is down to two reasons, one be-', bbox=BoundingBox(l=134.7650802210849, t=397.1998921931118, r=480.58682078841264, b=407.13259224795445, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='ing the simple problem of not having unlimited computational power and time', bbox=BoundingBox(l=134.7650802210849, t=409.15487225912034, r=480.5868507884127, b=419.0875823139631, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='to train these huge ANNs.', bbox=BoundingBox(l=134.7650802210849, t=421.11087232513455, r=250.23160041051008, b=431.04357237997726, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='But why does it matter? Surely we could just increase the number of hidden layers in our network, and perhaps increase the number of neurons within them? The simple answer to this question is no. This is down to two reasons, one being the simple problem of not having unlimited computational power and time to train these huge ANNs.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=2, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.83572387695312, t=440.1903076171875, r=480.64300537109375, b=522.4312133789062, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9872767329216003, cells=[Cell(id=45, text='The second reason is stopping or reducing the effects of overfitting.', bbox=BoundingBox(l=134.7650802210849, t=440.6088524327913, r=427.9843107021171, b=450.54156248763405, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='Overfitting', bbox=BoundingBox(l=430.231080705803, t=440.6785824331763, r=480.5920107884211, b=450.382172486754, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='is basically when a network is unable to learn effectively due to a number of', bbox=BoundingBox(l=134.7650802210849, t=452.5638424988, r=480.5868507884127, b=462.49655255364274, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='reasons. It is an important concept of most, if not all machine learning algo-', bbox=BoundingBox(l=134.7650802210849, t=464.51882256480854, r=480.58682078841264, b=474.4515326196513, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='rithms and it is important that every precaution is taken as to reduce its effects.', bbox=BoundingBox(l=134.7650802210849, t=476.4738126308172, r=480.58691078841275, b=486.4065226856599, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='If our models were to exhibit signs of overfitting then we may see a reduced', bbox=BoundingBox(l=134.7650802210849, t=488.4288026968258, r=480.58691078841275, b=498.3615127516686, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='ability to pinpoint generalised features for not only our training dataset, but', bbox=BoundingBox(l=134.7650802210849, t=500.38479276284, r=480.58691078841275, b=510.3175028176828, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='also our test and prediction sets.', bbox=BoundingBox(l=134.7650802210849, t=512.3397828288487, r=276.7719704540501, b=522.2724928836915, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The second reason is stopping or reducing the effects of overfitting. Overfitting is basically when a network is unable to learn effectively due to a number of reasons. It is an important concept of most, if not all machine learning algorithms and it is important that every precaution is taken as to reduce its effects. If our models were to exhibit signs of overfitting then we may see a reduced ability to pinpoint generalised features for not only our training dataset, but also our test and prediction sets.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=2, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8554229736328, t=531.4302368164062, r=480.58688078841266, b=565.6804531233654, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9846621751785278, cells=[Cell(id=53, text='This is the main reason behind reducing the complexity of our ANNs. The less', bbox=BoundingBox(l=134.7650802210849, t=531.8377629365054, r=480.58688078841266, b=541.7704729913482, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='parameters required to train, the less likely the network will overfit - and of', bbox=BoundingBox(l=134.7650802210849, t=543.792753002514, r=480.58679078841254, b=553.7254630573568, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='course, improve the predictive performance of the model.', bbox=BoundingBox(l=134.7650802210849, t=555.7477430685227, r=388.2833906369869, b=565.6804531233654, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='This is the main reason behind reducing the complexity of our ANNs. The less parameters required to train, the less likely the network will overfit - and of course, improve the predictive performance of the model.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=9, page_no=2, cluster=Cluster(id=9, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.10931396484375, t=596.1439819335938, r=248.64112854003906, b=608.6050833603708, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9415203332901001, cells=[Cell(id=56, text='2', bbox=BoundingBox(l=134.7650802210849, t=596.9607232960773, r=140.74268023089127, b=608.6050833603708, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='CNN architecture', bbox=BoundingBox(l=152.697880250504, t=596.9607232960773, r=248.63835040789635, b=608.6050833603708, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2 CNN architecture'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=10, page_no=2, cluster=Cluster(id=10, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.93942260742188, t=631.9437255859375, r=480.8302001953125, b=666.7557373046875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9827980399131775, cells=[Cell(id=58, text='As noted earlier, CNNs primarily focus on the basis that the input will be com-', bbox=BoundingBox(l=134.7650802210849, t=632.8857434944348, r=480.58682078841264, b=642.8184535492776, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='prised of images. This focuses the architecture to be set up in way to best suit', bbox=BoundingBox(l=134.7650802210849, t=644.8407435604436, r=480.58679078841254, b=654.7734536152864, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='the need for dealing with the specific type of data.', bbox=BoundingBox(l=134.7650802210849, t=656.7957436264522, r=355.0182505824148, b=666.728453681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='As noted earlier, CNNs primarily focus on the basis that the input will be comprised of images. This focuses the architecture to be set up in way to best suit the need for dealing with the specific type of data.')], headers=[TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=2, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=255.9642333984375, t=93.53244018554688, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9313411116600037, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=2, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=475.16705322265625, t=93.53336334228516, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.873039722442627, cells=[Cell(id=1, text='3', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='3')])), Page(page_no=3, size=Size(width=595.2760009765625, height=841.8900146484375), cells=[Cell(id=0, text='4', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=2, text='One of the key differences is that the neurons that the layers within the CNN', bbox=BoundingBox(l=134.76500022108476, t=119.67181066075966, r=480.58676078841245, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='are comprised of neurons organised into three dimensions, the spatial dimen-', bbox=BoundingBox(l=134.76500022108476, t=131.6268307267684, r=480.58676078841245, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='sionality of the input (', bbox=BoundingBox(l=134.76500022108476, t=143.58184079277714, r=231.18303037926052, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='height', bbox=BoundingBox(l=231.18500037926376, t=143.65155079316196, r=260.51489042738007, b=153.35510084673967, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='and the', bbox=BoundingBox(l=262.6799904309319, t=143.58184079277714, r=295.5266704848176, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='width', bbox=BoundingBox(l=297.6950104883748, t=143.65155079316196, r=324.80325053284645, b=153.35510084673967, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text=') and the', bbox=BoundingBox(l=324.803010532846, t=143.58184079277714, r=363.13907059573717, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='depth', bbox=BoundingBox(l=365.30103059928393, t=143.65155079316196, r=391.8613306428566, b=153.35510084673967, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='. The depth does not', bbox=BoundingBox(l=391.86203064285775, t=143.58184079277714, r=480.5889907884162, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='refer to the total number of layers within the ANN, but the third dimension of a', bbox=BoundingBox(l=134.76505022108483, t=155.53686085878599, r=480.58688078841266, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='activation volume. Unlike standard ANNS, the neurons within any given layer', bbox=BoundingBox(l=134.76505022108483, t=167.49188092479483, r=480.58679078841254, b=177.42456097963736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='will only connect to a small region of the layer preceding it.', bbox=BoundingBox(l=134.76505022108483, t=179.44787099080895, r=395.79517064931014, b=189.3805510456516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='In practice this would mean that for the example given earlier, the input ’vol-', bbox=BoundingBox(l=134.76505022108483, t=197.79687109212182, r=480.58679078841254, b=207.72955114696435, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='ume’ will have a dimensionality of', bbox=BoundingBox(l=134.76505022108483, t=209.75189115813055, r=287.34225047139086, b=219.6845712129732, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='64', bbox=BoundingBox(l=289.74603047533435, t=209.96105115928538, r=299.7086204916782, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='×', bbox=BoundingBox(l=301.60602049479087, t=209.40319115620525, r=309.35492050750315, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='64', bbox=BoundingBox(l=311.2510105106137, t=209.96105115928538, r=321.21359052695755, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='×', bbox=BoundingBox(l=323.1109905300703, t=209.40319115620525, r=330.85989054278247, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='3', bbox=BoundingBox(l=332.75699054589467, t=209.96105115928538, r=337.7382805540666, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='(height, width and depth), lead-', bbox=BoundingBox(l=340.14297055801154, t=209.75189115813055, r=480.5957307884272, b=219.6845712129732, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='ing to a final output layer comprised of a dimensionality of', bbox=BoundingBox(l=134.7649702210847, t=221.7069012241392, r=401.304350658348, b=231.63958127898184, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='1', bbox=BoundingBox(l=404.45795066352156, t=221.91607122529422, r=409.4392406716935, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='×', bbox=BoundingBox(l=412.142940676129, t=221.3582112222141, r=419.8918506888412, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='1', bbox=BoundingBox(l=422.59595069327736, t=221.91607122529422, r=427.5772407014493, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='×', bbox=BoundingBox(l=430.2819507058864, t=221.3582112222141, r=438.03085071859863, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='n', bbox=BoundingBox(l=440.7339507230331, t=221.91607122529422, r=446.7135007328427, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='(where', bbox=BoundingBox(l=449.86694073801596, t=221.7069012241392, r=480.59161078842044, b=231.63958127898184, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='n', bbox=BoundingBox(l=134.7649502210847, t=233.87207129130843, r=140.74451023089426, b=242.71887134015537, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='represents the possible number of classes) as we would have condensed the', bbox=BoundingBox(l=143.63596023563775, t=233.6629012901535, r=480.59103078841946, b=243.59558134499605, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='full input dimensionality into a smaller volume of class scores filed across the', bbox=BoundingBox(l=134.7649502210847, t=245.61792135616224, r=480.58670078841243, b=255.55059141100492, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='depth dimension.', bbox=BoundingBox(l=134.7649502210847, t=257.5729314221711, r=212.35367034837057, b=267.50561147701364, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='2.1', bbox=BoundingBox(l=134.7649502210847, t=294.7576216274838, r=147.2182002415145, b=304.46118168106136, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='Overall architecture', bbox=BoundingBox(l=157.18080025785835, t=294.7576216274838, r=247.65118040627686, b=304.46118168106136, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='CNNs are comprised of three types of layers. These are convolutional layers,', bbox=BoundingBox(l=134.7649502210847, t=321.8409417770224, r=480.5866407884123, b=331.77362183186506, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='pooling layers and', bbox=BoundingBox(l=134.7649502210847, t=333.7969018430365, r=219.5566303601872, b=343.7296118978793, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='fully-connected layers', bbox=BoundingBox(l=223.34396036640038, t=333.86663184342154, r=325.6299705342027, b=343.57022189699916, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='. When these layers are stacked, a', bbox=BoundingBox(l=325.6299705342027, t=333.7969018430365, r=480.5883207884151, b=343.7296118978793, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='CNN architecture has been formed. A simplified CNN architecture for MNIST', bbox=BoundingBox(l=134.7649702210847, t=345.7518919090451, r=480.58682078841264, b=355.684601963888, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='classification is illustrated in Figure 2.', bbox=BoundingBox(l=134.7649702210847, t=357.70687197505373, r=300.69211049329164, b=367.63958202989653, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='input', bbox=BoundingBox(l=208.48486034202372, t=477.9427126389276, r=223.0549303659262, b=485.8841526827757, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='0', bbox=BoundingBox(l=409.24475067137445, t=438.2520724197785, r=412.83917067727117, b=446.21008246371804, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='9', bbox=BoundingBox(l=409.24475067137445, t=470.3982525972714, r=412.83167067725884, b=478.3396626411194, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='convolution', bbox=BoundingBox(l=246.66763040466336, t=389.60748215119077, r=280.7496304605756, b=397.56549219513033, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text=' w/ReLu', bbox=BoundingBox(l=251.36052041236212, t=398.2413321988619, r=276.50482045361184, b=406.18280224271024, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='pooling', bbox=BoundingBox(l=289.15469047436426, t=397.5405221949925, r=311.1308005104165, b=405.49853223893206, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='output ', bbox=BoundingBox(l=401.81390065918396, t=483.1483726676703, r=421.985600692276, b=491.1063227116096, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='fully-connected', bbox=BoundingBox(l=313.20502051381925, t=495.8235127376552, r=358.2925405877863, b=503.7649527815033, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='w/ ReLu', bbox=BoundingBox(l=323.2782605303446, t=504.44409278525313, r=348.39026057154143, b=512.385492829101, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='fully-connected', bbox=BoundingBox(l=340.9916705594039, t=398.3609021995222, r=386.19403063355924, b=406.3023622433704, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='...', bbox=BoundingBox(l=408.3062106698348, t=454.31549250847155, r=413.68658067866136, b=462.2734925524111, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='Fig. 2: An simple CNN architecture, comprised of just five layers', bbox=BoundingBox(l=166.0520002724117, t=524.4947528959615, r=449.30869073710016, b=534.4274629508043, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='The basic functionality of the example CNN above can be broken down into', bbox=BoundingBox(l=134.76500022108476, t=560.3207430937722, r=480.5867307884124, b=570.2534431486149, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='four key areas.', bbox=BoundingBox(l=134.76500022108476, t=572.2757231597809, r=199.50195032728706, b=582.2084332146236, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='1.', bbox=BoundingBox(l=139.2480002284392, t=590.6257332610992, r=146.818050240858, b=600.5584433159419, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='As found in other forms of ANN, the', bbox=BoundingBox(l=149.34140024499763, t=590.6257332610992, r=314.51996051597655, b=600.5584433159419, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='input layer', bbox=BoundingBox(l=316.9819905200155, t=590.6954632614842, r=367.034090602127, b=600.3990433150618, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='will hold the pixel values', bbox=BoundingBox(l=369.49298060616087, t=590.6257332610992, r=480.5958907884275, b=600.5584433159419, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='of the image.', bbox=BoundingBox(l=151.7009702488686, t=602.5807333271079, r=208.7368503424371, b=612.5134433819507, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='2.', bbox=BoundingBox(l=139.24797022843916, t=620.9297334284206, r=147.41849024184307, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='The', bbox=BoundingBox(l=150.14200024631103, t=620.9297334284206, r=168.37860027622855, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='convolutional layer', bbox=BoundingBox(l=170.7909702801861, t=620.9994634288056, r=258.9898704248782, b=630.7030434823832, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='will determine the output of neurons of which are', bbox=BoundingBox(l=261.4029504288369, t=620.9297334284206, r=480.59009078841797, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='connected to local regions of the input through the calculation of the scalar', bbox=BoundingBox(l=151.70096024886857, t=632.8847334944293, r=480.5961907884279, b=642.817443549272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=65, text='product between their weights and the region connected to the input vol-', bbox=BoundingBox(l=151.70096024886857, t=644.8407235604434, r=480.59625078842805, b=654.7734336152862, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='ume. The', bbox=BoundingBox(l=151.70096024886857, t=656.7957336264523, r=192.50775031581293, b=666.728433681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='rectified linear unit', bbox=BoundingBox(l=194.56696031919108, t=656.8654636268373, r=281.1319904612028, b=666.5690436804149, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=68, text='(commonly shortened to ReLu) aims to apply', bbox=BoundingBox(l=283.19095046458057, t=656.7957336264523, r=480.58984078841746, b=666.728433681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))], predictions=PagePredictions(layout=LayoutPrediction(clusters=[Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=133.68788146972656, t=93.82508850097656, r=139.39495849609375, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.6813156604766846, cells=[Cell(id=0, text='4', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=1, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=-1.0, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.76507568359375, t=118.65596771240234, r=480.6532897949219, b=189.66046142578125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.986056923866272, cells=[Cell(id=2, text='One of the key differences is that the neurons that the layers within the CNN', bbox=BoundingBox(l=134.76500022108476, t=119.67181066075966, r=480.58676078841245, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='are comprised of neurons organised into three dimensions, the spatial dimen-', bbox=BoundingBox(l=134.76500022108476, t=131.6268307267684, r=480.58676078841245, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='sionality of the input (', bbox=BoundingBox(l=134.76500022108476, t=143.58184079277714, r=231.18303037926052, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='height', bbox=BoundingBox(l=231.18500037926376, t=143.65155079316196, r=260.51489042738007, b=153.35510084673967, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='and the', bbox=BoundingBox(l=262.6799904309319, t=143.58184079277714, r=295.5266704848176, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='width', bbox=BoundingBox(l=297.6950104883748, t=143.65155079316196, r=324.80325053284645, b=153.35510084673967, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text=') and the', bbox=BoundingBox(l=324.803010532846, t=143.58184079277714, r=363.13907059573717, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='depth', bbox=BoundingBox(l=365.30103059928393, t=143.65155079316196, r=391.8613306428566, b=153.35510084673967, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='. The depth does not', bbox=BoundingBox(l=391.86203064285775, t=143.58184079277714, r=480.5889907884162, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='refer to the total number of layers within the ANN, but the third dimension of a', bbox=BoundingBox(l=134.76505022108483, t=155.53686085878599, r=480.58688078841266, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='activation volume. Unlike standard ANNS, the neurons within any given layer', bbox=BoundingBox(l=134.76505022108483, t=167.49188092479483, r=480.58679078841254, b=177.42456097963736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='will only connect to a small region of the layer preceding it.', bbox=BoundingBox(l=134.76505022108483, t=179.44787099080895, r=395.79517064931014, b=189.3805510456516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.84022521972656, t=196.7172088623047, r=480.62689208984375, b=267.8779602050781, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9875405430793762, cells=[Cell(id=14, text='In practice this would mean that for the example given earlier, the input ’vol-', bbox=BoundingBox(l=134.76505022108483, t=197.79687109212182, r=480.58679078841254, b=207.72955114696435, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='ume’ will have a dimensionality of', bbox=BoundingBox(l=134.76505022108483, t=209.75189115813055, r=287.34225047139086, b=219.6845712129732, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='64', bbox=BoundingBox(l=289.74603047533435, t=209.96105115928538, r=299.7086204916782, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='×', bbox=BoundingBox(l=301.60602049479087, t=209.40319115620525, r=309.35492050750315, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='64', bbox=BoundingBox(l=311.2510105106137, t=209.96105115928538, r=321.21359052695755, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='×', bbox=BoundingBox(l=323.1109905300703, t=209.40319115620525, r=330.85989054278247, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='3', bbox=BoundingBox(l=332.75699054589467, t=209.96105115928538, r=337.7382805540666, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='(height, width and depth), lead-', bbox=BoundingBox(l=340.14297055801154, t=209.75189115813055, r=480.5957307884272, b=219.6845712129732, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='ing to a final output layer comprised of a dimensionality of', bbox=BoundingBox(l=134.7649702210847, t=221.7069012241392, r=401.304350658348, b=231.63958127898184, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='1', bbox=BoundingBox(l=404.45795066352156, t=221.91607122529422, r=409.4392406716935, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='×', bbox=BoundingBox(l=412.142940676129, t=221.3582112222141, r=419.8918506888412, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='1', bbox=BoundingBox(l=422.59595069327736, t=221.91607122529422, r=427.5772407014493, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='×', bbox=BoundingBox(l=430.2819507058864, t=221.3582112222141, r=438.03085071859863, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='n', bbox=BoundingBox(l=440.7339507230331, t=221.91607122529422, r=446.7135007328427, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='(where', bbox=BoundingBox(l=449.86694073801596, t=221.7069012241392, r=480.59161078842044, b=231.63958127898184, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='n', bbox=BoundingBox(l=134.7649502210847, t=233.87207129130843, r=140.74451023089426, b=242.71887134015537, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='represents the possible number of classes) as we would have condensed the', bbox=BoundingBox(l=143.63596023563775, t=233.6629012901535, r=480.59103078841946, b=243.59558134499605, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='full input dimensionality into a smaller volume of class scores filed across the', bbox=BoundingBox(l=134.7649502210847, t=245.61792135616224, r=480.58670078841243, b=255.55059141100492, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='depth dimension.', bbox=BoundingBox(l=134.7649502210847, t=257.5729314221711, r=212.35367034837057, b=267.50561147701364, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=4, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.0306854248047, t=294.2342224121094, r=247.65118040627686, b=304.46118168106136, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9402409791946411, cells=[Cell(id=33, text='2.1', bbox=BoundingBox(l=134.7649502210847, t=294.7576216274838, r=147.2182002415145, b=304.46118168106136, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='Overall architecture', bbox=BoundingBox(l=157.18080025785835, t=294.7576216274838, r=247.65118040627686, b=304.46118168106136, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.74267578125, t=321.2259826660156, r=480.75616455078125, b=367.63958202989653, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9827728271484375, cells=[Cell(id=35, text='CNNs are comprised of three types of layers. These are convolutional layers,', bbox=BoundingBox(l=134.7649502210847, t=321.8409417770224, r=480.5866407884123, b=331.77362183186506, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='pooling layers and', bbox=BoundingBox(l=134.7649502210847, t=333.7969018430365, r=219.5566303601872, b=343.7296118978793, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='fully-connected layers', bbox=BoundingBox(l=223.34396036640038, t=333.86663184342154, r=325.6299705342027, b=343.57022189699916, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='. When these layers are stacked, a', bbox=BoundingBox(l=325.6299705342027, t=333.7969018430365, r=480.5883207884151, b=343.7296118978793, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='CNN architecture has been formed. A simplified CNN architecture for MNIST', bbox=BoundingBox(l=134.7649702210847, t=345.7518919090451, r=480.58682078841264, b=355.684601963888, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='classification is illustrated in Figure 2.', bbox=BoundingBox(l=134.7649702210847, t=357.70687197505373, r=300.69211049329164, b=367.63958202989653, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=6, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=194.3790283203125, t=389.60748215119077, r=421.985600692276, b=512.5117797851562, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9499910473823547, cells=[Cell(id=41, text='input', bbox=BoundingBox(l=208.48486034202372, t=477.9427126389276, r=223.0549303659262, b=485.8841526827757, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='0', bbox=BoundingBox(l=409.24475067137445, t=438.2520724197785, r=412.83917067727117, b=446.21008246371804, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='9', bbox=BoundingBox(l=409.24475067137445, t=470.3982525972714, r=412.83167067725884, b=478.3396626411194, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='convolution', bbox=BoundingBox(l=246.66763040466336, t=389.60748215119077, r=280.7496304605756, b=397.56549219513033, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text=' w/ReLu', bbox=BoundingBox(l=251.36052041236212, t=398.2413321988619, r=276.50482045361184, b=406.18280224271024, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='pooling', bbox=BoundingBox(l=289.15469047436426, t=397.5405221949925, r=311.1308005104165, b=405.49853223893206, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='output ', bbox=BoundingBox(l=401.81390065918396, t=483.1483726676703, r=421.985600692276, b=491.1063227116096, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='fully-connected', bbox=BoundingBox(l=313.20502051381925, t=495.8235127376552, r=358.2925405877863, b=503.7649527815033, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='w/ ReLu', bbox=BoundingBox(l=323.2782605303446, t=504.44409278525313, r=348.39026057154143, b=512.385492829101, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='fully-connected', bbox=BoundingBox(l=340.9916705594039, t=398.3609021995222, r=386.19403063355924, b=406.3023622433704, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='...', bbox=BoundingBox(l=408.3062106698348, t=454.31549250847155, r=413.68658067866136, b=462.2734925524111, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=7, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=165.54881286621094, t=524.3402099609375, r=449.30869073710016, b=534.6312866210938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9542887210845947, cells=[Cell(id=52, text='Fig. 2: An simple CNN architecture, comprised of just five layers', bbox=BoundingBox(l=166.0520002724117, t=524.4947528959615, r=449.30869073710016, b=534.4274629508043, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.9955596923828, t=559.4691772460938, r=480.5867307884124, b=582.2084332146236, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.969181478023529, cells=[Cell(id=53, text='The basic functionality of the example CNN above can be broken down into', bbox=BoundingBox(l=134.76500022108476, t=560.3207430937722, r=480.5867307884124, b=570.2534431486149, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='four key areas.', bbox=BoundingBox(l=134.76500022108476, t=572.2757231597809, r=199.50195032728706, b=582.2084332146236, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=9, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.5282745361328, t=590.1478271484375, r=480.5958907884275, b=612.7933349609375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9722254276275635, cells=[Cell(id=55, text='1.', bbox=BoundingBox(l=139.2480002284392, t=590.6257332610992, r=146.818050240858, b=600.5584433159419, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='As found in other forms of ANN, the', bbox=BoundingBox(l=149.34140024499763, t=590.6257332610992, r=314.51996051597655, b=600.5584433159419, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='input layer', bbox=BoundingBox(l=316.9819905200155, t=590.6954632614842, r=367.034090602127, b=600.3990433150618, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='will hold the pixel values', bbox=BoundingBox(l=369.49298060616087, t=590.6257332610992, r=480.5958907884275, b=600.5584433159419, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='of the image.', bbox=BoundingBox(l=151.7009702488686, t=602.5807333271079, r=208.7368503424371, b=612.5134433819507, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=10, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.4064483642578, t=619.9978637695312, r=480.59625078842805, b=666.728433681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9786887168884277, cells=[Cell(id=60, text='2.', bbox=BoundingBox(l=139.24797022843916, t=620.9297334284206, r=147.41849024184307, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='The', bbox=BoundingBox(l=150.14200024631103, t=620.9297334284206, r=168.37860027622855, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='convolutional layer', bbox=BoundingBox(l=170.7909702801861, t=620.9994634288056, r=258.9898704248782, b=630.7030434823832, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='will determine the output of neurons of which are', bbox=BoundingBox(l=261.4029504288369, t=620.9297334284206, r=480.59009078841797, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='connected to local regions of the input through the calculation of the scalar', bbox=BoundingBox(l=151.70096024886857, t=632.8847334944293, r=480.5961907884279, b=642.817443549272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=65, text='product between their weights and the region connected to the input vol-', bbox=BoundingBox(l=151.70096024886857, t=644.8407235604434, r=480.59625078842805, b=654.7734336152862, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='ume. The', bbox=BoundingBox(l=151.70096024886857, t=656.7957336264523, r=192.50775031581293, b=666.728433681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='rectified linear unit', bbox=BoundingBox(l=194.56696031919108, t=656.8654636268373, r=281.1319904612028, b=666.5690436804149, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=68, text='(commonly shortened to ReLu) aims to apply', bbox=BoundingBox(l=283.19095046458057, t=656.7957336264523, r=480.58984078841746, b=666.728433681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))])]), tablestructure=TableStructurePrediction(table_map={}), figures_classification=None, equations_prediction=None), assembled=AssembledUnit(elements=[TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=3, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=133.68788146972656, t=93.82508850097656, r=139.39495849609375, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.6813156604766846, cells=[Cell(id=0, text='4', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='4'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=1, page_no=3, cluster=Cluster(id=1, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=-1.0, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea et al.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=3, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.76507568359375, t=118.65596771240234, r=480.6532897949219, b=189.66046142578125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.986056923866272, cells=[Cell(id=2, text='One of the key differences is that the neurons that the layers within the CNN', bbox=BoundingBox(l=134.76500022108476, t=119.67181066075966, r=480.58676078841245, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='are comprised of neurons organised into three dimensions, the spatial dimen-', bbox=BoundingBox(l=134.76500022108476, t=131.6268307267684, r=480.58676078841245, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='sionality of the input (', bbox=BoundingBox(l=134.76500022108476, t=143.58184079277714, r=231.18303037926052, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='height', bbox=BoundingBox(l=231.18500037926376, t=143.65155079316196, r=260.51489042738007, b=153.35510084673967, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='and the', bbox=BoundingBox(l=262.6799904309319, t=143.58184079277714, r=295.5266704848176, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='width', bbox=BoundingBox(l=297.6950104883748, t=143.65155079316196, r=324.80325053284645, b=153.35510084673967, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text=') and the', bbox=BoundingBox(l=324.803010532846, t=143.58184079277714, r=363.13907059573717, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='depth', bbox=BoundingBox(l=365.30103059928393, t=143.65155079316196, r=391.8613306428566, b=153.35510084673967, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='. The depth does not', bbox=BoundingBox(l=391.86203064285775, t=143.58184079277714, r=480.5889907884162, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='refer to the total number of layers within the ANN, but the third dimension of a', bbox=BoundingBox(l=134.76505022108483, t=155.53686085878599, r=480.58688078841266, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='activation volume. Unlike standard ANNS, the neurons within any given layer', bbox=BoundingBox(l=134.76505022108483, t=167.49188092479483, r=480.58679078841254, b=177.42456097963736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='will only connect to a small region of the layer preceding it.', bbox=BoundingBox(l=134.76505022108483, t=179.44787099080895, r=395.79517064931014, b=189.3805510456516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='One of the key differences is that the neurons that the layers within the CNN are comprised of neurons organised into three dimensions, the spatial dimensionality of the input ( height and the width ) and the depth . The depth does not refer to the total number of layers within the ANN, but the third dimension of a activation volume. Unlike standard ANNS, the neurons within any given layer will only connect to a small region of the layer preceding it.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=3, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.84022521972656, t=196.7172088623047, r=480.62689208984375, b=267.8779602050781, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9875405430793762, cells=[Cell(id=14, text='In practice this would mean that for the example given earlier, the input ’vol-', bbox=BoundingBox(l=134.76505022108483, t=197.79687109212182, r=480.58679078841254, b=207.72955114696435, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='ume’ will have a dimensionality of', bbox=BoundingBox(l=134.76505022108483, t=209.75189115813055, r=287.34225047139086, b=219.6845712129732, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='64', bbox=BoundingBox(l=289.74603047533435, t=209.96105115928538, r=299.7086204916782, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='×', bbox=BoundingBox(l=301.60602049479087, t=209.40319115620525, r=309.35492050750315, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='64', bbox=BoundingBox(l=311.2510105106137, t=209.96105115928538, r=321.21359052695755, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='×', bbox=BoundingBox(l=323.1109905300703, t=209.40319115620525, r=330.85989054278247, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='3', bbox=BoundingBox(l=332.75699054589467, t=209.96105115928538, r=337.7382805540666, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='(height, width and depth), lead-', bbox=BoundingBox(l=340.14297055801154, t=209.75189115813055, r=480.5957307884272, b=219.6845712129732, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='ing to a final output layer comprised of a dimensionality of', bbox=BoundingBox(l=134.7649702210847, t=221.7069012241392, r=401.304350658348, b=231.63958127898184, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='1', bbox=BoundingBox(l=404.45795066352156, t=221.91607122529422, r=409.4392406716935, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='×', bbox=BoundingBox(l=412.142940676129, t=221.3582112222141, r=419.8918506888412, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='1', bbox=BoundingBox(l=422.59595069327736, t=221.91607122529422, r=427.5772407014493, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='×', bbox=BoundingBox(l=430.2819507058864, t=221.3582112222141, r=438.03085071859863, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='n', bbox=BoundingBox(l=440.7339507230331, t=221.91607122529422, r=446.7135007328427, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='(where', bbox=BoundingBox(l=449.86694073801596, t=221.7069012241392, r=480.59161078842044, b=231.63958127898184, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='n', bbox=BoundingBox(l=134.7649502210847, t=233.87207129130843, r=140.74451023089426, b=242.71887134015537, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='represents the possible number of classes) as we would have condensed the', bbox=BoundingBox(l=143.63596023563775, t=233.6629012901535, r=480.59103078841946, b=243.59558134499605, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='full input dimensionality into a smaller volume of class scores filed across the', bbox=BoundingBox(l=134.7649502210847, t=245.61792135616224, r=480.58670078841243, b=255.55059141100492, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='depth dimension.', bbox=BoundingBox(l=134.7649502210847, t=257.5729314221711, r=212.35367034837057, b=267.50561147701364, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='In practice this would mean that for the example given earlier, the input ’volume’ will have a dimensionality of 64 × 64 × 3 (height, width and depth), leading to a final output layer comprised of a dimensionality of 1 × 1 × n (where n represents the possible number of classes) as we would have condensed the full input dimensionality into a smaller volume of class scores filed across the depth dimension.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=4, page_no=3, cluster=Cluster(id=4, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.0306854248047, t=294.2342224121094, r=247.65118040627686, b=304.46118168106136, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9402409791946411, cells=[Cell(id=33, text='2.1', bbox=BoundingBox(l=134.7649502210847, t=294.7576216274838, r=147.2182002415145, b=304.46118168106136, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='Overall architecture', bbox=BoundingBox(l=157.18080025785835, t=294.7576216274838, r=247.65118040627686, b=304.46118168106136, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2.1 Overall architecture'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=3, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.74267578125, t=321.2259826660156, r=480.75616455078125, b=367.63958202989653, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9827728271484375, cells=[Cell(id=35, text='CNNs are comprised of three types of layers. These are convolutional layers,', bbox=BoundingBox(l=134.7649502210847, t=321.8409417770224, r=480.5866407884123, b=331.77362183186506, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='pooling layers and', bbox=BoundingBox(l=134.7649502210847, t=333.7969018430365, r=219.5566303601872, b=343.7296118978793, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='fully-connected layers', bbox=BoundingBox(l=223.34396036640038, t=333.86663184342154, r=325.6299705342027, b=343.57022189699916, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='. When these layers are stacked, a', bbox=BoundingBox(l=325.6299705342027, t=333.7969018430365, r=480.5883207884151, b=343.7296118978793, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='CNN architecture has been formed. A simplified CNN architecture for MNIST', bbox=BoundingBox(l=134.7649702210847, t=345.7518919090451, r=480.58682078841264, b=355.684601963888, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='classification is illustrated in Figure 2.', bbox=BoundingBox(l=134.7649702210847, t=357.70687197505373, r=300.69211049329164, b=367.63958202989653, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='CNNs are comprised of three types of layers. These are convolutional layers, pooling layers and fully-connected layers . When these layers are stacked, a CNN architecture has been formed. A simplified CNN architecture for MNIST classification is illustrated in Figure 2.'), FigureElement(label=<DocItemLabel.PICTURE: 'picture'>, id=6, page_no=3, cluster=Cluster(id=6, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=194.3790283203125, t=389.60748215119077, r=421.985600692276, b=512.5117797851562, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9499910473823547, cells=[Cell(id=41, text='input', bbox=BoundingBox(l=208.48486034202372, t=477.9427126389276, r=223.0549303659262, b=485.8841526827757, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='0', bbox=BoundingBox(l=409.24475067137445, t=438.2520724197785, r=412.83917067727117, b=446.21008246371804, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='9', bbox=BoundingBox(l=409.24475067137445, t=470.3982525972714, r=412.83167067725884, b=478.3396626411194, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='convolution', bbox=BoundingBox(l=246.66763040466336, t=389.60748215119077, r=280.7496304605756, b=397.56549219513033, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text=' w/ReLu', bbox=BoundingBox(l=251.36052041236212, t=398.2413321988619, r=276.50482045361184, b=406.18280224271024, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='pooling', bbox=BoundingBox(l=289.15469047436426, t=397.5405221949925, r=311.1308005104165, b=405.49853223893206, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='output ', bbox=BoundingBox(l=401.81390065918396, t=483.1483726676703, r=421.985600692276, b=491.1063227116096, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='fully-connected', bbox=BoundingBox(l=313.20502051381925, t=495.8235127376552, r=358.2925405877863, b=503.7649527815033, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='w/ ReLu', bbox=BoundingBox(l=323.2782605303446, t=504.44409278525313, r=348.39026057154143, b=512.385492829101, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='fully-connected', bbox=BoundingBox(l=340.9916705594039, t=398.3609021995222, r=386.19403063355924, b=406.3023622433704, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='...', bbox=BoundingBox(l=408.3062106698348, t=454.31549250847155, r=413.68658067866136, b=462.2734925524111, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='', annotations=[], provenance=None, predicted_class=None, confidence=None), TextElement(label=<DocItemLabel.CAPTION: 'caption'>, id=7, page_no=3, cluster=Cluster(id=7, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=165.54881286621094, t=524.3402099609375, r=449.30869073710016, b=534.6312866210938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9542887210845947, cells=[Cell(id=52, text='Fig. 2: An simple CNN architecture, comprised of just five layers', bbox=BoundingBox(l=166.0520002724117, t=524.4947528959615, r=449.30869073710016, b=534.4274629508043, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Fig. 2: An simple CNN architecture, comprised of just five layers'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=3, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.9955596923828, t=559.4691772460938, r=480.5867307884124, b=582.2084332146236, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.969181478023529, cells=[Cell(id=53, text='The basic functionality of the example CNN above can be broken down into', bbox=BoundingBox(l=134.76500022108476, t=560.3207430937722, r=480.5867307884124, b=570.2534431486149, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='four key areas.', bbox=BoundingBox(l=134.76500022108476, t=572.2757231597809, r=199.50195032728706, b=582.2084332146236, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The basic functionality of the example CNN above can be broken down into four key areas.'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=9, page_no=3, cluster=Cluster(id=9, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.5282745361328, t=590.1478271484375, r=480.5958907884275, b=612.7933349609375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9722254276275635, cells=[Cell(id=55, text='1.', bbox=BoundingBox(l=139.2480002284392, t=590.6257332610992, r=146.818050240858, b=600.5584433159419, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='As found in other forms of ANN, the', bbox=BoundingBox(l=149.34140024499763, t=590.6257332610992, r=314.51996051597655, b=600.5584433159419, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='input layer', bbox=BoundingBox(l=316.9819905200155, t=590.6954632614842, r=367.034090602127, b=600.3990433150618, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='will hold the pixel values', bbox=BoundingBox(l=369.49298060616087, t=590.6257332610992, r=480.5958907884275, b=600.5584433159419, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='of the image.', bbox=BoundingBox(l=151.7009702488686, t=602.5807333271079, r=208.7368503424371, b=612.5134433819507, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='1. As found in other forms of ANN, the input layer will hold the pixel values of the image.'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=10, page_no=3, cluster=Cluster(id=10, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.4064483642578, t=619.9978637695312, r=480.59625078842805, b=666.728433681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9786887168884277, cells=[Cell(id=60, text='2.', bbox=BoundingBox(l=139.24797022843916, t=620.9297334284206, r=147.41849024184307, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='The', bbox=BoundingBox(l=150.14200024631103, t=620.9297334284206, r=168.37860027622855, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='convolutional layer', bbox=BoundingBox(l=170.7909702801861, t=620.9994634288056, r=258.9898704248782, b=630.7030434823832, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='will determine the output of neurons of which are', bbox=BoundingBox(l=261.4029504288369, t=620.9297334284206, r=480.59009078841797, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='connected to local regions of the input through the calculation of the scalar', bbox=BoundingBox(l=151.70096024886857, t=632.8847334944293, r=480.5961907884279, b=642.817443549272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=65, text='product between their weights and the region connected to the input vol-', bbox=BoundingBox(l=151.70096024886857, t=644.8407235604434, r=480.59625078842805, b=654.7734336152862, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='ume. The', bbox=BoundingBox(l=151.70096024886857, t=656.7957336264523, r=192.50775031581293, b=666.728433681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='rectified linear unit', bbox=BoundingBox(l=194.56696031919108, t=656.8654636268373, r=281.1319904612028, b=666.5690436804149, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=68, text='(commonly shortened to ReLu) aims to apply', bbox=BoundingBox(l=283.19095046458057, t=656.7957336264523, r=480.58984078841746, b=666.728433681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2. The convolutional layer will determine the output of neurons of which are connected to local regions of the input through the calculation of the scalar product between their weights and the region connected to the input volume. The rectified linear unit (commonly shortened to ReLu) aims to apply')], body=[TextElement(label=<DocItemLabel.TEXT: 'text'>, id=1, page_no=3, cluster=Cluster(id=1, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=-1.0, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea et al.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=3, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.76507568359375, t=118.65596771240234, r=480.6532897949219, b=189.66046142578125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.986056923866272, cells=[Cell(id=2, text='One of the key differences is that the neurons that the layers within the CNN', bbox=BoundingBox(l=134.76500022108476, t=119.67181066075966, r=480.58676078841245, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='are comprised of neurons organised into three dimensions, the spatial dimen-', bbox=BoundingBox(l=134.76500022108476, t=131.6268307267684, r=480.58676078841245, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='sionality of the input (', bbox=BoundingBox(l=134.76500022108476, t=143.58184079277714, r=231.18303037926052, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='height', bbox=BoundingBox(l=231.18500037926376, t=143.65155079316196, r=260.51489042738007, b=153.35510084673967, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='and the', bbox=BoundingBox(l=262.6799904309319, t=143.58184079277714, r=295.5266704848176, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='width', bbox=BoundingBox(l=297.6950104883748, t=143.65155079316196, r=324.80325053284645, b=153.35510084673967, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text=') and the', bbox=BoundingBox(l=324.803010532846, t=143.58184079277714, r=363.13907059573717, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='depth', bbox=BoundingBox(l=365.30103059928393, t=143.65155079316196, r=391.8613306428566, b=153.35510084673967, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='. The depth does not', bbox=BoundingBox(l=391.86203064285775, t=143.58184079277714, r=480.5889907884162, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='refer to the total number of layers within the ANN, but the third dimension of a', bbox=BoundingBox(l=134.76505022108483, t=155.53686085878599, r=480.58688078841266, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='activation volume. Unlike standard ANNS, the neurons within any given layer', bbox=BoundingBox(l=134.76505022108483, t=167.49188092479483, r=480.58679078841254, b=177.42456097963736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='will only connect to a small region of the layer preceding it.', bbox=BoundingBox(l=134.76505022108483, t=179.44787099080895, r=395.79517064931014, b=189.3805510456516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='One of the key differences is that the neurons that the layers within the CNN are comprised of neurons organised into three dimensions, the spatial dimensionality of the input ( height and the width ) and the depth . The depth does not refer to the total number of layers within the ANN, but the third dimension of a activation volume. Unlike standard ANNS, the neurons within any given layer will only connect to a small region of the layer preceding it.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=3, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.84022521972656, t=196.7172088623047, r=480.62689208984375, b=267.8779602050781, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9875405430793762, cells=[Cell(id=14, text='In practice this would mean that for the example given earlier, the input ’vol-', bbox=BoundingBox(l=134.76505022108483, t=197.79687109212182, r=480.58679078841254, b=207.72955114696435, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='ume’ will have a dimensionality of', bbox=BoundingBox(l=134.76505022108483, t=209.75189115813055, r=287.34225047139086, b=219.6845712129732, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='64', bbox=BoundingBox(l=289.74603047533435, t=209.96105115928538, r=299.7086204916782, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='×', bbox=BoundingBox(l=301.60602049479087, t=209.40319115620525, r=309.35492050750315, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='64', bbox=BoundingBox(l=311.2510105106137, t=209.96105115928538, r=321.21359052695755, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='×', bbox=BoundingBox(l=323.1109905300703, t=209.40319115620525, r=330.85989054278247, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='3', bbox=BoundingBox(l=332.75699054589467, t=209.96105115928538, r=337.7382805540666, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='(height, width and depth), lead-', bbox=BoundingBox(l=340.14297055801154, t=209.75189115813055, r=480.5957307884272, b=219.6845712129732, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='ing to a final output layer comprised of a dimensionality of', bbox=BoundingBox(l=134.7649702210847, t=221.7069012241392, r=401.304350658348, b=231.63958127898184, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='1', bbox=BoundingBox(l=404.45795066352156, t=221.91607122529422, r=409.4392406716935, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='×', bbox=BoundingBox(l=412.142940676129, t=221.3582112222141, r=419.8918506888412, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='1', bbox=BoundingBox(l=422.59595069327736, t=221.91607122529422, r=427.5772407014493, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='×', bbox=BoundingBox(l=430.2819507058864, t=221.3582112222141, r=438.03085071859863, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='n', bbox=BoundingBox(l=440.7339507230331, t=221.91607122529422, r=446.7135007328427, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='(where', bbox=BoundingBox(l=449.86694073801596, t=221.7069012241392, r=480.59161078842044, b=231.63958127898184, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='n', bbox=BoundingBox(l=134.7649502210847, t=233.87207129130843, r=140.74451023089426, b=242.71887134015537, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='represents the possible number of classes) as we would have condensed the', bbox=BoundingBox(l=143.63596023563775, t=233.6629012901535, r=480.59103078841946, b=243.59558134499605, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='full input dimensionality into a smaller volume of class scores filed across the', bbox=BoundingBox(l=134.7649502210847, t=245.61792135616224, r=480.58670078841243, b=255.55059141100492, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='depth dimension.', bbox=BoundingBox(l=134.7649502210847, t=257.5729314221711, r=212.35367034837057, b=267.50561147701364, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='In practice this would mean that for the example given earlier, the input ’volume’ will have a dimensionality of 64 × 64 × 3 (height, width and depth), leading to a final output layer comprised of a dimensionality of 1 × 1 × n (where n represents the possible number of classes) as we would have condensed the full input dimensionality into a smaller volume of class scores filed across the depth dimension.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=4, page_no=3, cluster=Cluster(id=4, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.0306854248047, t=294.2342224121094, r=247.65118040627686, b=304.46118168106136, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9402409791946411, cells=[Cell(id=33, text='2.1', bbox=BoundingBox(l=134.7649502210847, t=294.7576216274838, r=147.2182002415145, b=304.46118168106136, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='Overall architecture', bbox=BoundingBox(l=157.18080025785835, t=294.7576216274838, r=247.65118040627686, b=304.46118168106136, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2.1 Overall architecture'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=3, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.74267578125, t=321.2259826660156, r=480.75616455078125, b=367.63958202989653, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9827728271484375, cells=[Cell(id=35, text='CNNs are comprised of three types of layers. These are convolutional layers,', bbox=BoundingBox(l=134.7649502210847, t=321.8409417770224, r=480.5866407884123, b=331.77362183186506, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='pooling layers and', bbox=BoundingBox(l=134.7649502210847, t=333.7969018430365, r=219.5566303601872, b=343.7296118978793, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='fully-connected layers', bbox=BoundingBox(l=223.34396036640038, t=333.86663184342154, r=325.6299705342027, b=343.57022189699916, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='. When these layers are stacked, a', bbox=BoundingBox(l=325.6299705342027, t=333.7969018430365, r=480.5883207884151, b=343.7296118978793, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='CNN architecture has been formed. A simplified CNN architecture for MNIST', bbox=BoundingBox(l=134.7649702210847, t=345.7518919090451, r=480.58682078841264, b=355.684601963888, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='classification is illustrated in Figure 2.', bbox=BoundingBox(l=134.7649702210847, t=357.70687197505373, r=300.69211049329164, b=367.63958202989653, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='CNNs are comprised of three types of layers. These are convolutional layers, pooling layers and fully-connected layers . When these layers are stacked, a CNN architecture has been formed. A simplified CNN architecture for MNIST classification is illustrated in Figure 2.'), FigureElement(label=<DocItemLabel.PICTURE: 'picture'>, id=6, page_no=3, cluster=Cluster(id=6, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=194.3790283203125, t=389.60748215119077, r=421.985600692276, b=512.5117797851562, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9499910473823547, cells=[Cell(id=41, text='input', bbox=BoundingBox(l=208.48486034202372, t=477.9427126389276, r=223.0549303659262, b=485.8841526827757, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='0', bbox=BoundingBox(l=409.24475067137445, t=438.2520724197785, r=412.83917067727117, b=446.21008246371804, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='9', bbox=BoundingBox(l=409.24475067137445, t=470.3982525972714, r=412.83167067725884, b=478.3396626411194, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='convolution', bbox=BoundingBox(l=246.66763040466336, t=389.60748215119077, r=280.7496304605756, b=397.56549219513033, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text=' w/ReLu', bbox=BoundingBox(l=251.36052041236212, t=398.2413321988619, r=276.50482045361184, b=406.18280224271024, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='pooling', bbox=BoundingBox(l=289.15469047436426, t=397.5405221949925, r=311.1308005104165, b=405.49853223893206, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='output ', bbox=BoundingBox(l=401.81390065918396, t=483.1483726676703, r=421.985600692276, b=491.1063227116096, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='fully-connected', bbox=BoundingBox(l=313.20502051381925, t=495.8235127376552, r=358.2925405877863, b=503.7649527815033, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='w/ ReLu', bbox=BoundingBox(l=323.2782605303446, t=504.44409278525313, r=348.39026057154143, b=512.385492829101, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='fully-connected', bbox=BoundingBox(l=340.9916705594039, t=398.3609021995222, r=386.19403063355924, b=406.3023622433704, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='...', bbox=BoundingBox(l=408.3062106698348, t=454.31549250847155, r=413.68658067866136, b=462.2734925524111, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='', annotations=[], provenance=None, predicted_class=None, confidence=None), TextElement(label=<DocItemLabel.CAPTION: 'caption'>, id=7, page_no=3, cluster=Cluster(id=7, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=165.54881286621094, t=524.3402099609375, r=449.30869073710016, b=534.6312866210938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9542887210845947, cells=[Cell(id=52, text='Fig. 2: An simple CNN architecture, comprised of just five layers', bbox=BoundingBox(l=166.0520002724117, t=524.4947528959615, r=449.30869073710016, b=534.4274629508043, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Fig. 2: An simple CNN architecture, comprised of just five layers'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=3, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.9955596923828, t=559.4691772460938, r=480.5867307884124, b=582.2084332146236, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.969181478023529, cells=[Cell(id=53, text='The basic functionality of the example CNN above can be broken down into', bbox=BoundingBox(l=134.76500022108476, t=560.3207430937722, r=480.5867307884124, b=570.2534431486149, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='four key areas.', bbox=BoundingBox(l=134.76500022108476, t=572.2757231597809, r=199.50195032728706, b=582.2084332146236, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The basic functionality of the example CNN above can be broken down into four key areas.'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=9, page_no=3, cluster=Cluster(id=9, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.5282745361328, t=590.1478271484375, r=480.5958907884275, b=612.7933349609375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9722254276275635, cells=[Cell(id=55, text='1.', bbox=BoundingBox(l=139.2480002284392, t=590.6257332610992, r=146.818050240858, b=600.5584433159419, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='As found in other forms of ANN, the', bbox=BoundingBox(l=149.34140024499763, t=590.6257332610992, r=314.51996051597655, b=600.5584433159419, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='input layer', bbox=BoundingBox(l=316.9819905200155, t=590.6954632614842, r=367.034090602127, b=600.3990433150618, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='will hold the pixel values', bbox=BoundingBox(l=369.49298060616087, t=590.6257332610992, r=480.5958907884275, b=600.5584433159419, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='of the image.', bbox=BoundingBox(l=151.7009702488686, t=602.5807333271079, r=208.7368503424371, b=612.5134433819507, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='1. As found in other forms of ANN, the input layer will hold the pixel values of the image.'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=10, page_no=3, cluster=Cluster(id=10, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.4064483642578, t=619.9978637695312, r=480.59625078842805, b=666.728433681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9786887168884277, cells=[Cell(id=60, text='2.', bbox=BoundingBox(l=139.24797022843916, t=620.9297334284206, r=147.41849024184307, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='The', bbox=BoundingBox(l=150.14200024631103, t=620.9297334284206, r=168.37860027622855, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='convolutional layer', bbox=BoundingBox(l=170.7909702801861, t=620.9994634288056, r=258.9898704248782, b=630.7030434823832, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='will determine the output of neurons of which are', bbox=BoundingBox(l=261.4029504288369, t=620.9297334284206, r=480.59009078841797, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='connected to local regions of the input through the calculation of the scalar', bbox=BoundingBox(l=151.70096024886857, t=632.8847334944293, r=480.5961907884279, b=642.817443549272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=65, text='product between their weights and the region connected to the input vol-', bbox=BoundingBox(l=151.70096024886857, t=644.8407235604434, r=480.59625078842805, b=654.7734336152862, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='ume. The', bbox=BoundingBox(l=151.70096024886857, t=656.7957336264523, r=192.50775031581293, b=666.728433681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='rectified linear unit', bbox=BoundingBox(l=194.56696031919108, t=656.8654636268373, r=281.1319904612028, b=666.5690436804149, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=68, text='(commonly shortened to ReLu) aims to apply', bbox=BoundingBox(l=283.19095046458057, t=656.7957336264523, r=480.58984078841746, b=666.728433681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2. The convolutional layer will determine the output of neurons of which are connected to local regions of the input through the calculation of the scalar product between their weights and the region connected to the input volume. The rectified linear unit (commonly shortened to ReLu) aims to apply')], headers=[TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=3, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=133.68788146972656, t=93.82508850097656, r=139.39495849609375, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.6813156604766846, cells=[Cell(id=0, text='4', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='4')])), Page(page_no=4, size=Size(width=595.2760009765625, height=841.8900146484375), cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=1, text='5', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=2, text='an ’elementwise’ activation function such as sigmoid to the output of the', bbox=BoundingBox(l=151.70102024886864, t=119.67181066075966, r=480.5961907884279, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='activation produced by the previous layer.', bbox=BoundingBox(l=151.70102024886864, t=131.6268307267684, r=337.7227205540411, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='3.', bbox=BoundingBox(l=139.24802022843926, t=151.02282083386194, r=147.41855024184318, b=160.9555008887047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='The', bbox=BoundingBox(l=150.14204024631113, t=151.02282083386194, r=168.37866027622866, b=160.9555008887047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='pooling layer', bbox=BoundingBox(l=171.0980202806898, t=151.0925208342469, r=231.9396103805017, b=160.79608088782436, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='will then simply perform downsampling along the spa-', bbox=BoundingBox(l=234.6580203849613, t=151.02282083386194, r=480.59476078842556, b=160.9555008887047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='tial dimensionality of the given input, further reducing the number of pa-', bbox=BoundingBox(l=151.70102024886864, t=162.9778408998709, r=480.59641078842833, b=172.91052095471343, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='rameters within that activation.', bbox=BoundingBox(l=151.70102024886864, t=174.93383096588502, r=290.1014404759174, b=184.86651102072767, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='4.', bbox=BoundingBox(l=139.24802022843926, t=194.32983107297866, r=147.41855024184318, b=204.2625111278213, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='The', bbox=BoundingBox(l=150.14204024631113, t=194.32983107297866, r=168.37866027622866, b=204.2625111278213, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='fully-connected layers', bbox=BoundingBox(l=172.55103028307352, t=194.3995310733635, r=275.22559045151326, b=204.103081126941, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='will then perform the same duties found in', bbox=BoundingBox(l=279.3970304583566, t=194.32983107297866, r=480.5917107884205, b=204.2625111278213, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='standard ANNs and attempt to produce class scores from the activations,', bbox=BoundingBox(l=151.70103024886865, t=206.28485113898762, r=480.59628078842803, b=216.21752119383007, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='to be used for classification. It is also suggested that ReLu may be used', bbox=BoundingBox(l=151.70103024886865, t=218.23986120499615, r=480.5863007884118, b=228.1725412598389, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='between these layers, as to improve performance.', bbox=BoundingBox(l=151.70103024886865, t=230.1948812710051, r=369.3041406058511, b=240.12756132584775, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='Through this simple method of transformation, CNNs are able to transform', bbox=BoundingBox(l=134.7650302210848, t=249.59185137810402, r=480.5868507884127, b=259.5245314329467, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='the original input layer by layer using convolutional and downsampling tech-', bbox=BoundingBox(l=134.7650302210848, t=261.546871444113, r=480.58679078841254, b=271.47955149895563, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='niques to produce class scores for classification and regression purposes.', bbox=BoundingBox(l=134.7650302210848, t=273.5018915101218, r=453.4885907439574, b=283.43457156496436, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='Fig. 3: Activations taken from the first convolutional layer of a simplistic deep', bbox=BoundingBox(l=134.76500022108476, t=453.7317525052485, r=480.5866407884123, b=463.6644525600912, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='CNN, after training on the MNIST database of handwritten digits. If you look', bbox=BoundingBox(l=134.76500022108476, t=465.68673257125704, r=480.58679078841254, b=475.6194426260998, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='carefully, you can see that the network has successfully picked up on character-', bbox=BoundingBox(l=134.76500022108476, t=477.6417226372657, r=480.5867307884124, b=487.5744326921085, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='istics unique to specific numeric digits.', bbox=BoundingBox(l=134.76500022108476, t=489.59771270327985, r=305.6634505014472, b=499.53042275812265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='However, it is important to note that simply understanding the overall archi-', bbox=BoundingBox(l=134.76500022108476, t=526.4707329068717, r=480.5867307884124, b=536.4034429617145, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='tecture of a CNN architecture will not suffice. The creation and optimisation', bbox=BoundingBox(l=134.76500022108476, t=538.4257229728803, r=480.58679078841254, b=548.3584230277231, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='of these models can take quite some time, and can be quite confusing. We will', bbox=BoundingBox(l=134.76500022108476, t=550.3807030388889, r=480.5867307884124, b=560.3134130937317, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='now explore in detail the individual layers, detailing their hyperparameters', bbox=BoundingBox(l=134.76500022108476, t=562.3356931048977, r=480.58679078841254, b=572.2684031597404, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='and connectivities.', bbox=BoundingBox(l=134.76500022108476, t=574.2906831709063, r=217.085970356134, b=584.223383225749, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='2.2', bbox=BoundingBox(l=134.76500022108476, t=614.616403393562, r=147.21825024151457, b=624.3199934471397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='Convolutional layer', bbox=BoundingBox(l=157.18085025785842, t=614.616403393562, r=248.22903040722483, b=624.3199934471397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='As the name implies, the convolutional layer plays a vital role in how CNNs', bbox=BoundingBox(l=134.76500022108476, t=644.8406835604433, r=480.58688078841266, b=654.7733936152861, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='operate. The layers parameters focus around the use of learnable', bbox=BoundingBox(l=134.76500022108476, t=656.7956836264519, r=419.07770068750557, b=666.7283936812947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='kernels', bbox=BoundingBox(l=421.5659806915877, t=656.8654136268369, r=455.3192707469606, b=666.5690036804147, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='.', bbox=BoundingBox(l=455.3189707469602, t=656.7956836264519, r=457.80963075104614, b=666.7283936812947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))], predictions=PagePredictions(layout=LayoutPrediction(clusters=[Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=255.9207763671875, t=93.53521728515625, r=447.56707763671875, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9285128712654114, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=475.2783203125, t=93.44747924804688, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.873298704624176, cells=[Cell(id=1, text='5', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=2, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=150.73934936523438, t=118.60086059570312, r=480.5961907884279, b=141.57867431640625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7262013554573059, cells=[Cell(id=2, text='an ’elementwise’ activation function such as sigmoid to the output of the', bbox=BoundingBox(l=151.70102024886864, t=119.67181066075966, r=480.5961907884279, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='activation produced by the previous layer.', bbox=BoundingBox(l=151.70102024886864, t=131.6268307267684, r=337.7227205540411, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=3, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.22279357910156, t=150.14556884765625, r=480.59641078842833, b=184.86651102072767, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9725128412246704, cells=[Cell(id=4, text='3.', bbox=BoundingBox(l=139.24802022843926, t=151.02282083386194, r=147.41855024184318, b=160.9555008887047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='The', bbox=BoundingBox(l=150.14204024631113, t=151.02282083386194, r=168.37866027622866, b=160.9555008887047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='pooling layer', bbox=BoundingBox(l=171.0980202806898, t=151.0925208342469, r=231.9396103805017, b=160.79608088782436, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='will then simply perform downsampling along the spa-', bbox=BoundingBox(l=234.6580203849613, t=151.02282083386194, r=480.59476078842556, b=160.9555008887047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='tial dimensionality of the given input, further reducing the number of pa-', bbox=BoundingBox(l=151.70102024886864, t=162.9778408998709, r=480.59641078842833, b=172.91052095471343, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='rameters within that activation.', bbox=BoundingBox(l=151.70102024886864, t=174.93383096588502, r=290.1014404759174, b=184.86651102072767, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=4, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.19285583496094, t=193.37098693847656, r=480.59628078842803, b=240.20932006835938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.978056788444519, cells=[Cell(id=10, text='4.', bbox=BoundingBox(l=139.24802022843926, t=194.32983107297866, r=147.41855024184318, b=204.2625111278213, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='The', bbox=BoundingBox(l=150.14204024631113, t=194.32983107297866, r=168.37866027622866, b=204.2625111278213, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='fully-connected layers', bbox=BoundingBox(l=172.55103028307352, t=194.3995310733635, r=275.22559045151326, b=204.103081126941, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='will then perform the same duties found in', bbox=BoundingBox(l=279.3970304583566, t=194.32983107297866, r=480.5917107884205, b=204.2625111278213, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='standard ANNs and attempt to produce class scores from the activations,', bbox=BoundingBox(l=151.70103024886865, t=206.28485113898762, r=480.59628078842803, b=216.21752119383007, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='to be used for classification. It is also suggested that ReLu may be used', bbox=BoundingBox(l=151.70103024886865, t=218.23986120499615, r=480.5863007884118, b=228.1725412598389, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='between these layers, as to improve performance.', bbox=BoundingBox(l=151.70103024886865, t=230.1948812710051, r=369.3041406058511, b=240.12756132584775, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.94287109375, t=248.67041015625, r=480.5868507884127, b=283.60284423828125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9642502665519714, cells=[Cell(id=17, text='Through this simple method of transformation, CNNs are able to transform', bbox=BoundingBox(l=134.7650302210848, t=249.59185137810402, r=480.5868507884127, b=259.5245314329467, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='the original input layer by layer using convolutional and downsampling tech-', bbox=BoundingBox(l=134.7650302210848, t=261.546871444113, r=480.58679078841254, b=271.47955149895563, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='niques to produce class scores for classification and regression purposes.', bbox=BoundingBox(l=134.7650302210848, t=273.5018915101218, r=453.4885907439574, b=283.43457156496436, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=6, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=133.96603393554688, t=452.9920349121094, r=480.58679078841254, b=499.6073303222656, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7702291011810303, cells=[Cell(id=20, text='Fig. 3: Activations taken from the first convolutional layer of a simplistic deep', bbox=BoundingBox(l=134.76500022108476, t=453.7317525052485, r=480.5866407884123, b=463.6644525600912, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='CNN, after training on the MNIST database of handwritten digits. If you look', bbox=BoundingBox(l=134.76500022108476, t=465.68673257125704, r=480.58679078841254, b=475.6194426260998, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='carefully, you can see that the network has successfully picked up on character-', bbox=BoundingBox(l=134.76500022108476, t=477.6417226372657, r=480.5867307884124, b=487.5744326921085, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='istics unique to specific numeric digits.', bbox=BoundingBox(l=134.76500022108476, t=489.59771270327985, r=305.6634505014472, b=499.53042275812265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87347412109375, t=525.5286865234375, r=480.58679078841254, b=584.223383225749, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9733061194419861, cells=[Cell(id=24, text='However, it is important to note that simply understanding the overall archi-', bbox=BoundingBox(l=134.76500022108476, t=526.4707329068717, r=480.5867307884124, b=536.4034429617145, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='tecture of a CNN architecture will not suffice. The creation and optimisation', bbox=BoundingBox(l=134.76500022108476, t=538.4257229728803, r=480.58679078841254, b=548.3584230277231, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='of these models can take quite some time, and can be quite confusing. We will', bbox=BoundingBox(l=134.76500022108476, t=550.3807030388889, r=480.5867307884124, b=560.3134130937317, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='now explore in detail the individual layers, detailing their hyperparameters', bbox=BoundingBox(l=134.76500022108476, t=562.3356931048977, r=480.58679078841254, b=572.2684031597404, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='and connectivities.', bbox=BoundingBox(l=134.76500022108476, t=574.2906831709063, r=217.085970356134, b=584.223383225749, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=8, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.96798706054688, t=613.8170776367188, r=248.6064910888672, b=624.6824951171875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9571674466133118, cells=[Cell(id=29, text='2.2', bbox=BoundingBox(l=134.76500022108476, t=614.616403393562, r=147.21825024151457, b=624.3199934471397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='Convolutional layer', bbox=BoundingBox(l=157.18085025785842, t=614.616403393562, r=248.22903040722483, b=624.3199934471397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87005615234375, t=643.7536010742188, r=480.58688078841266, b=666.9384155273438, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9720144271850586, cells=[Cell(id=31, text='As the name implies, the convolutional layer plays a vital role in how CNNs', bbox=BoundingBox(l=134.76500022108476, t=644.8406835604433, r=480.58688078841266, b=654.7733936152861, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='operate. The layers parameters focus around the use of learnable', bbox=BoundingBox(l=134.76500022108476, t=656.7956836264519, r=419.07770068750557, b=666.7283936812947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='kernels', bbox=BoundingBox(l=421.5659806915877, t=656.8654136268369, r=455.3192707469606, b=666.5690036804147, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='.', bbox=BoundingBox(l=455.3189707469602, t=656.7956836264519, r=457.80963075104614, b=666.7283936812947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=10, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=135.41848754882812, t=307.9389953613281, r=479.3868713378906, b=442.5080871582031, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9779683947563171, cells=[])]), tablestructure=TableStructurePrediction(table_map={}), figures_classification=None, equations_prediction=None), assembled=AssembledUnit(elements=[TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=4, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=255.9207763671875, t=93.53521728515625, r=447.56707763671875, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9285128712654114, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=4, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=475.2783203125, t=93.44747924804688, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.873298704624176, cells=[Cell(id=1, text='5', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='5'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=2, page_no=4, cluster=Cluster(id=2, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=150.73934936523438, t=118.60086059570312, r=480.5961907884279, b=141.57867431640625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7262013554573059, cells=[Cell(id=2, text='an ’elementwise’ activation function such as sigmoid to the output of the', bbox=BoundingBox(l=151.70102024886864, t=119.67181066075966, r=480.5961907884279, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='activation produced by the previous layer.', bbox=BoundingBox(l=151.70102024886864, t=131.6268307267684, r=337.7227205540411, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='an ’elementwise’ activation function such as sigmoid to the output of the activation produced by the previous layer.'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=3, page_no=4, cluster=Cluster(id=3, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.22279357910156, t=150.14556884765625, r=480.59641078842833, b=184.86651102072767, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9725128412246704, cells=[Cell(id=4, text='3.', bbox=BoundingBox(l=139.24802022843926, t=151.02282083386194, r=147.41855024184318, b=160.9555008887047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='The', bbox=BoundingBox(l=150.14204024631113, t=151.02282083386194, r=168.37866027622866, b=160.9555008887047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='pooling layer', bbox=BoundingBox(l=171.0980202806898, t=151.0925208342469, r=231.9396103805017, b=160.79608088782436, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='will then simply perform downsampling along the spa-', bbox=BoundingBox(l=234.6580203849613, t=151.02282083386194, r=480.59476078842556, b=160.9555008887047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='tial dimensionality of the given input, further reducing the number of pa-', bbox=BoundingBox(l=151.70102024886864, t=162.9778408998709, r=480.59641078842833, b=172.91052095471343, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='rameters within that activation.', bbox=BoundingBox(l=151.70102024886864, t=174.93383096588502, r=290.1014404759174, b=184.86651102072767, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='3. The pooling layer will then simply perform downsampling along the spatial dimensionality of the given input, further reducing the number of parameters within that activation.'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=4, page_no=4, cluster=Cluster(id=4, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.19285583496094, t=193.37098693847656, r=480.59628078842803, b=240.20932006835938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.978056788444519, cells=[Cell(id=10, text='4.', bbox=BoundingBox(l=139.24802022843926, t=194.32983107297866, r=147.41855024184318, b=204.2625111278213, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='The', bbox=BoundingBox(l=150.14204024631113, t=194.32983107297866, r=168.37866027622866, b=204.2625111278213, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='fully-connected layers', bbox=BoundingBox(l=172.55103028307352, t=194.3995310733635, r=275.22559045151326, b=204.103081126941, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='will then perform the same duties found in', bbox=BoundingBox(l=279.3970304583566, t=194.32983107297866, r=480.5917107884205, b=204.2625111278213, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='standard ANNs and attempt to produce class scores from the activations,', bbox=BoundingBox(l=151.70103024886865, t=206.28485113898762, r=480.59628078842803, b=216.21752119383007, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='to be used for classification. It is also suggested that ReLu may be used', bbox=BoundingBox(l=151.70103024886865, t=218.23986120499615, r=480.5863007884118, b=228.1725412598389, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='between these layers, as to improve performance.', bbox=BoundingBox(l=151.70103024886865, t=230.1948812710051, r=369.3041406058511, b=240.12756132584775, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='4. The fully-connected layers will then perform the same duties found in standard ANNs and attempt to produce class scores from the activations, to be used for classification. It is also suggested that ReLu may be used between these layers, as to improve performance.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=4, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.94287109375, t=248.67041015625, r=480.5868507884127, b=283.60284423828125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9642502665519714, cells=[Cell(id=17, text='Through this simple method of transformation, CNNs are able to transform', bbox=BoundingBox(l=134.7650302210848, t=249.59185137810402, r=480.5868507884127, b=259.5245314329467, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='the original input layer by layer using convolutional and downsampling tech-', bbox=BoundingBox(l=134.7650302210848, t=261.546871444113, r=480.58679078841254, b=271.47955149895563, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='niques to produce class scores for classification and regression purposes.', bbox=BoundingBox(l=134.7650302210848, t=273.5018915101218, r=453.4885907439574, b=283.43457156496436, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Through this simple method of transformation, CNNs are able to transform the original input layer by layer using convolutional and downsampling techniques to produce class scores for classification and regression purposes.'), TextElement(label=<DocItemLabel.CAPTION: 'caption'>, id=6, page_no=4, cluster=Cluster(id=6, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=133.96603393554688, t=452.9920349121094, r=480.58679078841254, b=499.6073303222656, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7702291011810303, cells=[Cell(id=20, text='Fig. 3: Activations taken from the first convolutional layer of a simplistic deep', bbox=BoundingBox(l=134.76500022108476, t=453.7317525052485, r=480.5866407884123, b=463.6644525600912, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='CNN, after training on the MNIST database of handwritten digits. If you look', bbox=BoundingBox(l=134.76500022108476, t=465.68673257125704, r=480.58679078841254, b=475.6194426260998, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='carefully, you can see that the network has successfully picked up on character-', bbox=BoundingBox(l=134.76500022108476, t=477.6417226372657, r=480.5867307884124, b=487.5744326921085, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='istics unique to specific numeric digits.', bbox=BoundingBox(l=134.76500022108476, t=489.59771270327985, r=305.6634505014472, b=499.53042275812265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Fig. 3: Activations taken from the first convolutional layer of a simplistic deep CNN, after training on the MNIST database of handwritten digits. If you look carefully, you can see that the network has successfully picked up on characteristics unique to specific numeric digits.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=4, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87347412109375, t=525.5286865234375, r=480.58679078841254, b=584.223383225749, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9733061194419861, cells=[Cell(id=24, text='However, it is important to note that simply understanding the overall archi-', bbox=BoundingBox(l=134.76500022108476, t=526.4707329068717, r=480.5867307884124, b=536.4034429617145, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='tecture of a CNN architecture will not suffice. The creation and optimisation', bbox=BoundingBox(l=134.76500022108476, t=538.4257229728803, r=480.58679078841254, b=548.3584230277231, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='of these models can take quite some time, and can be quite confusing. We will', bbox=BoundingBox(l=134.76500022108476, t=550.3807030388889, r=480.5867307884124, b=560.3134130937317, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='now explore in detail the individual layers, detailing their hyperparameters', bbox=BoundingBox(l=134.76500022108476, t=562.3356931048977, r=480.58679078841254, b=572.2684031597404, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='and connectivities.', bbox=BoundingBox(l=134.76500022108476, t=574.2906831709063, r=217.085970356134, b=584.223383225749, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='However, it is important to note that simply understanding the overall architecture of a CNN architecture will not suffice. The creation and optimisation of these models can take quite some time, and can be quite confusing. We will now explore in detail the individual layers, detailing their hyperparameters and connectivities.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=8, page_no=4, cluster=Cluster(id=8, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.96798706054688, t=613.8170776367188, r=248.6064910888672, b=624.6824951171875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9571674466133118, cells=[Cell(id=29, text='2.2', bbox=BoundingBox(l=134.76500022108476, t=614.616403393562, r=147.21825024151457, b=624.3199934471397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='Convolutional layer', bbox=BoundingBox(l=157.18085025785842, t=614.616403393562, r=248.22903040722483, b=624.3199934471397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2.2 Convolutional layer'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=9, page_no=4, cluster=Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87005615234375, t=643.7536010742188, r=480.58688078841266, b=666.9384155273438, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9720144271850586, cells=[Cell(id=31, text='As the name implies, the convolutional layer plays a vital role in how CNNs', bbox=BoundingBox(l=134.76500022108476, t=644.8406835604433, r=480.58688078841266, b=654.7733936152861, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='operate. The layers parameters focus around the use of learnable', bbox=BoundingBox(l=134.76500022108476, t=656.7956836264519, r=419.07770068750557, b=666.7283936812947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='kernels', bbox=BoundingBox(l=421.5659806915877, t=656.8654136268369, r=455.3192707469606, b=666.5690036804147, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='.', bbox=BoundingBox(l=455.3189707469602, t=656.7956836264519, r=457.80963075104614, b=666.7283936812947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='As the name implies, the convolutional layer plays a vital role in how CNNs operate. The layers parameters focus around the use of learnable kernels .'), FigureElement(label=<DocItemLabel.PICTURE: 'picture'>, id=10, page_no=4, cluster=Cluster(id=10, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=135.41848754882812, t=307.9389953613281, r=479.3868713378906, b=442.5080871582031, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9779683947563171, cells=[]), text='', annotations=[], provenance=None, predicted_class=None, confidence=None)], body=[TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=2, page_no=4, cluster=Cluster(id=2, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=150.73934936523438, t=118.60086059570312, r=480.5961907884279, b=141.57867431640625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7262013554573059, cells=[Cell(id=2, text='an ’elementwise’ activation function such as sigmoid to the output of the', bbox=BoundingBox(l=151.70102024886864, t=119.67181066075966, r=480.5961907884279, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='activation produced by the previous layer.', bbox=BoundingBox(l=151.70102024886864, t=131.6268307267684, r=337.7227205540411, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='an ’elementwise’ activation function such as sigmoid to the output of the activation produced by the previous layer.'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=3, page_no=4, cluster=Cluster(id=3, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.22279357910156, t=150.14556884765625, r=480.59641078842833, b=184.86651102072767, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9725128412246704, cells=[Cell(id=4, text='3.', bbox=BoundingBox(l=139.24802022843926, t=151.02282083386194, r=147.41855024184318, b=160.9555008887047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='The', bbox=BoundingBox(l=150.14204024631113, t=151.02282083386194, r=168.37866027622866, b=160.9555008887047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='pooling layer', bbox=BoundingBox(l=171.0980202806898, t=151.0925208342469, r=231.9396103805017, b=160.79608088782436, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='will then simply perform downsampling along the spa-', bbox=BoundingBox(l=234.6580203849613, t=151.02282083386194, r=480.59476078842556, b=160.9555008887047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='tial dimensionality of the given input, further reducing the number of pa-', bbox=BoundingBox(l=151.70102024886864, t=162.9778408998709, r=480.59641078842833, b=172.91052095471343, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='rameters within that activation.', bbox=BoundingBox(l=151.70102024886864, t=174.93383096588502, r=290.1014404759174, b=184.86651102072767, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='3. The pooling layer will then simply perform downsampling along the spatial dimensionality of the given input, further reducing the number of parameters within that activation.'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=4, page_no=4, cluster=Cluster(id=4, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.19285583496094, t=193.37098693847656, r=480.59628078842803, b=240.20932006835938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.978056788444519, cells=[Cell(id=10, text='4.', bbox=BoundingBox(l=139.24802022843926, t=194.32983107297866, r=147.41855024184318, b=204.2625111278213, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='The', bbox=BoundingBox(l=150.14204024631113, t=194.32983107297866, r=168.37866027622866, b=204.2625111278213, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='fully-connected layers', bbox=BoundingBox(l=172.55103028307352, t=194.3995310733635, r=275.22559045151326, b=204.103081126941, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='will then perform the same duties found in', bbox=BoundingBox(l=279.3970304583566, t=194.32983107297866, r=480.5917107884205, b=204.2625111278213, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='standard ANNs and attempt to produce class scores from the activations,', bbox=BoundingBox(l=151.70103024886865, t=206.28485113898762, r=480.59628078842803, b=216.21752119383007, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='to be used for classification. It is also suggested that ReLu may be used', bbox=BoundingBox(l=151.70103024886865, t=218.23986120499615, r=480.5863007884118, b=228.1725412598389, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='between these layers, as to improve performance.', bbox=BoundingBox(l=151.70103024886865, t=230.1948812710051, r=369.3041406058511, b=240.12756132584775, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='4. The fully-connected layers will then perform the same duties found in standard ANNs and attempt to produce class scores from the activations, to be used for classification. It is also suggested that ReLu may be used between these layers, as to improve performance.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=4, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.94287109375, t=248.67041015625, r=480.5868507884127, b=283.60284423828125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9642502665519714, cells=[Cell(id=17, text='Through this simple method of transformation, CNNs are able to transform', bbox=BoundingBox(l=134.7650302210848, t=249.59185137810402, r=480.5868507884127, b=259.5245314329467, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='the original input layer by layer using convolutional and downsampling tech-', bbox=BoundingBox(l=134.7650302210848, t=261.546871444113, r=480.58679078841254, b=271.47955149895563, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='niques to produce class scores for classification and regression purposes.', bbox=BoundingBox(l=134.7650302210848, t=273.5018915101218, r=453.4885907439574, b=283.43457156496436, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Through this simple method of transformation, CNNs are able to transform the original input layer by layer using convolutional and downsampling techniques to produce class scores for classification and regression purposes.'), TextElement(label=<DocItemLabel.CAPTION: 'caption'>, id=6, page_no=4, cluster=Cluster(id=6, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=133.96603393554688, t=452.9920349121094, r=480.58679078841254, b=499.6073303222656, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7702291011810303, cells=[Cell(id=20, text='Fig. 3: Activations taken from the first convolutional layer of a simplistic deep', bbox=BoundingBox(l=134.76500022108476, t=453.7317525052485, r=480.5866407884123, b=463.6644525600912, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='CNN, after training on the MNIST database of handwritten digits. If you look', bbox=BoundingBox(l=134.76500022108476, t=465.68673257125704, r=480.58679078841254, b=475.6194426260998, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='carefully, you can see that the network has successfully picked up on character-', bbox=BoundingBox(l=134.76500022108476, t=477.6417226372657, r=480.5867307884124, b=487.5744326921085, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='istics unique to specific numeric digits.', bbox=BoundingBox(l=134.76500022108476, t=489.59771270327985, r=305.6634505014472, b=499.53042275812265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Fig. 3: Activations taken from the first convolutional layer of a simplistic deep CNN, after training on the MNIST database of handwritten digits. If you look carefully, you can see that the network has successfully picked up on characteristics unique to specific numeric digits.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=4, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87347412109375, t=525.5286865234375, r=480.58679078841254, b=584.223383225749, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9733061194419861, cells=[Cell(id=24, text='However, it is important to note that simply understanding the overall archi-', bbox=BoundingBox(l=134.76500022108476, t=526.4707329068717, r=480.5867307884124, b=536.4034429617145, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='tecture of a CNN architecture will not suffice. The creation and optimisation', bbox=BoundingBox(l=134.76500022108476, t=538.4257229728803, r=480.58679078841254, b=548.3584230277231, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='of these models can take quite some time, and can be quite confusing. We will', bbox=BoundingBox(l=134.76500022108476, t=550.3807030388889, r=480.5867307884124, b=560.3134130937317, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='now explore in detail the individual layers, detailing their hyperparameters', bbox=BoundingBox(l=134.76500022108476, t=562.3356931048977, r=480.58679078841254, b=572.2684031597404, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='and connectivities.', bbox=BoundingBox(l=134.76500022108476, t=574.2906831709063, r=217.085970356134, b=584.223383225749, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='However, it is important to note that simply understanding the overall architecture of a CNN architecture will not suffice. The creation and optimisation of these models can take quite some time, and can be quite confusing. We will now explore in detail the individual layers, detailing their hyperparameters and connectivities.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=8, page_no=4, cluster=Cluster(id=8, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.96798706054688, t=613.8170776367188, r=248.6064910888672, b=624.6824951171875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9571674466133118, cells=[Cell(id=29, text='2.2', bbox=BoundingBox(l=134.76500022108476, t=614.616403393562, r=147.21825024151457, b=624.3199934471397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='Convolutional layer', bbox=BoundingBox(l=157.18085025785842, t=614.616403393562, r=248.22903040722483, b=624.3199934471397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2.2 Convolutional layer'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=9, page_no=4, cluster=Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87005615234375, t=643.7536010742188, r=480.58688078841266, b=666.9384155273438, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9720144271850586, cells=[Cell(id=31, text='As the name implies, the convolutional layer plays a vital role in how CNNs', bbox=BoundingBox(l=134.76500022108476, t=644.8406835604433, r=480.58688078841266, b=654.7733936152861, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='operate. The layers parameters focus around the use of learnable', bbox=BoundingBox(l=134.76500022108476, t=656.7956836264519, r=419.07770068750557, b=666.7283936812947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='kernels', bbox=BoundingBox(l=421.5659806915877, t=656.8654136268369, r=455.3192707469606, b=666.5690036804147, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='.', bbox=BoundingBox(l=455.3189707469602, t=656.7956836264519, r=457.80963075104614, b=666.7283936812947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='As the name implies, the convolutional layer plays a vital role in how CNNs operate. The layers parameters focus around the use of learnable kernels .'), FigureElement(label=<DocItemLabel.PICTURE: 'picture'>, id=10, page_no=4, cluster=Cluster(id=10, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=135.41848754882812, t=307.9389953613281, r=479.3868713378906, b=442.5080871582031, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9779683947563171, cells=[]), text='', annotations=[], provenance=None, predicted_class=None, confidence=None)], headers=[TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=4, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=255.9207763671875, t=93.53521728515625, r=447.56707763671875, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9285128712654114, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=4, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=475.2783203125, t=93.44747924804688, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.873298704624176, cells=[Cell(id=1, text='5', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='5')])), Page(page_no=5, size=Size(width=595.2760009765625, height=841.8900146484375), cells=[Cell(id=0, text='6', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=2, text='These kernels are usually small in spatial dimensionality, but spreads along the', bbox=BoundingBox(l=134.76500022108476, t=119.67181066075966, r=480.58676078841245, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='entirety of the depth of the input. When the data hits a convolutional layer,', bbox=BoundingBox(l=134.76500022108476, t=131.6268307267684, r=480.58679078841254, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='the layer convolves each filter across the spatial dimensionality of the input to', bbox=BoundingBox(l=134.76500022108476, t=143.58184079277714, r=480.58676078841245, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='produce a 2D activation map. These activation maps can be visualised, as seen', bbox=BoundingBox(l=134.76500022108476, t=155.53686085878599, r=480.58676078841245, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='in Figure 3.', bbox=BoundingBox(l=134.76500022108476, t=167.49188092479483, r=184.42854030255882, b=177.42456097963736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='As we glide through the input, the scalar product is calculated for each value in', bbox=BoundingBox(l=134.76500022108476, t=187.70990103642725, r=480.58679078841254, b=197.6425710912697, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='that kernel. (Figure 4) From this the network will learn kernels that ’fire’ when', bbox=BoundingBox(l=134.76500022108476, t=199.6649111024359, r=480.5867307884124, b=209.59759115727866, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='they see a specific feature at a given spatial position of the input. These are', bbox=BoundingBox(l=134.76500022108476, t=211.61993116844474, r=480.58670078841243, b=221.55261122328739, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='commonly known as', bbox=BoundingBox(l=134.76500022108476, t=223.57495123445358, r=226.9688603723471, b=233.50762128929614, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='activations', bbox=BoundingBox(l=229.45900037643224, t=223.64465123483842, r=278.7041304572199, b=233.3482012884159, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='.', bbox=BoundingBox(l=278.7040104572196, t=223.57495123445358, r=281.19467046130563, b=233.50762128929614, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='0', bbox=BoundingBox(l=276.3548604533658, t=293.0746416181913, r=279.2306204580836, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='0', bbox=BoundingBox(l=287.97589047243036, t=293.0746416181913, r=290.85165047714816, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='0', bbox=BoundingBox(l=276.3548604533658, t=304.6956716823561, r=279.2306204580836, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='1', bbox=BoundingBox(l=287.97589047243036, t=304.6956716823561, r=290.85165047714816, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='0', bbox=BoundingBox(l=299.70312049166915, t=293.0746416181913, r=302.5788604963869, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='2', bbox=BoundingBox(l=299.70312049166915, t=304.6956716823561, r=302.5788604963869, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='0', bbox=BoundingBox(l=276.3548604533658, t=316.51141174759584, r=279.2306204580836, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='1', bbox=BoundingBox(l=287.97589047243036, t=316.51141174759584, r=290.85165047714816, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='1', bbox=BoundingBox(l=299.70312049166915, t=316.51141174759584, r=302.5788604963869, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='4', bbox=BoundingBox(l=342.4826705618499, t=293.0746416181913, r=345.3584005665676, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='0', bbox=BoundingBox(l=354.1214005809435, t=293.0746416181913, r=356.9971305856612, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='0', bbox=BoundingBox(l=342.4826705618499, t=304.6956716823561, r=345.3584005665676, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='0', bbox=BoundingBox(l=354.1214005809435, t=304.6956716823561, r=356.9971305856612, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='0', bbox=BoundingBox(l=365.8308706001531, t=293.0746416181913, r=368.70667060487085, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='0', bbox=BoundingBox(l=365.8308706001531, t=304.6956716823561, r=368.70667060487085, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='0', bbox=BoundingBox(l=342.4826705618499, t=316.51141174759584, r=345.3584005665676, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='0', bbox=BoundingBox(l=354.1214005809435, t=316.51141174759584, r=356.9971305856612, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='-4', bbox=BoundingBox(l=364.8516805985467, t=316.51141174759584, r=369.69177060648695, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='-8', bbox=BoundingBox(l=419.2581506878016, t=304.6956716823561, r=424.1100806957613, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='Pooled Vector', bbox=BoundingBox(l=269.128600441511, t=275.49743152114, r=309.9874305085408, b=283.3499715644973, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='Kernel', bbox=BoundingBox(l=345.9925505676079, t=276.30102152557697, r=365.1973305991138, b=284.1372015688439, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='Destination Pixel', bbox=BoundingBox(l=397.25488065170487, t=277.072381529836, r=446.12943073188455, b=284.9249815731936, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='0', bbox=BoundingBox(l=174.49387028626077, t=275.435301520797, r=177.37701029099065, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='0', bbox=BoundingBox(l=186.11195030532048, t=275.435301520797, r=188.99509031005033, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='0', bbox=BoundingBox(l=174.49387028626077, t=287.05633158496175, r=177.37701029099065, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='1', bbox=BoundingBox(l=186.11195030532048, t=287.05633158496175, r=188.99509031005033, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='0', bbox=BoundingBox(l=197.83917032455923, t=275.435301520797, r=200.7223103292891, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='2', bbox=BoundingBox(l=197.83917032455923, t=287.05633158496175, r=200.7223103292891, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='0', bbox=BoundingBox(l=174.49387028626077, t=298.8851916502739, r=177.36963029097853, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='1', bbox=BoundingBox(l=186.11195030532048, t=298.8851916502739, r=188.98772031003824, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='1', bbox=BoundingBox(l=197.83917032455923, t=298.8851916502739, r=200.714940329277, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='0', bbox=BoundingBox(l=209.65488034394315, t=275.435301520797, r=212.53799034867296, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='0', bbox=BoundingBox(l=221.269990362998, t=275.435301520797, r=224.15314036772787, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='1', bbox=BoundingBox(l=209.65488034394315, t=287.05633158496175, r=212.53799034867296, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='1', bbox=BoundingBox(l=221.269990362998, t=287.05633158496175, r=224.15314036772787, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='0', bbox=BoundingBox(l=232.99721038223674, t=275.435301520797, r=235.88034038696654, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='2', bbox=BoundingBox(l=232.99721038223674, t=287.05633158496175, r=235.88034038696654, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='1', bbox=BoundingBox(l=209.65488034394315, t=298.8851916502739, r=212.5306403486609, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='1', bbox=BoundingBox(l=221.269990362998, t=298.8851916502739, r=224.14575036771572, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='1', bbox=BoundingBox(l=232.99721038223674, t=298.8851916502739, r=235.87297038695448, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='1', bbox=BoundingBox(l=174.49387028626077, t=310.6949417154807, r=177.36963029097853, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='0', bbox=BoundingBox(l=186.11195030532048, t=310.6949417154807, r=188.98772031003824, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='0', bbox=BoundingBox(l=174.49387028626077, t=322.31604177964573, r=177.36963029097853, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='0', bbox=BoundingBox(l=186.11195030532048, t=322.31604177964573, r=188.98772031003824, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='0', bbox=BoundingBox(l=197.83917032455923, t=310.6949417154807, r=200.714940329277, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='1', bbox=BoundingBox(l=197.83917032455923, t=322.31604177964573, r=200.714940329277, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='0', bbox=BoundingBox(l=174.49387028626077, t=334.1317118448851, r=177.36963029097853, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='1', bbox=BoundingBox(l=186.11195030532048, t=334.1317118448851, r=188.98772031003824, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='1', bbox=BoundingBox(l=197.83917032455923, t=334.1317118448851, r=200.714940329277, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='0', bbox=BoundingBox(l=209.65488034394315, t=310.6949417154807, r=212.5306403486609, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='0', bbox=BoundingBox(l=221.269990362998, t=310.6949417154807, r=224.14575036771572, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='1', bbox=BoundingBox(l=209.65488034394315, t=322.31604177964573, r=212.5306403486609, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=65, text='1', bbox=BoundingBox(l=221.269990362998, t=322.31604177964573, r=224.14575036771572, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='0', bbox=BoundingBox(l=232.99721038223674, t=310.6949417154807, r=235.87297038695448, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='0', bbox=BoundingBox(l=232.99721038223674, t=322.31604177964573, r=235.87297038695448, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=68, text='1', bbox=BoundingBox(l=209.65488034394315, t=334.1317118448851, r=212.5306403486609, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=69, text='1', bbox=BoundingBox(l=221.269990362998, t=334.1317118448851, r=224.14575036771572, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=70, text='1', bbox=BoundingBox(l=232.99721038223674, t=334.1317118448851, r=235.87297038695448, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=71, text='Input Vector', bbox=BoundingBox(l=187.20328030711084, t=257.09973141955834, r=223.31931036635996, b=264.9359714628257, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=72, text='Fig. 4: A visual representation of a convolutional layer. The centre element of the', bbox=BoundingBox(l=134.76500022108476, t=355.21276196128264, r=480.58676078841245, b=365.1454720161255, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=73, text='kernel is placed over the input vector, of which is then calculated and replaced', bbox=BoundingBox(l=134.76500022108476, t=367.16775202729133, r=480.5867307884124, b=377.1004620821341, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=74, text='with a weighted sum of itself and any nearby pixels.', bbox=BoundingBox(l=134.76500022108476, t=379.12274209329996, r=365.1202105989872, b=389.0554521481427, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=75, text='Every kernel will have a corresponding activation map, of which will be stacked', bbox=BoundingBox(l=134.76500022108476, t=416.8167423014248, r=480.7759707887229, b=426.74945235626757, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=76, text='along the depth dimension to form the full output volume from the convolu-', bbox=BoundingBox(l=134.76500022108476, t=428.77172236743337, r=480.58682078841264, b=438.7044324222761, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=77, text='tional layer.', bbox=BoundingBox(l=134.76500022108476, t=440.72671243344206, r=186.40115030579494, b=450.6594224882848, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=78, text='As we alluded to earlier, training ANNs on inputs such as images results in', bbox=BoundingBox(l=134.76500022108476, t=460.9447025450743, r=480.58691078841275, b=470.8774125999171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=79, text='models of which are too big to train effectively. This comes down to the fully-', bbox=BoundingBox(l=134.76500022108476, t=472.89968261108294, r=480.58676078841245, b=482.8323926659257, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=80, text='connected manner of standard ANN neurons, so to mitigate against this every', bbox=BoundingBox(l=134.76500022108476, t=484.8546726770915, r=480.58679078841254, b=494.7873827319343, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=81, text='neuron in a convolutional layer is only connected to small region of the input', bbox=BoundingBox(l=134.76500022108476, t=496.80966274310015, r=480.5867307884124, b=506.74237279794295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=82, text='volume. The dimensionality of this region is commonly referred to as the', bbox=BoundingBox(l=134.76500022108476, t=508.7646428091088, r=465.0551807629326, b=518.6973528639516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=83, text='re-', bbox=BoundingBox(l=468.4169907684477, t=508.83438280949383, r=480.5912807884199, b=518.5379628630715, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=84, text='ceptive field size', bbox=BoundingBox(l=134.76498022108473, t=520.7893628755024, r=210.161930344775, b=530.49295292908, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=85, text='of the neuron. The magnitude of the connectivity through the', bbox=BoundingBox(l=212.4549903485368, t=520.7196328751174, r=480.5884407884152, b=530.6523429299602, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=86, text='depth is nearly always equal to the depth of the input.', bbox=BoundingBox(l=134.76498022108473, t=532.6756229411317, r=373.18994061222577, b=542.6083329959744, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=87, text='For example, if the input to the network is an image of size', bbox=BoundingBox(l=134.76498022108473, t=552.8926330527584, r=392.8262006444395, b=562.8253431076012, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=88, text='64', bbox=BoundingBox(l=395.26599064844197, t=553.1018630539137, r=405.22858066478585, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=89, text='×', bbox=BoundingBox(l=407.24899066810036, t=552.5439430508331, r=414.9978906808126, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=90, text='64', bbox=BoundingBox(l=417.0189806841282, t=553.1018630539137, r=426.98157070047205, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=91, text='×', bbox=BoundingBox(l=429.00299070378827, t=552.5439430508331, r=436.7518907165005, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=92, text='3', bbox=BoundingBox(l=438.77298071981613, t=553.1018630539137, r=443.75427072798806, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=93, text='(a RGB-', bbox=BoundingBox(l=446.1929907319888, t=552.8926330527584, r=480.5938407884241, b=562.8253431076012, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=94, text='coloured image with a dimensionality of', bbox=BoundingBox(l=134.76498022108473, t=564.847623118767, r=312.8862905132964, b=574.7803331736098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=95, text='64', bbox=BoundingBox(l=315.2929705172446, t=565.0568531199224, r=325.25555053358846, b=573.9036231687692, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=96, text='×', bbox=BoundingBox(l=327.1609805367143, t=564.4989331168418, r=334.90988054942653, b=573.9036231687692, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=97, text='64', bbox=BoundingBox(l=336.8149705525519, t=565.0568531199224, r=346.7775605688957, b=573.9036231687692, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=98, text=') and we set the receptive field', bbox=BoundingBox(l=346.77698056889477, t=564.847623118767, r=480.59470078842554, b=574.7803331736098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=99, text='size as', bbox=BoundingBox(l=134.76498022108473, t=576.8026131847757, r=163.6963802685473, b=586.7353232396185, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=100, text='6', bbox=BoundingBox(l=166.5449802732205, t=577.011843185931, r=171.52628028139242, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=101, text='×', bbox=BoundingBox(l=174.00598028546037, t=576.4539131828503, r=181.75490029817266, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=102, text='6', bbox=BoundingBox(l=184.2349903022413, t=577.011843185931, r=189.21628031041323, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=103, text=', we would have a total of', bbox=BoundingBox(l=189.21599031041274, t=576.8026131847757, r=305.3599505009493, b=586.7353232396185, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=104, text='108', bbox=BoundingBox(l=308.2089805056232, t=577.011843185931, r=323.15289053013896, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=105, text='weights on each neuron within the', bbox=BoundingBox(l=326.00299053481467, t=576.8026131847757, r=480.5926807884222, b=586.7353232396185, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=106, text='convolutional layer. (', bbox=BoundingBox(l=134.76498022108473, t=588.7576132507844, r=228.17432037432468, b=598.6903233056272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=107, text='6', bbox=BoundingBox(l=228.1749903743258, t=588.9668232519396, r=233.1562803824977, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=108, text='×', bbox=BoundingBox(l=235.49298038633108, t=588.4089232488592, r=243.24190039904332, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=109, text='6', bbox=BoundingBox(l=245.57799040287574, t=588.9668232519396, r=250.55928041104767, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=110, text='×', bbox=BoundingBox(l=252.8959804148811, t=588.4089232488592, r=260.6449004275933, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=111, text='3', bbox=BoundingBox(l=262.98099043142577, t=588.9668232519396, r=267.9622804395977, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=112, text='where', bbox=BoundingBox(l=270.618990443956, t=588.7576132507844, r=298.02612048891797, b=598.6903233056272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=113, text='3', bbox=BoundingBox(l=300.68198049327503, t=588.9668232519396, r=305.6632705014469, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=114, text='is the magnitude of connectivity across', bbox=BoundingBox(l=308.3189705058036, t=588.7576132507844, r=480.5922207884214, b=598.6903233056272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=115, text='the depth of the volume) To put this into perspective, a standard neuron seen', bbox=BoundingBox(l=134.7649702210847, t=600.7126133167931, r=480.5867307884124, b=610.6453233716359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=116, text='in other forms of ANN would contain', bbox=BoundingBox(l=134.7649702210847, t=612.6686033828073, r=301.70825049495863, b=622.6013134376501, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=117, text='12', bbox=BoundingBox(l=304.1969604990414, t=612.8778233839625, r=314.1595505153852, b=621.7246034328093, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=118, text=',', bbox=BoundingBox(l=314.1599705153859, t=612.8778233839625, r=316.9275805199262, b=621.7246034328093, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=119, text='288', bbox=BoundingBox(l=318.5879805226502, t=612.8778233839625, r=333.5318905471659, b=621.7246034328093, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=120, text='weights each.', bbox=BoundingBox(l=336.0229805512526, t=612.6686033828073, r=395.7687106492667, b=622.6013134376501, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=121, text='Convolutional layers are also able to significantly reduce the complexity of the', bbox=BoundingBox(l=134.76498022108473, t=632.885603494434, r=480.5867307884124, b=642.8183135492768, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=122, text='model through the optimisation of its output. These are optimised through', bbox=BoundingBox(l=134.76498022108473, t=644.8406035604428, r=480.5867307884124, b=654.7733136152856, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=123, text='three hyperparameters, the', bbox=BoundingBox(l=134.76498022108473, t=656.7956036264516, r=254.06711041680234, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=124, text='depth', bbox=BoundingBox(l=256.5559704208854, t=656.8653436268365, r=283.11627046445807, b=666.5689236804142, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=125, text=', the', bbox=BoundingBox(l=283.1169704644592, t=656.7956036264516, r=301.9164104953001, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=126, text='stride', bbox=BoundingBox(l=304.4069804993859, t=656.8653436268365, r=330.4093605420433, b=666.5689236804142, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=127, text='and setting', bbox=BoundingBox(l=332.8999905461293, t=656.7956036264516, r=381.98572062665545, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=128, text='zero-padding', bbox=BoundingBox(l=384.47598063074076, t=656.8653436268365, r=445.35742073061806, b=666.5689236804142, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=129, text='.', bbox=BoundingBox(l=445.3569907306173, t=656.7956036264516, r=447.84766073470337, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))], predictions=PagePredictions(layout=LayoutPrediction(clusters=[Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=133.78485107421875, t=94.09490966796875, r=139.71607971191406, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7533239722251892, cells=[Cell(id=0, text='6', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=167.21620178222656, t=93.54159545898438, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7271904349327087, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.7237091064453, t=118.72857666015625, r=480.7948913574219, b=177.42456097963736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9832537770271301, cells=[Cell(id=2, text='These kernels are usually small in spatial dimensionality, but spreads along the', bbox=BoundingBox(l=134.76500022108476, t=119.67181066075966, r=480.58676078841245, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='entirety of the depth of the input. When the data hits a convolutional layer,', bbox=BoundingBox(l=134.76500022108476, t=131.6268307267684, r=480.58679078841254, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='the layer convolves each filter across the spatial dimensionality of the input to', bbox=BoundingBox(l=134.76500022108476, t=143.58184079277714, r=480.58676078841245, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='produce a 2D activation map. These activation maps can be visualised, as seen', bbox=BoundingBox(l=134.76500022108476, t=155.53686085878599, r=480.58676078841245, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='in Figure 3.', bbox=BoundingBox(l=134.76500022108476, t=167.49188092479483, r=184.42854030255882, b=177.42456097963736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.92149353027344, t=186.75308227539062, r=480.5972595214844, b=233.50762128929614, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9789468050003052, cells=[Cell(id=7, text='As we glide through the input, the scalar product is calculated for each value in', bbox=BoundingBox(l=134.76500022108476, t=187.70990103642725, r=480.58679078841254, b=197.6425710912697, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='that kernel. (Figure 4) From this the network will learn kernels that ’fire’ when', bbox=BoundingBox(l=134.76500022108476, t=199.6649111024359, r=480.5867307884124, b=209.59759115727866, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='they see a specific feature at a given spatial position of the input. These are', bbox=BoundingBox(l=134.76500022108476, t=211.61993116844474, r=480.58670078841243, b=221.55261122328739, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='commonly known as', bbox=BoundingBox(l=134.76500022108476, t=223.57495123445358, r=226.9688603723471, b=233.50762128929614, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='activations', bbox=BoundingBox(l=229.45900037643224, t=223.64465123483842, r=278.7041304572199, b=233.3482012884159, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='.', bbox=BoundingBox(l=278.7040104572196, t=223.57495123445358, r=281.19467046130563, b=233.50762128929614, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=4, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=168.14492797851562, t=257.09973141955834, r=446.12943073188455, b=344.0976257324219, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.922330915927887, cells=[Cell(id=13, text='0', bbox=BoundingBox(l=276.3548604533658, t=293.0746416181913, r=279.2306204580836, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='0', bbox=BoundingBox(l=287.97589047243036, t=293.0746416181913, r=290.85165047714816, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='0', bbox=BoundingBox(l=276.3548604533658, t=304.6956716823561, r=279.2306204580836, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='1', bbox=BoundingBox(l=287.97589047243036, t=304.6956716823561, r=290.85165047714816, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='0', bbox=BoundingBox(l=299.70312049166915, t=293.0746416181913, r=302.5788604963869, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='2', bbox=BoundingBox(l=299.70312049166915, t=304.6956716823561, r=302.5788604963869, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='0', bbox=BoundingBox(l=276.3548604533658, t=316.51141174759584, r=279.2306204580836, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='1', bbox=BoundingBox(l=287.97589047243036, t=316.51141174759584, r=290.85165047714816, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='1', bbox=BoundingBox(l=299.70312049166915, t=316.51141174759584, r=302.5788604963869, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='4', bbox=BoundingBox(l=342.4826705618499, t=293.0746416181913, r=345.3584005665676, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='0', bbox=BoundingBox(l=354.1214005809435, t=293.0746416181913, r=356.9971305856612, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='0', bbox=BoundingBox(l=342.4826705618499, t=304.6956716823561, r=345.3584005665676, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='0', bbox=BoundingBox(l=354.1214005809435, t=304.6956716823561, r=356.9971305856612, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='0', bbox=BoundingBox(l=365.8308706001531, t=293.0746416181913, r=368.70667060487085, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='0', bbox=BoundingBox(l=365.8308706001531, t=304.6956716823561, r=368.70667060487085, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='0', bbox=BoundingBox(l=342.4826705618499, t=316.51141174759584, r=345.3584005665676, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='0', bbox=BoundingBox(l=354.1214005809435, t=316.51141174759584, r=356.9971305856612, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='-4', bbox=BoundingBox(l=364.8516805985467, t=316.51141174759584, r=369.69177060648695, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='-8', bbox=BoundingBox(l=419.2581506878016, t=304.6956716823561, r=424.1100806957613, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='Pooled Vector', bbox=BoundingBox(l=269.128600441511, t=275.49743152114, r=309.9874305085408, b=283.3499715644973, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='Kernel', bbox=BoundingBox(l=345.9925505676079, t=276.30102152557697, r=365.1973305991138, b=284.1372015688439, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='Destination Pixel', bbox=BoundingBox(l=397.25488065170487, t=277.072381529836, r=446.12943073188455, b=284.9249815731936, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='0', bbox=BoundingBox(l=174.49387028626077, t=275.435301520797, r=177.37701029099065, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='0', bbox=BoundingBox(l=186.11195030532048, t=275.435301520797, r=188.99509031005033, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='0', bbox=BoundingBox(l=174.49387028626077, t=287.05633158496175, r=177.37701029099065, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='1', bbox=BoundingBox(l=186.11195030532048, t=287.05633158496175, r=188.99509031005033, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='0', bbox=BoundingBox(l=197.83917032455923, t=275.435301520797, r=200.7223103292891, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='2', bbox=BoundingBox(l=197.83917032455923, t=287.05633158496175, r=200.7223103292891, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='0', bbox=BoundingBox(l=174.49387028626077, t=298.8851916502739, r=177.36963029097853, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='1', bbox=BoundingBox(l=186.11195030532048, t=298.8851916502739, r=188.98772031003824, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='1', bbox=BoundingBox(l=197.83917032455923, t=298.8851916502739, r=200.714940329277, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='0', bbox=BoundingBox(l=209.65488034394315, t=275.435301520797, r=212.53799034867296, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='0', bbox=BoundingBox(l=221.269990362998, t=275.435301520797, r=224.15314036772787, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='1', bbox=BoundingBox(l=209.65488034394315, t=287.05633158496175, r=212.53799034867296, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='1', bbox=BoundingBox(l=221.269990362998, t=287.05633158496175, r=224.15314036772787, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='0', bbox=BoundingBox(l=232.99721038223674, t=275.435301520797, r=235.88034038696654, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='2', bbox=BoundingBox(l=232.99721038223674, t=287.05633158496175, r=235.88034038696654, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='1', bbox=BoundingBox(l=209.65488034394315, t=298.8851916502739, r=212.5306403486609, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='1', bbox=BoundingBox(l=221.269990362998, t=298.8851916502739, r=224.14575036771572, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='1', bbox=BoundingBox(l=232.99721038223674, t=298.8851916502739, r=235.87297038695448, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='1', bbox=BoundingBox(l=174.49387028626077, t=310.6949417154807, r=177.36963029097853, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='0', bbox=BoundingBox(l=186.11195030532048, t=310.6949417154807, r=188.98772031003824, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='0', bbox=BoundingBox(l=174.49387028626077, t=322.31604177964573, r=177.36963029097853, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='0', bbox=BoundingBox(l=186.11195030532048, t=322.31604177964573, r=188.98772031003824, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='0', bbox=BoundingBox(l=197.83917032455923, t=310.6949417154807, r=200.714940329277, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='1', bbox=BoundingBox(l=197.83917032455923, t=322.31604177964573, r=200.714940329277, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='0', bbox=BoundingBox(l=174.49387028626077, t=334.1317118448851, r=177.36963029097853, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='1', bbox=BoundingBox(l=186.11195030532048, t=334.1317118448851, r=188.98772031003824, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='1', bbox=BoundingBox(l=197.83917032455923, t=334.1317118448851, r=200.714940329277, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='0', bbox=BoundingBox(l=209.65488034394315, t=310.6949417154807, r=212.5306403486609, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='0', bbox=BoundingBox(l=221.269990362998, t=310.6949417154807, r=224.14575036771572, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='1', bbox=BoundingBox(l=209.65488034394315, t=322.31604177964573, r=212.5306403486609, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=65, text='1', bbox=BoundingBox(l=221.269990362998, t=322.31604177964573, r=224.14575036771572, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='0', bbox=BoundingBox(l=232.99721038223674, t=310.6949417154807, r=235.87297038695448, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='0', bbox=BoundingBox(l=232.99721038223674, t=322.31604177964573, r=235.87297038695448, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=68, text='1', bbox=BoundingBox(l=209.65488034394315, t=334.1317118448851, r=212.5306403486609, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=69, text='1', bbox=BoundingBox(l=221.269990362998, t=334.1317118448851, r=224.14575036771572, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=70, text='1', bbox=BoundingBox(l=232.99721038223674, t=334.1317118448851, r=235.87297038695448, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=71, text='Input Vector', bbox=BoundingBox(l=187.20328030711084, t=257.09973141955834, r=223.31931036635996, b=264.9359714628257, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=5, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=134.05441284179688, t=354.8834533691406, r=480.58676078841245, b=389.0554521481427, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9449623823165894, cells=[Cell(id=72, text='Fig. 4: A visual representation of a convolutional layer. The centre element of the', bbox=BoundingBox(l=134.76500022108476, t=355.21276196128264, r=480.58676078841245, b=365.1454720161255, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=73, text='kernel is placed over the input vector, of which is then calculated and replaced', bbox=BoundingBox(l=134.76500022108476, t=367.16775202729133, r=480.5867307884124, b=377.1004620821341, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=74, text='with a weighted sum of itself and any nearby pixels.', bbox=BoundingBox(l=134.76500022108476, t=379.12274209329996, r=365.1202105989872, b=389.0554521481427, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.9464569091797, t=415.9989318847656, r=480.7759707887229, b=450.8507385253906, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9816268682479858, cells=[Cell(id=75, text='Every kernel will have a corresponding activation map, of which will be stacked', bbox=BoundingBox(l=134.76500022108476, t=416.8167423014248, r=480.7759707887229, b=426.74945235626757, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=76, text='along the depth dimension to form the full output volume from the convolu-', bbox=BoundingBox(l=134.76500022108476, t=428.77172236743337, r=480.58682078841264, b=438.7044324222761, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=77, text='tional layer.', bbox=BoundingBox(l=134.76500022108476, t=440.72671243344206, r=186.40115030579494, b=450.6594224882848, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.86756896972656, t=460.1748962402344, r=480.5912807884199, b=542.937255859375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9872538447380066, cells=[Cell(id=78, text='As we alluded to earlier, training ANNs on inputs such as images results in', bbox=BoundingBox(l=134.76500022108476, t=460.9447025450743, r=480.58691078841275, b=470.8774125999171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=79, text='models of which are too big to train effectively. This comes down to the fully-', bbox=BoundingBox(l=134.76500022108476, t=472.89968261108294, r=480.58676078841245, b=482.8323926659257, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=80, text='connected manner of standard ANN neurons, so to mitigate against this every', bbox=BoundingBox(l=134.76500022108476, t=484.8546726770915, r=480.58679078841254, b=494.7873827319343, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=81, text='neuron in a convolutional layer is only connected to small region of the input', bbox=BoundingBox(l=134.76500022108476, t=496.80966274310015, r=480.5867307884124, b=506.74237279794295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=82, text='volume. The dimensionality of this region is commonly referred to as the', bbox=BoundingBox(l=134.76500022108476, t=508.7646428091088, r=465.0551807629326, b=518.6973528639516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=83, text='re-', bbox=BoundingBox(l=468.4169907684477, t=508.83438280949383, r=480.5912807884199, b=518.5379628630715, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=84, text='ceptive field size', bbox=BoundingBox(l=134.76498022108473, t=520.7893628755024, r=210.161930344775, b=530.49295292908, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=85, text='of the neuron. The magnitude of the connectivity through the', bbox=BoundingBox(l=212.4549903485368, t=520.7196328751174, r=480.5884407884152, b=530.6523429299602, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=86, text='depth is nearly always equal to the depth of the input.', bbox=BoundingBox(l=134.76498022108473, t=532.6756229411317, r=373.18994061222577, b=542.6083329959744, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.9088134765625, t=552.2114868164062, r=480.59470078842554, b=622.6449584960938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9872121214866638, cells=[Cell(id=87, text='For example, if the input to the network is an image of size', bbox=BoundingBox(l=134.76498022108473, t=552.8926330527584, r=392.8262006444395, b=562.8253431076012, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=88, text='64', bbox=BoundingBox(l=395.26599064844197, t=553.1018630539137, r=405.22858066478585, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=89, text='×', bbox=BoundingBox(l=407.24899066810036, t=552.5439430508331, r=414.9978906808126, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=90, text='64', bbox=BoundingBox(l=417.0189806841282, t=553.1018630539137, r=426.98157070047205, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=91, text='×', bbox=BoundingBox(l=429.00299070378827, t=552.5439430508331, r=436.7518907165005, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=92, text='3', bbox=BoundingBox(l=438.77298071981613, t=553.1018630539137, r=443.75427072798806, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=93, text='(a RGB-', bbox=BoundingBox(l=446.1929907319888, t=552.8926330527584, r=480.5938407884241, b=562.8253431076012, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=94, text='coloured image with a dimensionality of', bbox=BoundingBox(l=134.76498022108473, t=564.847623118767, r=312.8862905132964, b=574.7803331736098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=95, text='64', bbox=BoundingBox(l=315.2929705172446, t=565.0568531199224, r=325.25555053358846, b=573.9036231687692, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=96, text='×', bbox=BoundingBox(l=327.1609805367143, t=564.4989331168418, r=334.90988054942653, b=573.9036231687692, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=97, text='64', bbox=BoundingBox(l=336.8149705525519, t=565.0568531199224, r=346.7775605688957, b=573.9036231687692, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=98, text=') and we set the receptive field', bbox=BoundingBox(l=346.77698056889477, t=564.847623118767, r=480.59470078842554, b=574.7803331736098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=99, text='size as', bbox=BoundingBox(l=134.76498022108473, t=576.8026131847757, r=163.6963802685473, b=586.7353232396185, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=100, text='6', bbox=BoundingBox(l=166.5449802732205, t=577.011843185931, r=171.52628028139242, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=101, text='×', bbox=BoundingBox(l=174.00598028546037, t=576.4539131828503, r=181.75490029817266, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=102, text='6', bbox=BoundingBox(l=184.2349903022413, t=577.011843185931, r=189.21628031041323, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=103, text=', we would have a total of', bbox=BoundingBox(l=189.21599031041274, t=576.8026131847757, r=305.3599505009493, b=586.7353232396185, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=104, text='108', bbox=BoundingBox(l=308.2089805056232, t=577.011843185931, r=323.15289053013896, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=105, text='weights on each neuron within the', bbox=BoundingBox(l=326.00299053481467, t=576.8026131847757, r=480.5926807884222, b=586.7353232396185, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=106, text='convolutional layer. (', bbox=BoundingBox(l=134.76498022108473, t=588.7576132507844, r=228.17432037432468, b=598.6903233056272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=107, text='6', bbox=BoundingBox(l=228.1749903743258, t=588.9668232519396, r=233.1562803824977, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=108, text='×', bbox=BoundingBox(l=235.49298038633108, t=588.4089232488592, r=243.24190039904332, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=109, text='6', bbox=BoundingBox(l=245.57799040287574, t=588.9668232519396, r=250.55928041104767, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=110, text='×', bbox=BoundingBox(l=252.8959804148811, t=588.4089232488592, r=260.6449004275933, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=111, text='3', bbox=BoundingBox(l=262.98099043142577, t=588.9668232519396, r=267.9622804395977, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=112, text='where', bbox=BoundingBox(l=270.618990443956, t=588.7576132507844, r=298.02612048891797, b=598.6903233056272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=113, text='3', bbox=BoundingBox(l=300.68198049327503, t=588.9668232519396, r=305.6632705014469, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=114, text='is the magnitude of connectivity across', bbox=BoundingBox(l=308.3189705058036, t=588.7576132507844, r=480.5922207884214, b=598.6903233056272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=115, text='the depth of the volume) To put this into perspective, a standard neuron seen', bbox=BoundingBox(l=134.7649702210847, t=600.7126133167931, r=480.5867307884124, b=610.6453233716359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=116, text='in other forms of ANN would contain', bbox=BoundingBox(l=134.7649702210847, t=612.6686033828073, r=301.70825049495863, b=622.6013134376501, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=117, text='12', bbox=BoundingBox(l=304.1969604990414, t=612.8778233839625, r=314.1595505153852, b=621.7246034328093, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=118, text=',', bbox=BoundingBox(l=314.1599705153859, t=612.8778233839625, r=316.9275805199262, b=621.7246034328093, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=119, text='288', bbox=BoundingBox(l=318.5879805226502, t=612.8778233839625, r=333.5318905471659, b=621.7246034328093, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=120, text='weights each.', bbox=BoundingBox(l=336.0229805512526, t=612.6686033828073, r=395.7687106492667, b=622.6013134376501, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.94898986816406, t=632.0844116210938, r=480.5867307884124, b=666.8765258789062, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9835361242294312, cells=[Cell(id=121, text='Convolutional layers are also able to significantly reduce the complexity of the', bbox=BoundingBox(l=134.76498022108473, t=632.885603494434, r=480.5867307884124, b=642.8183135492768, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=122, text='model through the optimisation of its output. These are optimised through', bbox=BoundingBox(l=134.76498022108473, t=644.8406035604428, r=480.5867307884124, b=654.7733136152856, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=123, text='three hyperparameters, the', bbox=BoundingBox(l=134.76498022108473, t=656.7956036264516, r=254.06711041680234, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=124, text='depth', bbox=BoundingBox(l=256.5559704208854, t=656.8653436268365, r=283.11627046445807, b=666.5689236804142, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=125, text=', the', bbox=BoundingBox(l=283.1169704644592, t=656.7956036264516, r=301.9164104953001, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=126, text='stride', bbox=BoundingBox(l=304.4069804993859, t=656.8653436268365, r=330.4093605420433, b=666.5689236804142, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=127, text='and setting', bbox=BoundingBox(l=332.8999905461293, t=656.7956036264516, r=381.98572062665545, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=128, text='zero-padding', bbox=BoundingBox(l=384.47598063074076, t=656.8653436268365, r=445.35742073061806, b=666.5689236804142, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=129, text='.', bbox=BoundingBox(l=445.3569907306173, t=656.7956036264516, r=447.84766073470337, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))])]), tablestructure=TableStructurePrediction(table_map={}), figures_classification=None, equations_prediction=None), assembled=AssembledUnit(elements=[TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=5, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=133.78485107421875, t=94.09490966796875, r=139.71607971191406, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7533239722251892, cells=[Cell(id=0, text='6', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='6'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=5, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=167.21620178222656, t=93.54159545898438, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7271904349327087, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea et al.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=5, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.7237091064453, t=118.72857666015625, r=480.7948913574219, b=177.42456097963736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9832537770271301, cells=[Cell(id=2, text='These kernels are usually small in spatial dimensionality, but spreads along the', bbox=BoundingBox(l=134.76500022108476, t=119.67181066075966, r=480.58676078841245, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='entirety of the depth of the input. When the data hits a convolutional layer,', bbox=BoundingBox(l=134.76500022108476, t=131.6268307267684, r=480.58679078841254, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='the layer convolves each filter across the spatial dimensionality of the input to', bbox=BoundingBox(l=134.76500022108476, t=143.58184079277714, r=480.58676078841245, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='produce a 2D activation map. These activation maps can be visualised, as seen', bbox=BoundingBox(l=134.76500022108476, t=155.53686085878599, r=480.58676078841245, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='in Figure 3.', bbox=BoundingBox(l=134.76500022108476, t=167.49188092479483, r=184.42854030255882, b=177.42456097963736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='These kernels are usually small in spatial dimensionality, but spreads along the entirety of the depth of the input. When the data hits a convolutional layer, the layer convolves each filter across the spatial dimensionality of the input to produce a 2D activation map. These activation maps can be visualised, as seen in Figure 3.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=5, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.92149353027344, t=186.75308227539062, r=480.5972595214844, b=233.50762128929614, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9789468050003052, cells=[Cell(id=7, text='As we glide through the input, the scalar product is calculated for each value in', bbox=BoundingBox(l=134.76500022108476, t=187.70990103642725, r=480.58679078841254, b=197.6425710912697, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='that kernel. (Figure 4) From this the network will learn kernels that ’fire’ when', bbox=BoundingBox(l=134.76500022108476, t=199.6649111024359, r=480.5867307884124, b=209.59759115727866, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='they see a specific feature at a given spatial position of the input. These are', bbox=BoundingBox(l=134.76500022108476, t=211.61993116844474, r=480.58670078841243, b=221.55261122328739, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='commonly known as', bbox=BoundingBox(l=134.76500022108476, t=223.57495123445358, r=226.9688603723471, b=233.50762128929614, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='activations', bbox=BoundingBox(l=229.45900037643224, t=223.64465123483842, r=278.7041304572199, b=233.3482012884159, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='.', bbox=BoundingBox(l=278.7040104572196, t=223.57495123445358, r=281.19467046130563, b=233.50762128929614, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='As we glide through the input, the scalar product is calculated for each value in that kernel. (Figure 4) From this the network will learn kernels that ’fire’ when they see a specific feature at a given spatial position of the input. These are commonly known as activations .'), FigureElement(label=<DocItemLabel.PICTURE: 'picture'>, id=4, page_no=5, cluster=Cluster(id=4, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=168.14492797851562, t=257.09973141955834, r=446.12943073188455, b=344.0976257324219, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.922330915927887, cells=[Cell(id=13, text='0', bbox=BoundingBox(l=276.3548604533658, t=293.0746416181913, r=279.2306204580836, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='0', bbox=BoundingBox(l=287.97589047243036, t=293.0746416181913, r=290.85165047714816, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='0', bbox=BoundingBox(l=276.3548604533658, t=304.6956716823561, r=279.2306204580836, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='1', bbox=BoundingBox(l=287.97589047243036, t=304.6956716823561, r=290.85165047714816, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='0', bbox=BoundingBox(l=299.70312049166915, t=293.0746416181913, r=302.5788604963869, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='2', bbox=BoundingBox(l=299.70312049166915, t=304.6956716823561, r=302.5788604963869, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='0', bbox=BoundingBox(l=276.3548604533658, t=316.51141174759584, r=279.2306204580836, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='1', bbox=BoundingBox(l=287.97589047243036, t=316.51141174759584, r=290.85165047714816, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='1', bbox=BoundingBox(l=299.70312049166915, t=316.51141174759584, r=302.5788604963869, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='4', bbox=BoundingBox(l=342.4826705618499, t=293.0746416181913, r=345.3584005665676, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='0', bbox=BoundingBox(l=354.1214005809435, t=293.0746416181913, r=356.9971305856612, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='0', bbox=BoundingBox(l=342.4826705618499, t=304.6956716823561, r=345.3584005665676, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='0', bbox=BoundingBox(l=354.1214005809435, t=304.6956716823561, r=356.9971305856612, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='0', bbox=BoundingBox(l=365.8308706001531, t=293.0746416181913, r=368.70667060487085, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='0', bbox=BoundingBox(l=365.8308706001531, t=304.6956716823561, r=368.70667060487085, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='0', bbox=BoundingBox(l=342.4826705618499, t=316.51141174759584, r=345.3584005665676, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='0', bbox=BoundingBox(l=354.1214005809435, t=316.51141174759584, r=356.9971305856612, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='-4', bbox=BoundingBox(l=364.8516805985467, t=316.51141174759584, r=369.69177060648695, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='-8', bbox=BoundingBox(l=419.2581506878016, t=304.6956716823561, r=424.1100806957613, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='Pooled Vector', bbox=BoundingBox(l=269.128600441511, t=275.49743152114, r=309.9874305085408, b=283.3499715644973, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='Kernel', bbox=BoundingBox(l=345.9925505676079, t=276.30102152557697, r=365.1973305991138, b=284.1372015688439, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='Destination Pixel', bbox=BoundingBox(l=397.25488065170487, t=277.072381529836, r=446.12943073188455, b=284.9249815731936, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='0', bbox=BoundingBox(l=174.49387028626077, t=275.435301520797, r=177.37701029099065, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='0', bbox=BoundingBox(l=186.11195030532048, t=275.435301520797, r=188.99509031005033, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='0', bbox=BoundingBox(l=174.49387028626077, t=287.05633158496175, r=177.37701029099065, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='1', bbox=BoundingBox(l=186.11195030532048, t=287.05633158496175, r=188.99509031005033, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='0', bbox=BoundingBox(l=197.83917032455923, t=275.435301520797, r=200.7223103292891, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='2', bbox=BoundingBox(l=197.83917032455923, t=287.05633158496175, r=200.7223103292891, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='0', bbox=BoundingBox(l=174.49387028626077, t=298.8851916502739, r=177.36963029097853, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='1', bbox=BoundingBox(l=186.11195030532048, t=298.8851916502739, r=188.98772031003824, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='1', bbox=BoundingBox(l=197.83917032455923, t=298.8851916502739, r=200.714940329277, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='0', bbox=BoundingBox(l=209.65488034394315, t=275.435301520797, r=212.53799034867296, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='0', bbox=BoundingBox(l=221.269990362998, t=275.435301520797, r=224.15314036772787, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='1', bbox=BoundingBox(l=209.65488034394315, t=287.05633158496175, r=212.53799034867296, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='1', bbox=BoundingBox(l=221.269990362998, t=287.05633158496175, r=224.15314036772787, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='0', bbox=BoundingBox(l=232.99721038223674, t=275.435301520797, r=235.88034038696654, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='2', bbox=BoundingBox(l=232.99721038223674, t=287.05633158496175, r=235.88034038696654, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='1', bbox=BoundingBox(l=209.65488034394315, t=298.8851916502739, r=212.5306403486609, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='1', bbox=BoundingBox(l=221.269990362998, t=298.8851916502739, r=224.14575036771572, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='1', bbox=BoundingBox(l=232.99721038223674, t=298.8851916502739, r=235.87297038695448, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='1', bbox=BoundingBox(l=174.49387028626077, t=310.6949417154807, r=177.36963029097853, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='0', bbox=BoundingBox(l=186.11195030532048, t=310.6949417154807, r=188.98772031003824, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='0', bbox=BoundingBox(l=174.49387028626077, t=322.31604177964573, r=177.36963029097853, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='0', bbox=BoundingBox(l=186.11195030532048, t=322.31604177964573, r=188.98772031003824, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='0', bbox=BoundingBox(l=197.83917032455923, t=310.6949417154807, r=200.714940329277, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='1', bbox=BoundingBox(l=197.83917032455923, t=322.31604177964573, r=200.714940329277, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='0', bbox=BoundingBox(l=174.49387028626077, t=334.1317118448851, r=177.36963029097853, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='1', bbox=BoundingBox(l=186.11195030532048, t=334.1317118448851, r=188.98772031003824, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='1', bbox=BoundingBox(l=197.83917032455923, t=334.1317118448851, r=200.714940329277, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='0', bbox=BoundingBox(l=209.65488034394315, t=310.6949417154807, r=212.5306403486609, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='0', bbox=BoundingBox(l=221.269990362998, t=310.6949417154807, r=224.14575036771572, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='1', bbox=BoundingBox(l=209.65488034394315, t=322.31604177964573, r=212.5306403486609, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=65, text='1', bbox=BoundingBox(l=221.269990362998, t=322.31604177964573, r=224.14575036771572, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='0', bbox=BoundingBox(l=232.99721038223674, t=310.6949417154807, r=235.87297038695448, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='0', bbox=BoundingBox(l=232.99721038223674, t=322.31604177964573, r=235.87297038695448, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=68, text='1', bbox=BoundingBox(l=209.65488034394315, t=334.1317118448851, r=212.5306403486609, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=69, text='1', bbox=BoundingBox(l=221.269990362998, t=334.1317118448851, r=224.14575036771572, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=70, text='1', bbox=BoundingBox(l=232.99721038223674, t=334.1317118448851, r=235.87297038695448, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=71, text='Input Vector', bbox=BoundingBox(l=187.20328030711084, t=257.09973141955834, r=223.31931036635996, b=264.9359714628257, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='', annotations=[], provenance=None, predicted_class=None, confidence=None), TextElement(label=<DocItemLabel.CAPTION: 'caption'>, id=5, page_no=5, cluster=Cluster(id=5, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=134.05441284179688, t=354.8834533691406, r=480.58676078841245, b=389.0554521481427, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9449623823165894, cells=[Cell(id=72, text='Fig. 4: A visual representation of a convolutional layer. The centre element of the', bbox=BoundingBox(l=134.76500022108476, t=355.21276196128264, r=480.58676078841245, b=365.1454720161255, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=73, text='kernel is placed over the input vector, of which is then calculated and replaced', bbox=BoundingBox(l=134.76500022108476, t=367.16775202729133, r=480.5867307884124, b=377.1004620821341, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=74, text='with a weighted sum of itself and any nearby pixels.', bbox=BoundingBox(l=134.76500022108476, t=379.12274209329996, r=365.1202105989872, b=389.0554521481427, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Fig. 4: A visual representation of a convolutional layer. The centre element of the kernel is placed over the input vector, of which is then calculated and replaced with a weighted sum of itself and any nearby pixels.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=5, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.9464569091797, t=415.9989318847656, r=480.7759707887229, b=450.8507385253906, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9816268682479858, cells=[Cell(id=75, text='Every kernel will have a corresponding activation map, of which will be stacked', bbox=BoundingBox(l=134.76500022108476, t=416.8167423014248, r=480.7759707887229, b=426.74945235626757, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=76, text='along the depth dimension to form the full output volume from the convolu-', bbox=BoundingBox(l=134.76500022108476, t=428.77172236743337, r=480.58682078841264, b=438.7044324222761, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=77, text='tional layer.', bbox=BoundingBox(l=134.76500022108476, t=440.72671243344206, r=186.40115030579494, b=450.6594224882848, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Every kernel will have a corresponding activation map, of which will be stacked along the depth dimension to form the full output volume from the convolutional layer.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=5, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.86756896972656, t=460.1748962402344, r=480.5912807884199, b=542.937255859375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9872538447380066, cells=[Cell(id=78, text='As we alluded to earlier, training ANNs on inputs such as images results in', bbox=BoundingBox(l=134.76500022108476, t=460.9447025450743, r=480.58691078841275, b=470.8774125999171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=79, text='models of which are too big to train effectively. This comes down to the fully-', bbox=BoundingBox(l=134.76500022108476, t=472.89968261108294, r=480.58676078841245, b=482.8323926659257, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=80, text='connected manner of standard ANN neurons, so to mitigate against this every', bbox=BoundingBox(l=134.76500022108476, t=484.8546726770915, r=480.58679078841254, b=494.7873827319343, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=81, text='neuron in a convolutional layer is only connected to small region of the input', bbox=BoundingBox(l=134.76500022108476, t=496.80966274310015, r=480.5867307884124, b=506.74237279794295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=82, text='volume. The dimensionality of this region is commonly referred to as the', bbox=BoundingBox(l=134.76500022108476, t=508.7646428091088, r=465.0551807629326, b=518.6973528639516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=83, text='re-', bbox=BoundingBox(l=468.4169907684477, t=508.83438280949383, r=480.5912807884199, b=518.5379628630715, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=84, text='ceptive field size', bbox=BoundingBox(l=134.76498022108473, t=520.7893628755024, r=210.161930344775, b=530.49295292908, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=85, text='of the neuron. The magnitude of the connectivity through the', bbox=BoundingBox(l=212.4549903485368, t=520.7196328751174, r=480.5884407884152, b=530.6523429299602, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=86, text='depth is nearly always equal to the depth of the input.', bbox=BoundingBox(l=134.76498022108473, t=532.6756229411317, r=373.18994061222577, b=542.6083329959744, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='As we alluded to earlier, training ANNs on inputs such as images results in models of which are too big to train effectively. This comes down to the fullyconnected manner of standard ANN neurons, so to mitigate against this every neuron in a convolutional layer is only connected to small region of the input volume. The dimensionality of this region is commonly referred to as the receptive field size of the neuron. The magnitude of the connectivity through the depth is nearly always equal to the depth of the input.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=5, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.9088134765625, t=552.2114868164062, r=480.59470078842554, b=622.6449584960938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9872121214866638, cells=[Cell(id=87, text='For example, if the input to the network is an image of size', bbox=BoundingBox(l=134.76498022108473, t=552.8926330527584, r=392.8262006444395, b=562.8253431076012, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=88, text='64', bbox=BoundingBox(l=395.26599064844197, t=553.1018630539137, r=405.22858066478585, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=89, text='×', bbox=BoundingBox(l=407.24899066810036, t=552.5439430508331, r=414.9978906808126, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=90, text='64', bbox=BoundingBox(l=417.0189806841282, t=553.1018630539137, r=426.98157070047205, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=91, text='×', bbox=BoundingBox(l=429.00299070378827, t=552.5439430508331, r=436.7518907165005, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=92, text='3', bbox=BoundingBox(l=438.77298071981613, t=553.1018630539137, r=443.75427072798806, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=93, text='(a RGB-', bbox=BoundingBox(l=446.1929907319888, t=552.8926330527584, r=480.5938407884241, b=562.8253431076012, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=94, text='coloured image with a dimensionality of', bbox=BoundingBox(l=134.76498022108473, t=564.847623118767, r=312.8862905132964, b=574.7803331736098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=95, text='64', bbox=BoundingBox(l=315.2929705172446, t=565.0568531199224, r=325.25555053358846, b=573.9036231687692, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=96, text='×', bbox=BoundingBox(l=327.1609805367143, t=564.4989331168418, r=334.90988054942653, b=573.9036231687692, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=97, text='64', bbox=BoundingBox(l=336.8149705525519, t=565.0568531199224, r=346.7775605688957, b=573.9036231687692, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=98, text=') and we set the receptive field', bbox=BoundingBox(l=346.77698056889477, t=564.847623118767, r=480.59470078842554, b=574.7803331736098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=99, text='size as', bbox=BoundingBox(l=134.76498022108473, t=576.8026131847757, r=163.6963802685473, b=586.7353232396185, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=100, text='6', bbox=BoundingBox(l=166.5449802732205, t=577.011843185931, r=171.52628028139242, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=101, text='×', bbox=BoundingBox(l=174.00598028546037, t=576.4539131828503, r=181.75490029817266, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=102, text='6', bbox=BoundingBox(l=184.2349903022413, t=577.011843185931, r=189.21628031041323, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=103, text=', we would have a total of', bbox=BoundingBox(l=189.21599031041274, t=576.8026131847757, r=305.3599505009493, b=586.7353232396185, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=104, text='108', bbox=BoundingBox(l=308.2089805056232, t=577.011843185931, r=323.15289053013896, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=105, text='weights on each neuron within the', bbox=BoundingBox(l=326.00299053481467, t=576.8026131847757, r=480.5926807884222, b=586.7353232396185, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=106, text='convolutional layer. (', bbox=BoundingBox(l=134.76498022108473, t=588.7576132507844, r=228.17432037432468, b=598.6903233056272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=107, text='6', bbox=BoundingBox(l=228.1749903743258, t=588.9668232519396, r=233.1562803824977, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=108, text='×', bbox=BoundingBox(l=235.49298038633108, t=588.4089232488592, r=243.24190039904332, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=109, text='6', bbox=BoundingBox(l=245.57799040287574, t=588.9668232519396, r=250.55928041104767, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=110, text='×', bbox=BoundingBox(l=252.8959804148811, t=588.4089232488592, r=260.6449004275933, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=111, text='3', bbox=BoundingBox(l=262.98099043142577, t=588.9668232519396, r=267.9622804395977, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=112, text='where', bbox=BoundingBox(l=270.618990443956, t=588.7576132507844, r=298.02612048891797, b=598.6903233056272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=113, text='3', bbox=BoundingBox(l=300.68198049327503, t=588.9668232519396, r=305.6632705014469, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=114, text='is the magnitude of connectivity across', bbox=BoundingBox(l=308.3189705058036, t=588.7576132507844, r=480.5922207884214, b=598.6903233056272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=115, text='the depth of the volume) To put this into perspective, a standard neuron seen', bbox=BoundingBox(l=134.7649702210847, t=600.7126133167931, r=480.5867307884124, b=610.6453233716359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=116, text='in other forms of ANN would contain', bbox=BoundingBox(l=134.7649702210847, t=612.6686033828073, r=301.70825049495863, b=622.6013134376501, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=117, text='12', bbox=BoundingBox(l=304.1969604990414, t=612.8778233839625, r=314.1595505153852, b=621.7246034328093, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=118, text=',', bbox=BoundingBox(l=314.1599705153859, t=612.8778233839625, r=316.9275805199262, b=621.7246034328093, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=119, text='288', bbox=BoundingBox(l=318.5879805226502, t=612.8778233839625, r=333.5318905471659, b=621.7246034328093, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=120, text='weights each.', bbox=BoundingBox(l=336.0229805512526, t=612.6686033828073, r=395.7687106492667, b=622.6013134376501, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='For example, if the input to the network is an image of size 64 × 64 × 3 (a RGBcoloured image with a dimensionality of 64 × 64 ) and we set the receptive field size as 6 × 6 , we would have a total of 108 weights on each neuron within the convolutional layer. ( 6 × 6 × 3 where 3 is the magnitude of connectivity across the depth of the volume) To put this into perspective, a standard neuron seen in other forms of ANN would contain 12 , 288 weights each.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=9, page_no=5, cluster=Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.94898986816406, t=632.0844116210938, r=480.5867307884124, b=666.8765258789062, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9835361242294312, cells=[Cell(id=121, text='Convolutional layers are also able to significantly reduce the complexity of the', bbox=BoundingBox(l=134.76498022108473, t=632.885603494434, r=480.5867307884124, b=642.8183135492768, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=122, text='model through the optimisation of its output. These are optimised through', bbox=BoundingBox(l=134.76498022108473, t=644.8406035604428, r=480.5867307884124, b=654.7733136152856, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=123, text='three hyperparameters, the', bbox=BoundingBox(l=134.76498022108473, t=656.7956036264516, r=254.06711041680234, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=124, text='depth', bbox=BoundingBox(l=256.5559704208854, t=656.8653436268365, r=283.11627046445807, b=666.5689236804142, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=125, text=', the', bbox=BoundingBox(l=283.1169704644592, t=656.7956036264516, r=301.9164104953001, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=126, text='stride', bbox=BoundingBox(l=304.4069804993859, t=656.8653436268365, r=330.4093605420433, b=666.5689236804142, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=127, text='and setting', bbox=BoundingBox(l=332.8999905461293, t=656.7956036264516, r=381.98572062665545, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=128, text='zero-padding', bbox=BoundingBox(l=384.47598063074076, t=656.8653436268365, r=445.35742073061806, b=666.5689236804142, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=129, text='.', bbox=BoundingBox(l=445.3569907306173, t=656.7956036264516, r=447.84766073470337, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Convolutional layers are also able to significantly reduce the complexity of the model through the optimisation of its output. These are optimised through three hyperparameters, the depth , the stride and setting zero-padding .')], body=[TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=5, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.7237091064453, t=118.72857666015625, r=480.7948913574219, b=177.42456097963736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9832537770271301, cells=[Cell(id=2, text='These kernels are usually small in spatial dimensionality, but spreads along the', bbox=BoundingBox(l=134.76500022108476, t=119.67181066075966, r=480.58676078841245, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='entirety of the depth of the input. When the data hits a convolutional layer,', bbox=BoundingBox(l=134.76500022108476, t=131.6268307267684, r=480.58679078841254, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='the layer convolves each filter across the spatial dimensionality of the input to', bbox=BoundingBox(l=134.76500022108476, t=143.58184079277714, r=480.58676078841245, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='produce a 2D activation map. These activation maps can be visualised, as seen', bbox=BoundingBox(l=134.76500022108476, t=155.53686085878599, r=480.58676078841245, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='in Figure 3.', bbox=BoundingBox(l=134.76500022108476, t=167.49188092479483, r=184.42854030255882, b=177.42456097963736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='These kernels are usually small in spatial dimensionality, but spreads along the entirety of the depth of the input. When the data hits a convolutional layer, the layer convolves each filter across the spatial dimensionality of the input to produce a 2D activation map. These activation maps can be visualised, as seen in Figure 3.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=5, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.92149353027344, t=186.75308227539062, r=480.5972595214844, b=233.50762128929614, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9789468050003052, cells=[Cell(id=7, text='As we glide through the input, the scalar product is calculated for each value in', bbox=BoundingBox(l=134.76500022108476, t=187.70990103642725, r=480.58679078841254, b=197.6425710912697, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='that kernel. (Figure 4) From this the network will learn kernels that ’fire’ when', bbox=BoundingBox(l=134.76500022108476, t=199.6649111024359, r=480.5867307884124, b=209.59759115727866, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='they see a specific feature at a given spatial position of the input. These are', bbox=BoundingBox(l=134.76500022108476, t=211.61993116844474, r=480.58670078841243, b=221.55261122328739, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='commonly known as', bbox=BoundingBox(l=134.76500022108476, t=223.57495123445358, r=226.9688603723471, b=233.50762128929614, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='activations', bbox=BoundingBox(l=229.45900037643224, t=223.64465123483842, r=278.7041304572199, b=233.3482012884159, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='.', bbox=BoundingBox(l=278.7040104572196, t=223.57495123445358, r=281.19467046130563, b=233.50762128929614, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='As we glide through the input, the scalar product is calculated for each value in that kernel. (Figure 4) From this the network will learn kernels that ’fire’ when they see a specific feature at a given spatial position of the input. These are commonly known as activations .'), FigureElement(label=<DocItemLabel.PICTURE: 'picture'>, id=4, page_no=5, cluster=Cluster(id=4, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=168.14492797851562, t=257.09973141955834, r=446.12943073188455, b=344.0976257324219, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.922330915927887, cells=[Cell(id=13, text='0', bbox=BoundingBox(l=276.3548604533658, t=293.0746416181913, r=279.2306204580836, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='0', bbox=BoundingBox(l=287.97589047243036, t=293.0746416181913, r=290.85165047714816, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='0', bbox=BoundingBox(l=276.3548604533658, t=304.6956716823561, r=279.2306204580836, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='1', bbox=BoundingBox(l=287.97589047243036, t=304.6956716823561, r=290.85165047714816, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='0', bbox=BoundingBox(l=299.70312049166915, t=293.0746416181913, r=302.5788604963869, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='2', bbox=BoundingBox(l=299.70312049166915, t=304.6956716823561, r=302.5788604963869, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='0', bbox=BoundingBox(l=276.3548604533658, t=316.51141174759584, r=279.2306204580836, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='1', bbox=BoundingBox(l=287.97589047243036, t=316.51141174759584, r=290.85165047714816, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='1', bbox=BoundingBox(l=299.70312049166915, t=316.51141174759584, r=302.5788604963869, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='4', bbox=BoundingBox(l=342.4826705618499, t=293.0746416181913, r=345.3584005665676, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='0', bbox=BoundingBox(l=354.1214005809435, t=293.0746416181913, r=356.9971305856612, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='0', bbox=BoundingBox(l=342.4826705618499, t=304.6956716823561, r=345.3584005665676, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='0', bbox=BoundingBox(l=354.1214005809435, t=304.6956716823561, r=356.9971305856612, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='0', bbox=BoundingBox(l=365.8308706001531, t=293.0746416181913, r=368.70667060487085, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='0', bbox=BoundingBox(l=365.8308706001531, t=304.6956716823561, r=368.70667060487085, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='0', bbox=BoundingBox(l=342.4826705618499, t=316.51141174759584, r=345.3584005665676, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='0', bbox=BoundingBox(l=354.1214005809435, t=316.51141174759584, r=356.9971305856612, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='-4', bbox=BoundingBox(l=364.8516805985467, t=316.51141174759584, r=369.69177060648695, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='-8', bbox=BoundingBox(l=419.2581506878016, t=304.6956716823561, r=424.1100806957613, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='Pooled Vector', bbox=BoundingBox(l=269.128600441511, t=275.49743152114, r=309.9874305085408, b=283.3499715644973, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='Kernel', bbox=BoundingBox(l=345.9925505676079, t=276.30102152557697, r=365.1973305991138, b=284.1372015688439, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='Destination Pixel', bbox=BoundingBox(l=397.25488065170487, t=277.072381529836, r=446.12943073188455, b=284.9249815731936, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='0', bbox=BoundingBox(l=174.49387028626077, t=275.435301520797, r=177.37701029099065, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='0', bbox=BoundingBox(l=186.11195030532048, t=275.435301520797, r=188.99509031005033, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='0', bbox=BoundingBox(l=174.49387028626077, t=287.05633158496175, r=177.37701029099065, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='1', bbox=BoundingBox(l=186.11195030532048, t=287.05633158496175, r=188.99509031005033, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='0', bbox=BoundingBox(l=197.83917032455923, t=275.435301520797, r=200.7223103292891, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='2', bbox=BoundingBox(l=197.83917032455923, t=287.05633158496175, r=200.7223103292891, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='0', bbox=BoundingBox(l=174.49387028626077, t=298.8851916502739, r=177.36963029097853, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='1', bbox=BoundingBox(l=186.11195030532048, t=298.8851916502739, r=188.98772031003824, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='1', bbox=BoundingBox(l=197.83917032455923, t=298.8851916502739, r=200.714940329277, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='0', bbox=BoundingBox(l=209.65488034394315, t=275.435301520797, r=212.53799034867296, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='0', bbox=BoundingBox(l=221.269990362998, t=275.435301520797, r=224.15314036772787, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='1', bbox=BoundingBox(l=209.65488034394315, t=287.05633158496175, r=212.53799034867296, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='1', bbox=BoundingBox(l=221.269990362998, t=287.05633158496175, r=224.15314036772787, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='0', bbox=BoundingBox(l=232.99721038223674, t=275.435301520797, r=235.88034038696654, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='2', bbox=BoundingBox(l=232.99721038223674, t=287.05633158496175, r=235.88034038696654, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='1', bbox=BoundingBox(l=209.65488034394315, t=298.8851916502739, r=212.5306403486609, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='1', bbox=BoundingBox(l=221.269990362998, t=298.8851916502739, r=224.14575036771572, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='1', bbox=BoundingBox(l=232.99721038223674, t=298.8851916502739, r=235.87297038695448, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='1', bbox=BoundingBox(l=174.49387028626077, t=310.6949417154807, r=177.36963029097853, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='0', bbox=BoundingBox(l=186.11195030532048, t=310.6949417154807, r=188.98772031003824, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='0', bbox=BoundingBox(l=174.49387028626077, t=322.31604177964573, r=177.36963029097853, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='0', bbox=BoundingBox(l=186.11195030532048, t=322.31604177964573, r=188.98772031003824, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='0', bbox=BoundingBox(l=197.83917032455923, t=310.6949417154807, r=200.714940329277, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='1', bbox=BoundingBox(l=197.83917032455923, t=322.31604177964573, r=200.714940329277, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='0', bbox=BoundingBox(l=174.49387028626077, t=334.1317118448851, r=177.36963029097853, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='1', bbox=BoundingBox(l=186.11195030532048, t=334.1317118448851, r=188.98772031003824, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='1', bbox=BoundingBox(l=197.83917032455923, t=334.1317118448851, r=200.714940329277, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='0', bbox=BoundingBox(l=209.65488034394315, t=310.6949417154807, r=212.5306403486609, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='0', bbox=BoundingBox(l=221.269990362998, t=310.6949417154807, r=224.14575036771572, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='1', bbox=BoundingBox(l=209.65488034394315, t=322.31604177964573, r=212.5306403486609, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=65, text='1', bbox=BoundingBox(l=221.269990362998, t=322.31604177964573, r=224.14575036771572, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='0', bbox=BoundingBox(l=232.99721038223674, t=310.6949417154807, r=235.87297038695448, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='0', bbox=BoundingBox(l=232.99721038223674, t=322.31604177964573, r=235.87297038695448, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=68, text='1', bbox=BoundingBox(l=209.65488034394315, t=334.1317118448851, r=212.5306403486609, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=69, text='1', bbox=BoundingBox(l=221.269990362998, t=334.1317118448851, r=224.14575036771572, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=70, text='1', bbox=BoundingBox(l=232.99721038223674, t=334.1317118448851, r=235.87297038695448, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=71, text='Input Vector', bbox=BoundingBox(l=187.20328030711084, t=257.09973141955834, r=223.31931036635996, b=264.9359714628257, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='', annotations=[], provenance=None, predicted_class=None, confidence=None), TextElement(label=<DocItemLabel.CAPTION: 'caption'>, id=5, page_no=5, cluster=Cluster(id=5, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=134.05441284179688, t=354.8834533691406, r=480.58676078841245, b=389.0554521481427, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9449623823165894, cells=[Cell(id=72, text='Fig. 4: A visual representation of a convolutional layer. The centre element of the', bbox=BoundingBox(l=134.76500022108476, t=355.21276196128264, r=480.58676078841245, b=365.1454720161255, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=73, text='kernel is placed over the input vector, of which is then calculated and replaced', bbox=BoundingBox(l=134.76500022108476, t=367.16775202729133, r=480.5867307884124, b=377.1004620821341, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=74, text='with a weighted sum of itself and any nearby pixels.', bbox=BoundingBox(l=134.76500022108476, t=379.12274209329996, r=365.1202105989872, b=389.0554521481427, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Fig. 4: A visual representation of a convolutional layer. The centre element of the kernel is placed over the input vector, of which is then calculated and replaced with a weighted sum of itself and any nearby pixels.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=5, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.9464569091797, t=415.9989318847656, r=480.7759707887229, b=450.8507385253906, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9816268682479858, cells=[Cell(id=75, text='Every kernel will have a corresponding activation map, of which will be stacked', bbox=BoundingBox(l=134.76500022108476, t=416.8167423014248, r=480.7759707887229, b=426.74945235626757, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=76, text='along the depth dimension to form the full output volume from the convolu-', bbox=BoundingBox(l=134.76500022108476, t=428.77172236743337, r=480.58682078841264, b=438.7044324222761, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=77, text='tional layer.', bbox=BoundingBox(l=134.76500022108476, t=440.72671243344206, r=186.40115030579494, b=450.6594224882848, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Every kernel will have a corresponding activation map, of which will be stacked along the depth dimension to form the full output volume from the convolutional layer.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=5, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.86756896972656, t=460.1748962402344, r=480.5912807884199, b=542.937255859375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9872538447380066, cells=[Cell(id=78, text='As we alluded to earlier, training ANNs on inputs such as images results in', bbox=BoundingBox(l=134.76500022108476, t=460.9447025450743, r=480.58691078841275, b=470.8774125999171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=79, text='models of which are too big to train effectively. This comes down to the fully-', bbox=BoundingBox(l=134.76500022108476, t=472.89968261108294, r=480.58676078841245, b=482.8323926659257, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=80, text='connected manner of standard ANN neurons, so to mitigate against this every', bbox=BoundingBox(l=134.76500022108476, t=484.8546726770915, r=480.58679078841254, b=494.7873827319343, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=81, text='neuron in a convolutional layer is only connected to small region of the input', bbox=BoundingBox(l=134.76500022108476, t=496.80966274310015, r=480.5867307884124, b=506.74237279794295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=82, text='volume. The dimensionality of this region is commonly referred to as the', bbox=BoundingBox(l=134.76500022108476, t=508.7646428091088, r=465.0551807629326, b=518.6973528639516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=83, text='re-', bbox=BoundingBox(l=468.4169907684477, t=508.83438280949383, r=480.5912807884199, b=518.5379628630715, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=84, text='ceptive field size', bbox=BoundingBox(l=134.76498022108473, t=520.7893628755024, r=210.161930344775, b=530.49295292908, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=85, text='of the neuron. The magnitude of the connectivity through the', bbox=BoundingBox(l=212.4549903485368, t=520.7196328751174, r=480.5884407884152, b=530.6523429299602, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=86, text='depth is nearly always equal to the depth of the input.', bbox=BoundingBox(l=134.76498022108473, t=532.6756229411317, r=373.18994061222577, b=542.6083329959744, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='As we alluded to earlier, training ANNs on inputs such as images results in models of which are too big to train effectively. This comes down to the fullyconnected manner of standard ANN neurons, so to mitigate against this every neuron in a convolutional layer is only connected to small region of the input volume. The dimensionality of this region is commonly referred to as the receptive field size of the neuron. The magnitude of the connectivity through the depth is nearly always equal to the depth of the input.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=5, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.9088134765625, t=552.2114868164062, r=480.59470078842554, b=622.6449584960938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9872121214866638, cells=[Cell(id=87, text='For example, if the input to the network is an image of size', bbox=BoundingBox(l=134.76498022108473, t=552.8926330527584, r=392.8262006444395, b=562.8253431076012, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=88, text='64', bbox=BoundingBox(l=395.26599064844197, t=553.1018630539137, r=405.22858066478585, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=89, text='×', bbox=BoundingBox(l=407.24899066810036, t=552.5439430508331, r=414.9978906808126, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=90, text='64', bbox=BoundingBox(l=417.0189806841282, t=553.1018630539137, r=426.98157070047205, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=91, text='×', bbox=BoundingBox(l=429.00299070378827, t=552.5439430508331, r=436.7518907165005, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=92, text='3', bbox=BoundingBox(l=438.77298071981613, t=553.1018630539137, r=443.75427072798806, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=93, text='(a RGB-', bbox=BoundingBox(l=446.1929907319888, t=552.8926330527584, r=480.5938407884241, b=562.8253431076012, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=94, text='coloured image with a dimensionality of', bbox=BoundingBox(l=134.76498022108473, t=564.847623118767, r=312.8862905132964, b=574.7803331736098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=95, text='64', bbox=BoundingBox(l=315.2929705172446, t=565.0568531199224, r=325.25555053358846, b=573.9036231687692, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=96, text='×', bbox=BoundingBox(l=327.1609805367143, t=564.4989331168418, r=334.90988054942653, b=573.9036231687692, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=97, text='64', bbox=BoundingBox(l=336.8149705525519, t=565.0568531199224, r=346.7775605688957, b=573.9036231687692, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=98, text=') and we set the receptive field', bbox=BoundingBox(l=346.77698056889477, t=564.847623118767, r=480.59470078842554, b=574.7803331736098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=99, text='size as', bbox=BoundingBox(l=134.76498022108473, t=576.8026131847757, r=163.6963802685473, b=586.7353232396185, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=100, text='6', bbox=BoundingBox(l=166.5449802732205, t=577.011843185931, r=171.52628028139242, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=101, text='×', bbox=BoundingBox(l=174.00598028546037, t=576.4539131828503, r=181.75490029817266, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=102, text='6', bbox=BoundingBox(l=184.2349903022413, t=577.011843185931, r=189.21628031041323, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=103, text=', we would have a total of', bbox=BoundingBox(l=189.21599031041274, t=576.8026131847757, r=305.3599505009493, b=586.7353232396185, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=104, text='108', bbox=BoundingBox(l=308.2089805056232, t=577.011843185931, r=323.15289053013896, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=105, text='weights on each neuron within the', bbox=BoundingBox(l=326.00299053481467, t=576.8026131847757, r=480.5926807884222, b=586.7353232396185, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=106, text='convolutional layer. (', bbox=BoundingBox(l=134.76498022108473, t=588.7576132507844, r=228.17432037432468, b=598.6903233056272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=107, text='6', bbox=BoundingBox(l=228.1749903743258, t=588.9668232519396, r=233.1562803824977, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=108, text='×', bbox=BoundingBox(l=235.49298038633108, t=588.4089232488592, r=243.24190039904332, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=109, text='6', bbox=BoundingBox(l=245.57799040287574, t=588.9668232519396, r=250.55928041104767, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=110, text='×', bbox=BoundingBox(l=252.8959804148811, t=588.4089232488592, r=260.6449004275933, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=111, text='3', bbox=BoundingBox(l=262.98099043142577, t=588.9668232519396, r=267.9622804395977, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=112, text='where', bbox=BoundingBox(l=270.618990443956, t=588.7576132507844, r=298.02612048891797, b=598.6903233056272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=113, text='3', bbox=BoundingBox(l=300.68198049327503, t=588.9668232519396, r=305.6632705014469, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=114, text='is the magnitude of connectivity across', bbox=BoundingBox(l=308.3189705058036, t=588.7576132507844, r=480.5922207884214, b=598.6903233056272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=115, text='the depth of the volume) To put this into perspective, a standard neuron seen', bbox=BoundingBox(l=134.7649702210847, t=600.7126133167931, r=480.5867307884124, b=610.6453233716359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=116, text='in other forms of ANN would contain', bbox=BoundingBox(l=134.7649702210847, t=612.6686033828073, r=301.70825049495863, b=622.6013134376501, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=117, text='12', bbox=BoundingBox(l=304.1969604990414, t=612.8778233839625, r=314.1595505153852, b=621.7246034328093, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=118, text=',', bbox=BoundingBox(l=314.1599705153859, t=612.8778233839625, r=316.9275805199262, b=621.7246034328093, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=119, text='288', bbox=BoundingBox(l=318.5879805226502, t=612.8778233839625, r=333.5318905471659, b=621.7246034328093, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=120, text='weights each.', bbox=BoundingBox(l=336.0229805512526, t=612.6686033828073, r=395.7687106492667, b=622.6013134376501, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='For example, if the input to the network is an image of size 64 × 64 × 3 (a RGBcoloured image with a dimensionality of 64 × 64 ) and we set the receptive field size as 6 × 6 , we would have a total of 108 weights on each neuron within the convolutional layer. ( 6 × 6 × 3 where 3 is the magnitude of connectivity across the depth of the volume) To put this into perspective, a standard neuron seen in other forms of ANN would contain 12 , 288 weights each.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=9, page_no=5, cluster=Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.94898986816406, t=632.0844116210938, r=480.5867307884124, b=666.8765258789062, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9835361242294312, cells=[Cell(id=121, text='Convolutional layers are also able to significantly reduce the complexity of the', bbox=BoundingBox(l=134.76498022108473, t=632.885603494434, r=480.5867307884124, b=642.8183135492768, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=122, text='model through the optimisation of its output. These are optimised through', bbox=BoundingBox(l=134.76498022108473, t=644.8406035604428, r=480.5867307884124, b=654.7733136152856, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=123, text='three hyperparameters, the', bbox=BoundingBox(l=134.76498022108473, t=656.7956036264516, r=254.06711041680234, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=124, text='depth', bbox=BoundingBox(l=256.5559704208854, t=656.8653436268365, r=283.11627046445807, b=666.5689236804142, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=125, text=', the', bbox=BoundingBox(l=283.1169704644592, t=656.7956036264516, r=301.9164104953001, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=126, text='stride', bbox=BoundingBox(l=304.4069804993859, t=656.8653436268365, r=330.4093605420433, b=666.5689236804142, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=127, text='and setting', bbox=BoundingBox(l=332.8999905461293, t=656.7956036264516, r=381.98572062665545, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=128, text='zero-padding', bbox=BoundingBox(l=384.47598063074076, t=656.8653436268365, r=445.35742073061806, b=666.5689236804142, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=129, text='.', bbox=BoundingBox(l=445.3569907306173, t=656.7956036264516, r=447.84766073470337, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Convolutional layers are also able to significantly reduce the complexity of the model through the optimisation of its output. These are optimised through three hyperparameters, the depth , the stride and setting zero-padding .')], headers=[TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=5, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=133.78485107421875, t=94.09490966796875, r=139.71607971191406, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7533239722251892, cells=[Cell(id=0, text='6', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='6'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=5, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=167.21620178222656, t=93.54159545898438, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7271904349327087, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea et al.')])), Page(page_no=6, size=Size(width=595.2760009765625, height=841.8900146484375), cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=1, text='7', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=2, text='The', bbox=BoundingBox(l=134.76501022108476, t=119.67181066075966, r=151.4424102484444, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='depth', bbox=BoundingBox(l=154.47302025341617, t=119.74151066114439, r=181.03331029698887, b=129.445060714722, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='of the output volume produced by the convolutional layers can be', bbox=BoundingBox(l=184.06403030196083, t=119.67181066075966, r=480.59082078841914, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='manually set through the number of neurons within the layer to a the same', bbox=BoundingBox(l=134.7650302210848, t=131.6268307267684, r=480.58676078841245, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='region of the input. This can be seen with other forms of ANNs, where the', bbox=BoundingBox(l=134.7650302210848, t=143.58184079277714, r=480.5867307884124, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='all of the neurons in the hidden layer are directly connected to every single', bbox=BoundingBox(l=134.7650302210848, t=155.53686085878599, r=480.58682078841264, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='neuron beforehand. Reducing this hyperparameter can significantly minimise', bbox=BoundingBox(l=134.7650302210848, t=167.49188092479483, r=480.5868507884127, b=177.42456097963736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='the total number of neurons of the network, but it can also significantly reduce', bbox=BoundingBox(l=134.7650302210848, t=179.44787099080895, r=480.58676078841245, b=189.3805510456516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='the pattern recognition capabilities of the model.', bbox=BoundingBox(l=134.7650302210848, t=191.4028910568178, r=348.2934605713826, b=201.33557111166033, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='We are also able to define the', bbox=BoundingBox(l=134.7650302210848, t=212.40490117277898, r=259.2974904253829, b=222.33758122762163, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='stride', bbox=BoundingBox(l=261.2180204285336, t=212.47460117316382, r=287.220400471191, b=222.17816122674128, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='in which we set the depth around the spatial', bbox=BoundingBox(l=289.1430104743451, t=212.40490117277898, r=480.5943607884249, b=222.33758122762163, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='dimensionality of the input in order to place the receptive field. For example if', bbox=BoundingBox(l=134.76500022108476, t=224.3609012387932, r=480.58682078841264, b=234.29357129363575, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='we were to set a stride as 1, then we would have a heavily overlapped receptive', bbox=BoundingBox(l=134.76500022108476, t=236.31591130480183, r=480.58676078841245, b=246.24859135964448, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='field producing extremely large activations. Alternatively, setting the stride to a', bbox=BoundingBox(l=134.76500022108476, t=248.27093137081079, r=480.5867307884124, b=258.2036114256533, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='greater number will reduce the amount of overlapping and produce an output', bbox=BoundingBox(l=134.76500022108476, t=260.2259514368196, r=480.5867307884124, b=270.1586314916623, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='of lower spatial dimensions.', bbox=BoundingBox(l=134.76500022108476, t=272.18096150282827, r=258.9986004248926, b=282.1136415576709, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='Zero-padding', bbox=BoundingBox(l=134.76500022108476, t=293.25366161917987, r=197.31020032369145, b=302.95721167275724, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='is the simple process of padding the border of the input, and', bbox=BoundingBox(l=201.02701032978894, t=293.1839616187949, r=480.5876207884139, b=303.1166316736376, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='is an effective method to give further control as to the dimensionality of the', bbox=BoundingBox(l=134.76501022108476, t=305.13897168480366, r=480.58676078841245, b=315.0716517396463, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='output volumes.', bbox=BoundingBox(l=134.76501022108476, t=317.0939917508125, r=207.4521303403295, b=327.02667180565516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='It is important to understand that through using these techniques, we will alter', bbox=BoundingBox(l=134.76501022108476, t=338.0969518667789, r=480.58682078841264, b=348.0296619216217, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='the spatial dimensionality of the convolutional layers output. To calculate this,', bbox=BoundingBox(l=134.76501022108476, t=350.0519419327876, r=480.5867307884124, b=359.98465198763034, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='you can make use of the following formula:', bbox=BoundingBox(l=134.76501022108476, t=362.0069219987962, r=326.46536053557315, b=371.93963205363895, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='(', bbox=BoundingBox(l=277.54602045532, t=396.29214218809966, r=281.42047046167613, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='V', bbox=BoundingBox(l=281.42102046167696, t=396.29214218809966, r=287.23221047121035, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='−', bbox=BoundingBox(l=291.66003047847425, t=395.73422218501906, r=299.4089404911865, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='R', bbox=BoundingBox(l=301.6230504948188, t=396.29214218809966, r=309.18765050722874, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text=') + 2', bbox=BoundingBox(l=309.2640405073541, t=396.29214218809966, r=330.29208054185096, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='Z', bbox=BoundingBox(l=330.2960505418574, t=396.29214218809966, r=337.09653055301374, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='S', bbox=BoundingBox(l=295.7569904851954, t=409.8659922630467, r=301.8660604952175, b=418.7127623118935, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='+ 1', bbox=BoundingBox(l=304.6550004997928, t=409.8659922630467, r=319.59689052430525, b=418.7127623118935, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='Where', bbox=BoundingBox(l=134.76500022108476, t=438.3707524204338, r=163.82590026875977, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='V', bbox=BoundingBox(l=165.8860002721394, t=438.579982421589, r=171.6971902816728, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='represents the input volume size (', bbox=BoundingBox(l=175.97101028868408, t=438.3707524204338, r=323.0787705300174, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='height', bbox=BoundingBox(l=323.0780005300161, t=438.579982421589, r=349.92023057405135, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='×', bbox=BoundingBox(l=350.5410205750698, t=438.02206241850854, r=358.289920587782, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='width', bbox=BoundingBox(l=358.9100005887993, t=438.579982421589, r=383.8175006296605, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='×', bbox=BoundingBox(l=384.43600063067515, t=438.02206241850854, r=392.18491064338747, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='depth', bbox=BoundingBox(l=392.80499064440465, t=438.579982421589, r=417.71249068526595, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='),', bbox=BoundingBox(l=417.71201068526517, t=438.3707524204338, r=423.52020069479363, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='R', bbox=BoundingBox(l=425.5800206981728, t=438.579982421589, r=433.1446207105826, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='represents', bbox=BoundingBox(l=435.28201071408915, t=438.3707524204338, r=480.59192078842096, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='the receptive field size,', bbox=BoundingBox(l=134.76501022108476, t=450.3257424864424, r=236.31381038767768, b=460.25845254128524, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='Z', bbox=BoundingBox(l=239.082020392219, t=450.5349724875977, r=245.8824904033753, b=459.3817425364445, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='is the amount of zero padding set and', bbox=BoundingBox(l=249.36801040909333, t=450.3257424864424, r=418.18417068603975, b=460.25845254128524, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='S', bbox=BoundingBox(l=420.95203069058044, t=450.5349724875977, r=427.06110070060254, b=459.3817425364445, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='referring to', bbox=BoundingBox(l=430.4080207060932, t=450.3257424864424, r=480.58963078841714, b=460.25845254128524, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='the stride. If the calculated result from this equation is not equal to a whole', bbox=BoundingBox(l=134.76501022108476, t=462.28073255245107, r=480.58676078841245, b=472.2134426072938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='integer then the stride has been incorrectly set, as the neurons will be unable to', bbox=BoundingBox(l=134.76501022108476, t=474.2357126184597, r=480.58679078841254, b=484.1684226733024, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='fit neatly across the given input.', bbox=BoundingBox(l=134.76501022108476, t=486.19070268446836, r=275.6660504522358, b=496.1234127393111, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='Despite our best efforts so far we will still find that our models are still enor-', bbox=BoundingBox(l=134.76501022108476, t=507.19369280043486, r=480.58691078841275, b=517.1264028552775, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='mous if we use an image input of any', bbox=BoundingBox(l=134.76501022108476, t=519.1486828664436, r=304.7069704998781, b=529.0813929212864, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='real', bbox=BoundingBox(l=307.7520105048735, t=519.0789428660585, r=322.5166005290951, b=528.8721929201313, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='dimensionality. However, methods', bbox=BoundingBox(l=325.5650005340961, t=519.1486828664436, r=480.5930507884228, b=529.0813929212864, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='have been developed as to greatly curtail the overall number of parameters', bbox=BoundingBox(l=134.76500022108476, t=531.1036629324522, r=480.5866107884122, b=541.036372987295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='within the convolutional layer.', bbox=BoundingBox(l=134.76500022108476, t=543.0596629984664, r=269.78809044259293, b=552.9923730533092, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='Parameter sharing', bbox=BoundingBox(l=134.76500022108476, t=564.1314031148125, r=217.39481035664068, b=573.8349931683902, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='works on the assumption that if one region feature is useful', bbox=BoundingBox(l=219.77301036054214, t=564.0616731144275, r=480.5939607884243, b=573.9943831692703, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='to compute at a set spatial region, then it is likely to be useful in another region.', bbox=BoundingBox(l=134.76501022108476, t=576.0166631804361, r=480.58679078841254, b=585.9493732352789, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='If we constrain each individual activation map within the output volume to the', bbox=BoundingBox(l=134.76501022108476, t=587.9726532464504, r=480.5867307884124, b=597.9053633012932, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='same weights and bias, then we will see a massive reduction in the number of', bbox=BoundingBox(l=134.76501022108476, t=599.927653312459, r=480.58670078841243, b=609.8603633673018, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='parameters being produced by the convolutional layer.', bbox=BoundingBox(l=134.76501022108476, t=611.8826633784678, r=376.0691506169492, b=621.8153634333105, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=65, text='As a result of this as the backpropagation stage occurs, each neuron in the out-', bbox=BoundingBox(l=134.76501022108476, t=632.8856634944344, r=480.58676078841245, b=642.8183735492772, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='put will represent the overall gradient of which can be totalled across the depth', bbox=BoundingBox(l=134.76501022108476, t=644.8406635604431, r=480.5868507884127, b=654.7733736152859, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='- thus only updating a single set of weights, as opposed to every single one.', bbox=BoundingBox(l=134.76501022108476, t=656.7956636264519, r=466.95804076605424, b=666.7283736812946, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))], predictions=PagePredictions(layout=LayoutPrediction(clusters=[Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=255.95968627929688, t=93.53328704833984, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9278361201286316, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=475.1293029785156, t=93.63327026367188, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8769420981407166, cells=[Cell(id=1, text='7', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.85671997070312, t=118.58126831054688, r=480.73211669921875, b=201.79269409179688, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9878745675086975, cells=[Cell(id=2, text='The', bbox=BoundingBox(l=134.76501022108476, t=119.67181066075966, r=151.4424102484444, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='depth', bbox=BoundingBox(l=154.47302025341617, t=119.74151066114439, r=181.03331029698887, b=129.445060714722, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='of the output volume produced by the convolutional layers can be', bbox=BoundingBox(l=184.06403030196083, t=119.67181066075966, r=480.59082078841914, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='manually set through the number of neurons within the layer to a the same', bbox=BoundingBox(l=134.7650302210848, t=131.6268307267684, r=480.58676078841245, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='region of the input. This can be seen with other forms of ANNs, where the', bbox=BoundingBox(l=134.7650302210848, t=143.58184079277714, r=480.5867307884124, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='all of the neurons in the hidden layer are directly connected to every single', bbox=BoundingBox(l=134.7650302210848, t=155.53686085878599, r=480.58682078841264, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='neuron beforehand. Reducing this hyperparameter can significantly minimise', bbox=BoundingBox(l=134.7650302210848, t=167.49188092479483, r=480.5868507884127, b=177.42456097963736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='the total number of neurons of the network, but it can also significantly reduce', bbox=BoundingBox(l=134.7650302210848, t=179.44787099080895, r=480.58676078841245, b=189.3805510456516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='the pattern recognition capabilities of the model.', bbox=BoundingBox(l=134.7650302210848, t=191.4028910568178, r=348.2934605713826, b=201.33557111166033, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.76483154296875, t=211.41566467285156, r=480.6409912109375, b=282.1136415576709, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9880473613739014, cells=[Cell(id=11, text='We are also able to define the', bbox=BoundingBox(l=134.7650302210848, t=212.40490117277898, r=259.2974904253829, b=222.33758122762163, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='stride', bbox=BoundingBox(l=261.2180204285336, t=212.47460117316382, r=287.220400471191, b=222.17816122674128, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='in which we set the depth around the spatial', bbox=BoundingBox(l=289.1430104743451, t=212.40490117277898, r=480.5943607884249, b=222.33758122762163, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='dimensionality of the input in order to place the receptive field. For example if', bbox=BoundingBox(l=134.76500022108476, t=224.3609012387932, r=480.58682078841264, b=234.29357129363575, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='we were to set a stride as 1, then we would have a heavily overlapped receptive', bbox=BoundingBox(l=134.76500022108476, t=236.31591130480183, r=480.58676078841245, b=246.24859135964448, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='field producing extremely large activations. Alternatively, setting the stride to a', bbox=BoundingBox(l=134.76500022108476, t=248.27093137081079, r=480.5867307884124, b=258.2036114256533, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='greater number will reduce the amount of overlapping and produce an output', bbox=BoundingBox(l=134.76500022108476, t=260.2259514368196, r=480.5867307884124, b=270.1586314916623, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='of lower spatial dimensions.', bbox=BoundingBox(l=134.76500022108476, t=272.18096150282827, r=258.9986004248926, b=282.1136415576709, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.788818359375, t=292.6251220703125, r=480.5876207884139, b=327.02667180565516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9830900430679321, cells=[Cell(id=19, text='Zero-padding', bbox=BoundingBox(l=134.76500022108476, t=293.25366161917987, r=197.31020032369145, b=302.95721167275724, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='is the simple process of padding the border of the input, and', bbox=BoundingBox(l=201.02701032978894, t=293.1839616187949, r=480.5876207884139, b=303.1166316736376, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='is an effective method to give further control as to the dimensionality of the', bbox=BoundingBox(l=134.76501022108476, t=305.13897168480366, r=480.58676078841245, b=315.0716517396463, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='output volumes.', bbox=BoundingBox(l=134.76501022108476, t=317.0939917508125, r=207.4521303403295, b=327.02667180565516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.85841369628906, t=337.35748291015625, r=480.78466796875, b=372.1994323730469, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9820359349250793, cells=[Cell(id=23, text='It is important to understand that through using these techniques, we will alter', bbox=BoundingBox(l=134.76501022108476, t=338.0969518667789, r=480.58682078841264, b=348.0296619216217, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='the spatial dimensionality of the convolutional layers output. To calculate this,', bbox=BoundingBox(l=134.76501022108476, t=350.0519419327876, r=480.5867307884124, b=359.98465198763034, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='you can make use of the following formula:', bbox=BoundingBox(l=134.76501022108476, t=362.0069219987962, r=326.46536053557315, b=371.93963205363895, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=6, label=<DocItemLabel.FORMULA: 'formula'>, bbox=BoundingBox(l=276.7643127441406, t=395.47357177734375, r=338.3543395996094, b=418.7127623118935, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9492904543876648, cells=[Cell(id=26, text='(', bbox=BoundingBox(l=277.54602045532, t=396.29214218809966, r=281.42047046167613, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='V', bbox=BoundingBox(l=281.42102046167696, t=396.29214218809966, r=287.23221047121035, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='−', bbox=BoundingBox(l=291.66003047847425, t=395.73422218501906, r=299.4089404911865, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='R', bbox=BoundingBox(l=301.6230504948188, t=396.29214218809966, r=309.18765050722874, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text=') + 2', bbox=BoundingBox(l=309.2640405073541, t=396.29214218809966, r=330.29208054185096, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='Z', bbox=BoundingBox(l=330.2960505418574, t=396.29214218809966, r=337.09653055301374, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='S', bbox=BoundingBox(l=295.7569904851954, t=409.8659922630467, r=301.8660604952175, b=418.7127623118935, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='+ 1', bbox=BoundingBox(l=304.6550004997928, t=409.8659922630467, r=319.59689052430525, b=418.7127623118935, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=134.1217041015625, t=437.6549987792969, r=480.6238708496094, b=496.4444885253906, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.986332356929779, cells=[Cell(id=34, text='Where', bbox=BoundingBox(l=134.76500022108476, t=438.3707524204338, r=163.82590026875977, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='V', bbox=BoundingBox(l=165.8860002721394, t=438.579982421589, r=171.6971902816728, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='represents the input volume size (', bbox=BoundingBox(l=175.97101028868408, t=438.3707524204338, r=323.0787705300174, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='height', bbox=BoundingBox(l=323.0780005300161, t=438.579982421589, r=349.92023057405135, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='×', bbox=BoundingBox(l=350.5410205750698, t=438.02206241850854, r=358.289920587782, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='width', bbox=BoundingBox(l=358.9100005887993, t=438.579982421589, r=383.8175006296605, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='×', bbox=BoundingBox(l=384.43600063067515, t=438.02206241850854, r=392.18491064338747, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='depth', bbox=BoundingBox(l=392.80499064440465, t=438.579982421589, r=417.71249068526595, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='),', bbox=BoundingBox(l=417.71201068526517, t=438.3707524204338, r=423.52020069479363, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='R', bbox=BoundingBox(l=425.5800206981728, t=438.579982421589, r=433.1446207105826, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='represents', bbox=BoundingBox(l=435.28201071408915, t=438.3707524204338, r=480.59192078842096, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='the receptive field size,', bbox=BoundingBox(l=134.76501022108476, t=450.3257424864424, r=236.31381038767768, b=460.25845254128524, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='Z', bbox=BoundingBox(l=239.082020392219, t=450.5349724875977, r=245.8824904033753, b=459.3817425364445, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='is the amount of zero padding set and', bbox=BoundingBox(l=249.36801040909333, t=450.3257424864424, r=418.18417068603975, b=460.25845254128524, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='S', bbox=BoundingBox(l=420.95203069058044, t=450.5349724875977, r=427.06110070060254, b=459.3817425364445, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='referring to', bbox=BoundingBox(l=430.4080207060932, t=450.3257424864424, r=480.58963078841714, b=460.25845254128524, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='the stride. If the calculated result from this equation is not equal to a whole', bbox=BoundingBox(l=134.76501022108476, t=462.28073255245107, r=480.58676078841245, b=472.2134426072938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='integer then the stride has been incorrectly set, as the neurons will be unable to', bbox=BoundingBox(l=134.76501022108476, t=474.2357126184597, r=480.58679078841254, b=484.1684226733024, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='fit neatly across the given input.', bbox=BoundingBox(l=134.76501022108476, t=486.19070268446836, r=275.6660504522358, b=496.1234127393111, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8834686279297, t=506.8936462402344, r=480.5930507884228, b=552.9923730533092, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9862993359565735, cells=[Cell(id=53, text='Despite our best efforts so far we will still find that our models are still enor-', bbox=BoundingBox(l=134.76501022108476, t=507.19369280043486, r=480.58691078841275, b=517.1264028552775, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='mous if we use an image input of any', bbox=BoundingBox(l=134.76501022108476, t=519.1486828664436, r=304.7069704998781, b=529.0813929212864, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='real', bbox=BoundingBox(l=307.7520105048735, t=519.0789428660585, r=322.5166005290951, b=528.8721929201313, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='dimensionality. However, methods', bbox=BoundingBox(l=325.5650005340961, t=519.1486828664436, r=480.5930507884228, b=529.0813929212864, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='have been developed as to greatly curtail the overall number of parameters', bbox=BoundingBox(l=134.76500022108476, t=531.1036629324522, r=480.5866107884122, b=541.036372987295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='within the convolutional layer.', bbox=BoundingBox(l=134.76500022108476, t=543.0596629984664, r=269.78809044259293, b=552.9923730533092, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8805694580078, t=563.507568359375, r=480.5939607884243, b=622.790283203125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9858874678611755, cells=[Cell(id=59, text='Parameter sharing', bbox=BoundingBox(l=134.76500022108476, t=564.1314031148125, r=217.39481035664068, b=573.8349931683902, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='works on the assumption that if one region feature is useful', bbox=BoundingBox(l=219.77301036054214, t=564.0616731144275, r=480.5939607884243, b=573.9943831692703, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='to compute at a set spatial region, then it is likely to be useful in another region.', bbox=BoundingBox(l=134.76501022108476, t=576.0166631804361, r=480.58679078841254, b=585.9493732352789, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='If we constrain each individual activation map within the output volume to the', bbox=BoundingBox(l=134.76501022108476, t=587.9726532464504, r=480.5867307884124, b=597.9053633012932, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='same weights and bias, then we will see a massive reduction in the number of', bbox=BoundingBox(l=134.76501022108476, t=599.927653312459, r=480.58670078841243, b=609.8603633673018, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='parameters being produced by the convolutional layer.', bbox=BoundingBox(l=134.76501022108476, t=611.8826633784678, r=376.0691506169492, b=621.8153634333105, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=10, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.85092163085938, t=632.0819702148438, r=480.5868507884127, b=666.8784790039062, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9838565587997437, cells=[Cell(id=65, text='As a result of this as the backpropagation stage occurs, each neuron in the out-', bbox=BoundingBox(l=134.76501022108476, t=632.8856634944344, r=480.58676078841245, b=642.8183735492772, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='put will represent the overall gradient of which can be totalled across the depth', bbox=BoundingBox(l=134.76501022108476, t=644.8406635604431, r=480.5868507884127, b=654.7733736152859, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='- thus only updating a single set of weights, as opposed to every single one.', bbox=BoundingBox(l=134.76501022108476, t=656.7956636264519, r=466.95804076605424, b=666.7283736812946, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))])]), tablestructure=TableStructurePrediction(table_map={}), figures_classification=None, equations_prediction=None), assembled=AssembledUnit(elements=[TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=6, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=255.95968627929688, t=93.53328704833984, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9278361201286316, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=6, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=475.1293029785156, t=93.63327026367188, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8769420981407166, cells=[Cell(id=1, text='7', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='7'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=6, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.85671997070312, t=118.58126831054688, r=480.73211669921875, b=201.79269409179688, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9878745675086975, cells=[Cell(id=2, text='The', bbox=BoundingBox(l=134.76501022108476, t=119.67181066075966, r=151.4424102484444, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='depth', bbox=BoundingBox(l=154.47302025341617, t=119.74151066114439, r=181.03331029698887, b=129.445060714722, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='of the output volume produced by the convolutional layers can be', bbox=BoundingBox(l=184.06403030196083, t=119.67181066075966, r=480.59082078841914, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='manually set through the number of neurons within the layer to a the same', bbox=BoundingBox(l=134.7650302210848, t=131.6268307267684, r=480.58676078841245, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='region of the input. This can be seen with other forms of ANNs, where the', bbox=BoundingBox(l=134.7650302210848, t=143.58184079277714, r=480.5867307884124, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='all of the neurons in the hidden layer are directly connected to every single', bbox=BoundingBox(l=134.7650302210848, t=155.53686085878599, r=480.58682078841264, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='neuron beforehand. Reducing this hyperparameter can significantly minimise', bbox=BoundingBox(l=134.7650302210848, t=167.49188092479483, r=480.5868507884127, b=177.42456097963736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='the total number of neurons of the network, but it can also significantly reduce', bbox=BoundingBox(l=134.7650302210848, t=179.44787099080895, r=480.58676078841245, b=189.3805510456516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='the pattern recognition capabilities of the model.', bbox=BoundingBox(l=134.7650302210848, t=191.4028910568178, r=348.2934605713826, b=201.33557111166033, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The depth of the output volume produced by the convolutional layers can be manually set through the number of neurons within the layer to a the same region of the input. This can be seen with other forms of ANNs, where the all of the neurons in the hidden layer are directly connected to every single neuron beforehand. Reducing this hyperparameter can significantly minimise the total number of neurons of the network, but it can also significantly reduce the pattern recognition capabilities of the model.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=6, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.76483154296875, t=211.41566467285156, r=480.6409912109375, b=282.1136415576709, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9880473613739014, cells=[Cell(id=11, text='We are also able to define the', bbox=BoundingBox(l=134.7650302210848, t=212.40490117277898, r=259.2974904253829, b=222.33758122762163, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='stride', bbox=BoundingBox(l=261.2180204285336, t=212.47460117316382, r=287.220400471191, b=222.17816122674128, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='in which we set the depth around the spatial', bbox=BoundingBox(l=289.1430104743451, t=212.40490117277898, r=480.5943607884249, b=222.33758122762163, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='dimensionality of the input in order to place the receptive field. For example if', bbox=BoundingBox(l=134.76500022108476, t=224.3609012387932, r=480.58682078841264, b=234.29357129363575, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='we were to set a stride as 1, then we would have a heavily overlapped receptive', bbox=BoundingBox(l=134.76500022108476, t=236.31591130480183, r=480.58676078841245, b=246.24859135964448, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='field producing extremely large activations. Alternatively, setting the stride to a', bbox=BoundingBox(l=134.76500022108476, t=248.27093137081079, r=480.5867307884124, b=258.2036114256533, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='greater number will reduce the amount of overlapping and produce an output', bbox=BoundingBox(l=134.76500022108476, t=260.2259514368196, r=480.5867307884124, b=270.1586314916623, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='of lower spatial dimensions.', bbox=BoundingBox(l=134.76500022108476, t=272.18096150282827, r=258.9986004248926, b=282.1136415576709, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='We are also able to define the stride in which we set the depth around the spatial dimensionality of the input in order to place the receptive field. For example if we were to set a stride as 1, then we would have a heavily overlapped receptive field producing extremely large activations. Alternatively, setting the stride to a greater number will reduce the amount of overlapping and produce an output of lower spatial dimensions.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=4, page_no=6, cluster=Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.788818359375, t=292.6251220703125, r=480.5876207884139, b=327.02667180565516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9830900430679321, cells=[Cell(id=19, text='Zero-padding', bbox=BoundingBox(l=134.76500022108476, t=293.25366161917987, r=197.31020032369145, b=302.95721167275724, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='is the simple process of padding the border of the input, and', bbox=BoundingBox(l=201.02701032978894, t=293.1839616187949, r=480.5876207884139, b=303.1166316736376, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='is an effective method to give further control as to the dimensionality of the', bbox=BoundingBox(l=134.76501022108476, t=305.13897168480366, r=480.58676078841245, b=315.0716517396463, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='output volumes.', bbox=BoundingBox(l=134.76501022108476, t=317.0939917508125, r=207.4521303403295, b=327.02667180565516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Zero-padding is the simple process of padding the border of the input, and is an effective method to give further control as to the dimensionality of the output volumes.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=6, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.85841369628906, t=337.35748291015625, r=480.78466796875, b=372.1994323730469, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9820359349250793, cells=[Cell(id=23, text='It is important to understand that through using these techniques, we will alter', bbox=BoundingBox(l=134.76501022108476, t=338.0969518667789, r=480.58682078841264, b=348.0296619216217, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='the spatial dimensionality of the convolutional layers output. To calculate this,', bbox=BoundingBox(l=134.76501022108476, t=350.0519419327876, r=480.5867307884124, b=359.98465198763034, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='you can make use of the following formula:', bbox=BoundingBox(l=134.76501022108476, t=362.0069219987962, r=326.46536053557315, b=371.93963205363895, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='It is important to understand that through using these techniques, we will alter the spatial dimensionality of the convolutional layers output. To calculate this, you can make use of the following formula:'), TextElement(label=<DocItemLabel.FORMULA: 'formula'>, id=6, page_no=6, cluster=Cluster(id=6, label=<DocItemLabel.FORMULA: 'formula'>, bbox=BoundingBox(l=276.7643127441406, t=395.47357177734375, r=338.3543395996094, b=418.7127623118935, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9492904543876648, cells=[Cell(id=26, text='(', bbox=BoundingBox(l=277.54602045532, t=396.29214218809966, r=281.42047046167613, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='V', bbox=BoundingBox(l=281.42102046167696, t=396.29214218809966, r=287.23221047121035, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='−', bbox=BoundingBox(l=291.66003047847425, t=395.73422218501906, r=299.4089404911865, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='R', bbox=BoundingBox(l=301.6230504948188, t=396.29214218809966, r=309.18765050722874, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text=') + 2', bbox=BoundingBox(l=309.2640405073541, t=396.29214218809966, r=330.29208054185096, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='Z', bbox=BoundingBox(l=330.2960505418574, t=396.29214218809966, r=337.09653055301374, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='S', bbox=BoundingBox(l=295.7569904851954, t=409.8659922630467, r=301.8660604952175, b=418.7127623118935, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='+ 1', bbox=BoundingBox(l=304.6550004997928, t=409.8659922630467, r=319.59689052430525, b=418.7127623118935, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='( V − R ) + 2 Z S + 1'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=6, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=134.1217041015625, t=437.6549987792969, r=480.6238708496094, b=496.4444885253906, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.986332356929779, cells=[Cell(id=34, text='Where', bbox=BoundingBox(l=134.76500022108476, t=438.3707524204338, r=163.82590026875977, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='V', bbox=BoundingBox(l=165.8860002721394, t=438.579982421589, r=171.6971902816728, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='represents the input volume size (', bbox=BoundingBox(l=175.97101028868408, t=438.3707524204338, r=323.0787705300174, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='height', bbox=BoundingBox(l=323.0780005300161, t=438.579982421589, r=349.92023057405135, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='×', bbox=BoundingBox(l=350.5410205750698, t=438.02206241850854, r=358.289920587782, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='width', bbox=BoundingBox(l=358.9100005887993, t=438.579982421589, r=383.8175006296605, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='×', bbox=BoundingBox(l=384.43600063067515, t=438.02206241850854, r=392.18491064338747, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='depth', bbox=BoundingBox(l=392.80499064440465, t=438.579982421589, r=417.71249068526595, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='),', bbox=BoundingBox(l=417.71201068526517, t=438.3707524204338, r=423.52020069479363, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='R', bbox=BoundingBox(l=425.5800206981728, t=438.579982421589, r=433.1446207105826, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='represents', bbox=BoundingBox(l=435.28201071408915, t=438.3707524204338, r=480.59192078842096, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='the receptive field size,', bbox=BoundingBox(l=134.76501022108476, t=450.3257424864424, r=236.31381038767768, b=460.25845254128524, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='Z', bbox=BoundingBox(l=239.082020392219, t=450.5349724875977, r=245.8824904033753, b=459.3817425364445, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='is the amount of zero padding set and', bbox=BoundingBox(l=249.36801040909333, t=450.3257424864424, r=418.18417068603975, b=460.25845254128524, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='S', bbox=BoundingBox(l=420.95203069058044, t=450.5349724875977, r=427.06110070060254, b=459.3817425364445, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='referring to', bbox=BoundingBox(l=430.4080207060932, t=450.3257424864424, r=480.58963078841714, b=460.25845254128524, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='the stride. If the calculated result from this equation is not equal to a whole', bbox=BoundingBox(l=134.76501022108476, t=462.28073255245107, r=480.58676078841245, b=472.2134426072938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='integer then the stride has been incorrectly set, as the neurons will be unable to', bbox=BoundingBox(l=134.76501022108476, t=474.2357126184597, r=480.58679078841254, b=484.1684226733024, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='fit neatly across the given input.', bbox=BoundingBox(l=134.76501022108476, t=486.19070268446836, r=275.6660504522358, b=496.1234127393111, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Where V represents the input volume size ( height × width × depth ), R represents the receptive field size, Z is the amount of zero padding set and S referring to the stride. If the calculated result from this equation is not equal to a whole integer then the stride has been incorrectly set, as the neurons will be unable to fit neatly across the given input.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=6, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8834686279297, t=506.8936462402344, r=480.5930507884228, b=552.9923730533092, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9862993359565735, cells=[Cell(id=53, text='Despite our best efforts so far we will still find that our models are still enor-', bbox=BoundingBox(l=134.76501022108476, t=507.19369280043486, r=480.58691078841275, b=517.1264028552775, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='mous if we use an image input of any', bbox=BoundingBox(l=134.76501022108476, t=519.1486828664436, r=304.7069704998781, b=529.0813929212864, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='real', bbox=BoundingBox(l=307.7520105048735, t=519.0789428660585, r=322.5166005290951, b=528.8721929201313, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='dimensionality. However, methods', bbox=BoundingBox(l=325.5650005340961, t=519.1486828664436, r=480.5930507884228, b=529.0813929212864, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='have been developed as to greatly curtail the overall number of parameters', bbox=BoundingBox(l=134.76500022108476, t=531.1036629324522, r=480.5866107884122, b=541.036372987295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='within the convolutional layer.', bbox=BoundingBox(l=134.76500022108476, t=543.0596629984664, r=269.78809044259293, b=552.9923730533092, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Despite our best efforts so far we will still find that our models are still enormous if we use an image input of any real dimensionality. However, methods have been developed as to greatly curtail the overall number of parameters within the convolutional layer.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=9, page_no=6, cluster=Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8805694580078, t=563.507568359375, r=480.5939607884243, b=622.790283203125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9858874678611755, cells=[Cell(id=59, text='Parameter sharing', bbox=BoundingBox(l=134.76500022108476, t=564.1314031148125, r=217.39481035664068, b=573.8349931683902, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='works on the assumption that if one region feature is useful', bbox=BoundingBox(l=219.77301036054214, t=564.0616731144275, r=480.5939607884243, b=573.9943831692703, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='to compute at a set spatial region, then it is likely to be useful in another region.', bbox=BoundingBox(l=134.76501022108476, t=576.0166631804361, r=480.58679078841254, b=585.9493732352789, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='If we constrain each individual activation map within the output volume to the', bbox=BoundingBox(l=134.76501022108476, t=587.9726532464504, r=480.5867307884124, b=597.9053633012932, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='same weights and bias, then we will see a massive reduction in the number of', bbox=BoundingBox(l=134.76501022108476, t=599.927653312459, r=480.58670078841243, b=609.8603633673018, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='parameters being produced by the convolutional layer.', bbox=BoundingBox(l=134.76501022108476, t=611.8826633784678, r=376.0691506169492, b=621.8153634333105, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Parameter sharing works on the assumption that if one region feature is useful to compute at a set spatial region, then it is likely to be useful in another region. If we constrain each individual activation map within the output volume to the same weights and bias, then we will see a massive reduction in the number of parameters being produced by the convolutional layer.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=10, page_no=6, cluster=Cluster(id=10, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.85092163085938, t=632.0819702148438, r=480.5868507884127, b=666.8784790039062, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9838565587997437, cells=[Cell(id=65, text='As a result of this as the backpropagation stage occurs, each neuron in the out-', bbox=BoundingBox(l=134.76501022108476, t=632.8856634944344, r=480.58676078841245, b=642.8183735492772, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='put will represent the overall gradient of which can be totalled across the depth', bbox=BoundingBox(l=134.76501022108476, t=644.8406635604431, r=480.5868507884127, b=654.7733736152859, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='- thus only updating a single set of weights, as opposed to every single one.', bbox=BoundingBox(l=134.76501022108476, t=656.7956636264519, r=466.95804076605424, b=666.7283736812946, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='As a result of this as the backpropagation stage occurs, each neuron in the output will represent the overall gradient of which can be totalled across the depth - thus only updating a single set of weights, as opposed to every single one.')], body=[TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=6, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.85671997070312, t=118.58126831054688, r=480.73211669921875, b=201.79269409179688, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9878745675086975, cells=[Cell(id=2, text='The', bbox=BoundingBox(l=134.76501022108476, t=119.67181066075966, r=151.4424102484444, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='depth', bbox=BoundingBox(l=154.47302025341617, t=119.74151066114439, r=181.03331029698887, b=129.445060714722, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='of the output volume produced by the convolutional layers can be', bbox=BoundingBox(l=184.06403030196083, t=119.67181066075966, r=480.59082078841914, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='manually set through the number of neurons within the layer to a the same', bbox=BoundingBox(l=134.7650302210848, t=131.6268307267684, r=480.58676078841245, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='region of the input. This can be seen with other forms of ANNs, where the', bbox=BoundingBox(l=134.7650302210848, t=143.58184079277714, r=480.5867307884124, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='all of the neurons in the hidden layer are directly connected to every single', bbox=BoundingBox(l=134.7650302210848, t=155.53686085878599, r=480.58682078841264, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='neuron beforehand. Reducing this hyperparameter can significantly minimise', bbox=BoundingBox(l=134.7650302210848, t=167.49188092479483, r=480.5868507884127, b=177.42456097963736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='the total number of neurons of the network, but it can also significantly reduce', bbox=BoundingBox(l=134.7650302210848, t=179.44787099080895, r=480.58676078841245, b=189.3805510456516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='the pattern recognition capabilities of the model.', bbox=BoundingBox(l=134.7650302210848, t=191.4028910568178, r=348.2934605713826, b=201.33557111166033, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The depth of the output volume produced by the convolutional layers can be manually set through the number of neurons within the layer to a the same region of the input. This can be seen with other forms of ANNs, where the all of the neurons in the hidden layer are directly connected to every single neuron beforehand. Reducing this hyperparameter can significantly minimise the total number of neurons of the network, but it can also significantly reduce the pattern recognition capabilities of the model.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=6, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.76483154296875, t=211.41566467285156, r=480.6409912109375, b=282.1136415576709, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9880473613739014, cells=[Cell(id=11, text='We are also able to define the', bbox=BoundingBox(l=134.7650302210848, t=212.40490117277898, r=259.2974904253829, b=222.33758122762163, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='stride', bbox=BoundingBox(l=261.2180204285336, t=212.47460117316382, r=287.220400471191, b=222.17816122674128, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='in which we set the depth around the spatial', bbox=BoundingBox(l=289.1430104743451, t=212.40490117277898, r=480.5943607884249, b=222.33758122762163, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='dimensionality of the input in order to place the receptive field. For example if', bbox=BoundingBox(l=134.76500022108476, t=224.3609012387932, r=480.58682078841264, b=234.29357129363575, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='we were to set a stride as 1, then we would have a heavily overlapped receptive', bbox=BoundingBox(l=134.76500022108476, t=236.31591130480183, r=480.58676078841245, b=246.24859135964448, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='field producing extremely large activations. Alternatively, setting the stride to a', bbox=BoundingBox(l=134.76500022108476, t=248.27093137081079, r=480.5867307884124, b=258.2036114256533, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='greater number will reduce the amount of overlapping and produce an output', bbox=BoundingBox(l=134.76500022108476, t=260.2259514368196, r=480.5867307884124, b=270.1586314916623, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='of lower spatial dimensions.', bbox=BoundingBox(l=134.76500022108476, t=272.18096150282827, r=258.9986004248926, b=282.1136415576709, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='We are also able to define the stride in which we set the depth around the spatial dimensionality of the input in order to place the receptive field. For example if we were to set a stride as 1, then we would have a heavily overlapped receptive field producing extremely large activations. Alternatively, setting the stride to a greater number will reduce the amount of overlapping and produce an output of lower spatial dimensions.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=4, page_no=6, cluster=Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.788818359375, t=292.6251220703125, r=480.5876207884139, b=327.02667180565516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9830900430679321, cells=[Cell(id=19, text='Zero-padding', bbox=BoundingBox(l=134.76500022108476, t=293.25366161917987, r=197.31020032369145, b=302.95721167275724, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='is the simple process of padding the border of the input, and', bbox=BoundingBox(l=201.02701032978894, t=293.1839616187949, r=480.5876207884139, b=303.1166316736376, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='is an effective method to give further control as to the dimensionality of the', bbox=BoundingBox(l=134.76501022108476, t=305.13897168480366, r=480.58676078841245, b=315.0716517396463, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='output volumes.', bbox=BoundingBox(l=134.76501022108476, t=317.0939917508125, r=207.4521303403295, b=327.02667180565516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Zero-padding is the simple process of padding the border of the input, and is an effective method to give further control as to the dimensionality of the output volumes.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=6, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.85841369628906, t=337.35748291015625, r=480.78466796875, b=372.1994323730469, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9820359349250793, cells=[Cell(id=23, text='It is important to understand that through using these techniques, we will alter', bbox=BoundingBox(l=134.76501022108476, t=338.0969518667789, r=480.58682078841264, b=348.0296619216217, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='the spatial dimensionality of the convolutional layers output. To calculate this,', bbox=BoundingBox(l=134.76501022108476, t=350.0519419327876, r=480.5867307884124, b=359.98465198763034, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='you can make use of the following formula:', bbox=BoundingBox(l=134.76501022108476, t=362.0069219987962, r=326.46536053557315, b=371.93963205363895, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='It is important to understand that through using these techniques, we will alter the spatial dimensionality of the convolutional layers output. To calculate this, you can make use of the following formula:'), TextElement(label=<DocItemLabel.FORMULA: 'formula'>, id=6, page_no=6, cluster=Cluster(id=6, label=<DocItemLabel.FORMULA: 'formula'>, bbox=BoundingBox(l=276.7643127441406, t=395.47357177734375, r=338.3543395996094, b=418.7127623118935, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9492904543876648, cells=[Cell(id=26, text='(', bbox=BoundingBox(l=277.54602045532, t=396.29214218809966, r=281.42047046167613, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='V', bbox=BoundingBox(l=281.42102046167696, t=396.29214218809966, r=287.23221047121035, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='−', bbox=BoundingBox(l=291.66003047847425, t=395.73422218501906, r=299.4089404911865, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='R', bbox=BoundingBox(l=301.6230504948188, t=396.29214218809966, r=309.18765050722874, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text=') + 2', bbox=BoundingBox(l=309.2640405073541, t=396.29214218809966, r=330.29208054185096, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='Z', bbox=BoundingBox(l=330.2960505418574, t=396.29214218809966, r=337.09653055301374, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='S', bbox=BoundingBox(l=295.7569904851954, t=409.8659922630467, r=301.8660604952175, b=418.7127623118935, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='+ 1', bbox=BoundingBox(l=304.6550004997928, t=409.8659922630467, r=319.59689052430525, b=418.7127623118935, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='( V − R ) + 2 Z S + 1'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=6, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=134.1217041015625, t=437.6549987792969, r=480.6238708496094, b=496.4444885253906, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.986332356929779, cells=[Cell(id=34, text='Where', bbox=BoundingBox(l=134.76500022108476, t=438.3707524204338, r=163.82590026875977, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='V', bbox=BoundingBox(l=165.8860002721394, t=438.579982421589, r=171.6971902816728, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='represents the input volume size (', bbox=BoundingBox(l=175.97101028868408, t=438.3707524204338, r=323.0787705300174, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='height', bbox=BoundingBox(l=323.0780005300161, t=438.579982421589, r=349.92023057405135, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='×', bbox=BoundingBox(l=350.5410205750698, t=438.02206241850854, r=358.289920587782, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='width', bbox=BoundingBox(l=358.9100005887993, t=438.579982421589, r=383.8175006296605, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='×', bbox=BoundingBox(l=384.43600063067515, t=438.02206241850854, r=392.18491064338747, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='depth', bbox=BoundingBox(l=392.80499064440465, t=438.579982421589, r=417.71249068526595, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='),', bbox=BoundingBox(l=417.71201068526517, t=438.3707524204338, r=423.52020069479363, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='R', bbox=BoundingBox(l=425.5800206981728, t=438.579982421589, r=433.1446207105826, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='represents', bbox=BoundingBox(l=435.28201071408915, t=438.3707524204338, r=480.59192078842096, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='the receptive field size,', bbox=BoundingBox(l=134.76501022108476, t=450.3257424864424, r=236.31381038767768, b=460.25845254128524, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='Z', bbox=BoundingBox(l=239.082020392219, t=450.5349724875977, r=245.8824904033753, b=459.3817425364445, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='is the amount of zero padding set and', bbox=BoundingBox(l=249.36801040909333, t=450.3257424864424, r=418.18417068603975, b=460.25845254128524, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='S', bbox=BoundingBox(l=420.95203069058044, t=450.5349724875977, r=427.06110070060254, b=459.3817425364445, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='referring to', bbox=BoundingBox(l=430.4080207060932, t=450.3257424864424, r=480.58963078841714, b=460.25845254128524, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='the stride. If the calculated result from this equation is not equal to a whole', bbox=BoundingBox(l=134.76501022108476, t=462.28073255245107, r=480.58676078841245, b=472.2134426072938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='integer then the stride has been incorrectly set, as the neurons will be unable to', bbox=BoundingBox(l=134.76501022108476, t=474.2357126184597, r=480.58679078841254, b=484.1684226733024, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='fit neatly across the given input.', bbox=BoundingBox(l=134.76501022108476, t=486.19070268446836, r=275.6660504522358, b=496.1234127393111, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Where V represents the input volume size ( height × width × depth ), R represents the receptive field size, Z is the amount of zero padding set and S referring to the stride. If the calculated result from this equation is not equal to a whole integer then the stride has been incorrectly set, as the neurons will be unable to fit neatly across the given input.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=6, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8834686279297, t=506.8936462402344, r=480.5930507884228, b=552.9923730533092, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9862993359565735, cells=[Cell(id=53, text='Despite our best efforts so far we will still find that our models are still enor-', bbox=BoundingBox(l=134.76501022108476, t=507.19369280043486, r=480.58691078841275, b=517.1264028552775, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='mous if we use an image input of any', bbox=BoundingBox(l=134.76501022108476, t=519.1486828664436, r=304.7069704998781, b=529.0813929212864, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='real', bbox=BoundingBox(l=307.7520105048735, t=519.0789428660585, r=322.5166005290951, b=528.8721929201313, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='dimensionality. However, methods', bbox=BoundingBox(l=325.5650005340961, t=519.1486828664436, r=480.5930507884228, b=529.0813929212864, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='have been developed as to greatly curtail the overall number of parameters', bbox=BoundingBox(l=134.76500022108476, t=531.1036629324522, r=480.5866107884122, b=541.036372987295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='within the convolutional layer.', bbox=BoundingBox(l=134.76500022108476, t=543.0596629984664, r=269.78809044259293, b=552.9923730533092, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Despite our best efforts so far we will still find that our models are still enormous if we use an image input of any real dimensionality. However, methods have been developed as to greatly curtail the overall number of parameters within the convolutional layer.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=9, page_no=6, cluster=Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8805694580078, t=563.507568359375, r=480.5939607884243, b=622.790283203125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9858874678611755, cells=[Cell(id=59, text='Parameter sharing', bbox=BoundingBox(l=134.76500022108476, t=564.1314031148125, r=217.39481035664068, b=573.8349931683902, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='works on the assumption that if one region feature is useful', bbox=BoundingBox(l=219.77301036054214, t=564.0616731144275, r=480.5939607884243, b=573.9943831692703, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='to compute at a set spatial region, then it is likely to be useful in another region.', bbox=BoundingBox(l=134.76501022108476, t=576.0166631804361, r=480.58679078841254, b=585.9493732352789, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='If we constrain each individual activation map within the output volume to the', bbox=BoundingBox(l=134.76501022108476, t=587.9726532464504, r=480.5867307884124, b=597.9053633012932, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='same weights and bias, then we will see a massive reduction in the number of', bbox=BoundingBox(l=134.76501022108476, t=599.927653312459, r=480.58670078841243, b=609.8603633673018, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='parameters being produced by the convolutional layer.', bbox=BoundingBox(l=134.76501022108476, t=611.8826633784678, r=376.0691506169492, b=621.8153634333105, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Parameter sharing works on the assumption that if one region feature is useful to compute at a set spatial region, then it is likely to be useful in another region. If we constrain each individual activation map within the output volume to the same weights and bias, then we will see a massive reduction in the number of parameters being produced by the convolutional layer.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=10, page_no=6, cluster=Cluster(id=10, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.85092163085938, t=632.0819702148438, r=480.5868507884127, b=666.8784790039062, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9838565587997437, cells=[Cell(id=65, text='As a result of this as the backpropagation stage occurs, each neuron in the out-', bbox=BoundingBox(l=134.76501022108476, t=632.8856634944344, r=480.58676078841245, b=642.8183735492772, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='put will represent the overall gradient of which can be totalled across the depth', bbox=BoundingBox(l=134.76501022108476, t=644.8406635604431, r=480.5868507884127, b=654.7733736152859, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='- thus only updating a single set of weights, as opposed to every single one.', bbox=BoundingBox(l=134.76501022108476, t=656.7956636264519, r=466.95804076605424, b=666.7283736812946, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='As a result of this as the backpropagation stage occurs, each neuron in the output will represent the overall gradient of which can be totalled across the depth - thus only updating a single set of weights, as opposed to every single one.')], headers=[TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=6, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=255.95968627929688, t=93.53328704833984, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9278361201286316, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=6, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=475.1293029785156, t=93.63327026367188, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8769420981407166, cells=[Cell(id=1, text='7', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='7')])), Page(page_no=7, size=Size(width=595.2760009765625, height=841.8900146484375), cells=[Cell(id=0, text='8', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=2, text='2.3', bbox=BoundingBox(l=134.76500022108476, t=119.74151066114439, r=147.21825024151457, b=129.445060714722, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='Pooling layer', bbox=BoundingBox(l=157.18085025785842, t=119.74151066114439, r=217.79330035729438, b=129.445060714722, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='Pooling layers aim to gradually reduce the dimensionality of the representa-', bbox=BoundingBox(l=134.76500022108476, t=148.41980081948964, r=480.58676078841245, b=158.3524708743322, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='tion, and thus further reduce the number of parameters and the computational', bbox=BoundingBox(l=134.76500022108476, t=160.37579088550376, r=480.5867307884124, b=170.3084709403464, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='complexity of the model.', bbox=BoundingBox(l=134.76500022108476, t=172.3308109515126, r=244.44325040101418, b=182.26348100635516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='The pooling layer operates over each activation map in the input, and scales', bbox=BoundingBox(l=134.76500022108476, t=191.21179105576266, r=480.58679078841254, b=201.1444711106052, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='its dimensionality using the “MAX” function. In most CNNs, these come in the', bbox=BoundingBox(l=134.76500022108476, t=203.1668011217714, r=480.58667078841233, b=213.09948117661406, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='form of', bbox=BoundingBox(l=134.76500022108476, t=215.12182118778026, r=168.0400802756732, b=225.0545012426228, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='max-pooling layers', bbox=BoundingBox(l=171.07899028065862, t=215.1915211881651, r=258.78973042454993, b=224.89508124174267, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='with kernels of a dimensionality of', bbox=BoundingBox(l=261.8280004295342, t=215.12182118778026, r=417.961820685675, b=225.0545012426228, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='2', bbox=BoundingBox(l=421.0000006906592, t=215.33099118893517, r=425.9812906988311, b=224.17779123778212, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='×', bbox=BoundingBox(l=428.5979907031238, t=214.77313118585494, r=436.346890715836, b=224.17779123778212, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='2', bbox=BoundingBox(l=438.9629807201278, t=215.33099118893517, r=443.94427072829967, b=224.17779123778212, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='applied', bbox=BoundingBox(l=446.97800073327664, t=215.12182118778026, r=480.59180078842076, b=225.0545012426228, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='with a stride of', bbox=BoundingBox(l=134.76498022108473, t=227.07782125379447, r=205.8581103377145, b=237.01049130863703, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='2', bbox=BoundingBox(l=209.82999034423042, t=227.2869812549493, r=214.81128035240235, b=236.13378130379635, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='along the spatial dimensions of the input. This scales the', bbox=BoundingBox(l=218.78899035892786, t=227.07782125379447, r=480.5960707884277, b=237.01049130863703, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='activation map down to 25% of the original size - whilst maintaining the depth', bbox=BoundingBox(l=134.76498022108473, t=239.03283131980334, r=480.58670078841243, b=248.96551137464587, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='volume to its standard size.', bbox=BoundingBox(l=134.76498022108473, t=250.98785138581206, r=255.8603704197442, b=260.9205314406546, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='Due to the destructive nature of the pooling layer, there are only two generally', bbox=BoundingBox(l=134.76498022108473, t=269.8688314900621, r=480.58679078841254, b=279.80151154490466, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='observed methods of max-pooling. Usually, the stride and filters of the pooling', bbox=BoundingBox(l=134.76498022108473, t=281.82385155607096, r=480.58682078841264, b=291.7565316109135, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='layers are both set to', bbox=BoundingBox(l=134.76498022108473, t=293.779841622085, r=228.8518103754361, b=303.7125216769276, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='2', bbox=BoundingBox(l=232.17899038089442, t=293.98901162324, r=237.16028038906634, b=302.83581167208695, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='×', bbox=BoundingBox(l=239.99298039371342, t=293.43115162015977, r=247.74190040642566, b=302.83581167208695, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='2', bbox=BoundingBox(l=250.57397041107177, t=293.98901162324, r=255.55527041924367, b=302.83581167208695, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text=', which will allow the layer to extend through the', bbox=BoundingBox(l=255.55498041924324, t=293.779841622085, r=480.590150788418, b=303.7125216769276, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='entirety of the spatial dimensionality of the input. Furthermore', bbox=BoundingBox(l=134.76498022108473, t=305.7348616880938, r=421.52844069152604, b=315.66754174293646, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='overlapping', bbox=BoundingBox(l=425.23898069761333, t=305.80456168847866, r=480.5911907884197, b=315.50811174205614, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='pooling', bbox=BoundingBox(l=134.76498022108473, t=317.7595817544875, r=170.19199027920348, b=327.4631318080651, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='may be utilised, where the stride is set to', bbox=BoundingBox(l=173.6349802848518, t=317.68988175410277, r=360.75253059182194, b=327.6225518089452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='2', bbox=BoundingBox(l=364.1929905974661, t=317.8990417552576, r=369.174290605638, b=326.74584180410454, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='with a kernel size set to', bbox=BoundingBox(l=372.6170006112859, t=317.68988175410277, r=480.5916407884205, b=327.6225518089452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='3', bbox=BoundingBox(l=134.76500022108476, t=329.8540618212663, r=139.74629022925666, b=338.7008318701132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='. Due to the destructive nature of pooling, having a kernel size above', bbox=BoundingBox(l=139.7460002292562, t=329.6448318201111, r=452.0536207416033, b=339.57754187495385, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='3', bbox=BoundingBox(l=455.33002074697833, t=329.8540618212663, r=460.3113107551502, b=338.7008318701132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='will', bbox=BoundingBox(l=463.5860307605225, t=329.6448318201111, r=480.5921907884214, b=339.57754187495385, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='usually greatly decrease the performance of the model.', bbox=BoundingBox(l=134.76501022108476, t=341.59982188611974, r=376.31824061735784, b=351.5325319409625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='It is also important to understand that beyond max-pooling, CNN architectures', bbox=BoundingBox(l=134.76501022108476, t=360.4818119903754, r=480.58679078841254, b=370.4145220452181, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='may contain general-pooling.', bbox=BoundingBox(l=134.76501022108476, t=372.436792056384, r=263.6810304325742, b=382.3695021112268, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='General pooling', bbox=BoundingBox(l=265.9370104362751, t=372.506532056769, r=340.13846055800417, b=382.21011211034676, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='layers are comprised of pooling', bbox=BoundingBox(l=342.3940105617044, t=372.436792056384, r=480.59521078842636, b=382.3695021112268, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='neurons that are able to perform a multitude of common operations including', bbox=BoundingBox(l=134.76501022108476, t=384.39178212239267, r=480.58679078841254, b=394.3244921772354, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='L1/L2-normalisation, and average pooling. However, this tutorial will primar-', bbox=BoundingBox(l=134.76501022108476, t=396.34677218840125, r=480.5868507884127, b=406.2794822432441, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='ily focus on the use of max-pooling.', bbox=BoundingBox(l=134.76501022108476, t=408.30175225440985, r=292.00470047903974, b=418.23446230925265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='2.4', bbox=BoundingBox(l=134.76501022108476, t=447.08248246853503, r=147.21826024151457, b=456.78607252211265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='Fully-connected layer', bbox=BoundingBox(l=157.18086025785843, t=447.08248246853503, r=255.41211041900885, b=456.78607252211265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='The fully-connected layer contains neurons of which are directly connected to', bbox=BoundingBox(l=134.76501022108476, t=475.76174262688556, r=480.58682078841264, b=485.6944526817283, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='the neurons in the two adjacent layers, without being connected to any layers', bbox=BoundingBox(l=134.76501022108476, t=487.7167326928942, r=480.58676078841245, b=497.649442747737, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='within them. This is analogous to way that neurons are arranged in traditional', bbox=BoundingBox(l=134.76501022108476, t=499.6717227589028, r=480.5867307884124, b=509.60443281374563, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='forms of ANN. (Figure 1)', bbox=BoundingBox(l=134.76501022108476, t=511.62771282491707, r=246.1269404037763, b=521.5604228797598, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='3', bbox=BoundingBox(l=134.76501022108476, t=550.9896830422515, r=140.74261023089116, b=562.6340631065451, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='Recipes', bbox=BoundingBox(l=152.6978102505039, t=550.9896830422515, r=195.18660032020762, b=562.6340631065451, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='Despite the relatively small number of layers required to form a CNN, there', bbox=BoundingBox(l=134.76501022108476, t=585.0647232303944, r=480.58682078841264, b=594.9974332852372, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='is no set way of formulating a CNN architecture. That being said, it would be', bbox=BoundingBox(l=134.76501022108476, t=597.0197232964031, r=480.58679078841254, b=606.9524333512459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='idiotic to simply throw a few of layers together and expect it to work. Through', bbox=BoundingBox(l=134.76501022108476, t=608.9747333624118, r=480.58682078841264, b=618.9074434172546, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='reading of related literature it is obvious that much like other forms of ANNs,', bbox=BoundingBox(l=134.76501022108476, t=620.9297334284206, r=480.5868507884127, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='CNNs tend to follow a common architecture. This common architecture is illus-', bbox=BoundingBox(l=134.76501022108476, t=632.8857234944347, r=480.58676078841245, b=642.8184335492775, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='trated in Figure 2, where convolutional layers are stacked, followed by pooling', bbox=BoundingBox(l=134.76501022108476, t=644.8407235604434, r=480.58688078841266, b=654.7734336152862, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='layers in a repeated manner before feeding forward to fully-connected layers.', bbox=BoundingBox(l=134.76501022108476, t=656.7957336264523, r=475.0078107792602, b=666.728433681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))], predictions=PagePredictions(layout=LayoutPrediction(clusters=[Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=133.78009033203125, t=93.9936294555664, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7759202718734741, cells=[Cell(id=0, text='8', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=167.4324493408203, t=93.5379638671875, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7988561391830444, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=2, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.94790649414062, t=118.79036712646484, r=218.04713439941406, b=129.75680541992188, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9537138938903809, cells=[Cell(id=2, text='2.3', bbox=BoundingBox(l=134.76500022108476, t=119.74151066114439, r=147.21825024151457, b=129.445060714722, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='Pooling layer', bbox=BoundingBox(l=157.18085025785842, t=119.74151066114439, r=217.79330035729438, b=129.445060714722, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=134.07662963867188, t=147.324462890625, r=480.58676078841245, b=182.26348100635516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9842262864112854, cells=[Cell(id=4, text='Pooling layers aim to gradually reduce the dimensionality of the representa-', bbox=BoundingBox(l=134.76500022108476, t=148.41980081948964, r=480.58676078841245, b=158.3524708743322, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='tion, and thus further reduce the number of parameters and the computational', bbox=BoundingBox(l=134.76500022108476, t=160.37579088550376, r=480.5867307884124, b=170.3084709403464, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='complexity of the model.', bbox=BoundingBox(l=134.76500022108476, t=172.3308109515126, r=244.44325040101418, b=182.26348100635516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.83888244628906, t=190.89772033691406, r=480.61639404296875, b=260.9205314406546, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9867627024650574, cells=[Cell(id=7, text='The pooling layer operates over each activation map in the input, and scales', bbox=BoundingBox(l=134.76500022108476, t=191.21179105576266, r=480.58679078841254, b=201.1444711106052, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='its dimensionality using the “MAX” function. In most CNNs, these come in the', bbox=BoundingBox(l=134.76500022108476, t=203.1668011217714, r=480.58667078841233, b=213.09948117661406, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='form of', bbox=BoundingBox(l=134.76500022108476, t=215.12182118778026, r=168.0400802756732, b=225.0545012426228, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='max-pooling layers', bbox=BoundingBox(l=171.07899028065862, t=215.1915211881651, r=258.78973042454993, b=224.89508124174267, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='with kernels of a dimensionality of', bbox=BoundingBox(l=261.8280004295342, t=215.12182118778026, r=417.961820685675, b=225.0545012426228, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='2', bbox=BoundingBox(l=421.0000006906592, t=215.33099118893517, r=425.9812906988311, b=224.17779123778212, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='×', bbox=BoundingBox(l=428.5979907031238, t=214.77313118585494, r=436.346890715836, b=224.17779123778212, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='2', bbox=BoundingBox(l=438.9629807201278, t=215.33099118893517, r=443.94427072829967, b=224.17779123778212, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='applied', bbox=BoundingBox(l=446.97800073327664, t=215.12182118778026, r=480.59180078842076, b=225.0545012426228, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='with a stride of', bbox=BoundingBox(l=134.76498022108473, t=227.07782125379447, r=205.8581103377145, b=237.01049130863703, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='2', bbox=BoundingBox(l=209.82999034423042, t=227.2869812549493, r=214.81128035240235, b=236.13378130379635, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='along the spatial dimensions of the input. This scales the', bbox=BoundingBox(l=218.78899035892786, t=227.07782125379447, r=480.5960707884277, b=237.01049130863703, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='activation map down to 25% of the original size - whilst maintaining the depth', bbox=BoundingBox(l=134.76498022108473, t=239.03283131980334, r=480.58670078841243, b=248.96551137464587, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='volume to its standard size.', bbox=BoundingBox(l=134.76498022108473, t=250.98785138581206, r=255.8603704197442, b=260.9205314406546, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8048095703125, t=269.3749694824219, r=480.6969909667969, b=351.6319580078125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9878619909286499, cells=[Cell(id=21, text='Due to the destructive nature of the pooling layer, there are only two generally', bbox=BoundingBox(l=134.76498022108473, t=269.8688314900621, r=480.58679078841254, b=279.80151154490466, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='observed methods of max-pooling. Usually, the stride and filters of the pooling', bbox=BoundingBox(l=134.76498022108473, t=281.82385155607096, r=480.58682078841264, b=291.7565316109135, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='layers are both set to', bbox=BoundingBox(l=134.76498022108473, t=293.779841622085, r=228.8518103754361, b=303.7125216769276, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='2', bbox=BoundingBox(l=232.17899038089442, t=293.98901162324, r=237.16028038906634, b=302.83581167208695, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='×', bbox=BoundingBox(l=239.99298039371342, t=293.43115162015977, r=247.74190040642566, b=302.83581167208695, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='2', bbox=BoundingBox(l=250.57397041107177, t=293.98901162324, r=255.55527041924367, b=302.83581167208695, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text=', which will allow the layer to extend through the', bbox=BoundingBox(l=255.55498041924324, t=293.779841622085, r=480.590150788418, b=303.7125216769276, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='entirety of the spatial dimensionality of the input. Furthermore', bbox=BoundingBox(l=134.76498022108473, t=305.7348616880938, r=421.52844069152604, b=315.66754174293646, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='overlapping', bbox=BoundingBox(l=425.23898069761333, t=305.80456168847866, r=480.5911907884197, b=315.50811174205614, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='pooling', bbox=BoundingBox(l=134.76498022108473, t=317.7595817544875, r=170.19199027920348, b=327.4631318080651, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='may be utilised, where the stride is set to', bbox=BoundingBox(l=173.6349802848518, t=317.68988175410277, r=360.75253059182194, b=327.6225518089452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='2', bbox=BoundingBox(l=364.1929905974661, t=317.8990417552576, r=369.174290605638, b=326.74584180410454, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='with a kernel size set to', bbox=BoundingBox(l=372.6170006112859, t=317.68988175410277, r=480.5916407884205, b=327.6225518089452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='3', bbox=BoundingBox(l=134.76500022108476, t=329.8540618212663, r=139.74629022925666, b=338.7008318701132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='. Due to the destructive nature of pooling, having a kernel size above', bbox=BoundingBox(l=139.7460002292562, t=329.6448318201111, r=452.0536207416033, b=339.57754187495385, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='3', bbox=BoundingBox(l=455.33002074697833, t=329.8540618212663, r=460.3113107551502, b=338.7008318701132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='will', bbox=BoundingBox(l=463.5860307605225, t=329.6448318201111, r=480.5921907884214, b=339.57754187495385, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='usually greatly decrease the performance of the model.', bbox=BoundingBox(l=134.76501022108476, t=341.59982188611974, r=376.31824061735784, b=351.5325319409625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.9580078125, t=359.5020751953125, r=480.59521078842636, b=418.52410888671875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9869884252548218, cells=[Cell(id=39, text='It is also important to understand that beyond max-pooling, CNN architectures', bbox=BoundingBox(l=134.76501022108476, t=360.4818119903754, r=480.58679078841254, b=370.4145220452181, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='may contain general-pooling.', bbox=BoundingBox(l=134.76501022108476, t=372.436792056384, r=263.6810304325742, b=382.3695021112268, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='General pooling', bbox=BoundingBox(l=265.9370104362751, t=372.506532056769, r=340.13846055800417, b=382.21011211034676, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='layers are comprised of pooling', bbox=BoundingBox(l=342.3940105617044, t=372.436792056384, r=480.59521078842636, b=382.3695021112268, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='neurons that are able to perform a multitude of common operations including', bbox=BoundingBox(l=134.76501022108476, t=384.39178212239267, r=480.58679078841254, b=394.3244921772354, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='L1/L2-normalisation, and average pooling. However, this tutorial will primar-', bbox=BoundingBox(l=134.76501022108476, t=396.34677218840125, r=480.5868507884127, b=406.2794822432441, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='ily focus on the use of max-pooling.', bbox=BoundingBox(l=134.76501022108476, t=408.30175225440985, r=292.00470047903974, b=418.23446230925265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=7, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.02223205566406, t=446.0529479980469, r=255.7871551513672, b=456.9926452636719, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9582642912864685, cells=[Cell(id=46, text='2.4', bbox=BoundingBox(l=134.76501022108476, t=447.08248246853503, r=147.21826024151457, b=456.78607252211265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='Fully-connected layer', bbox=BoundingBox(l=157.18086025785843, t=447.08248246853503, r=255.41211041900885, b=456.78607252211265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.71995544433594, t=474.6971435546875, r=480.58682078841264, b=521.6184692382812, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9859198331832886, cells=[Cell(id=48, text='The fully-connected layer contains neurons of which are directly connected to', bbox=BoundingBox(l=134.76501022108476, t=475.76174262688556, r=480.58682078841264, b=485.6944526817283, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='the neurons in the two adjacent layers, without being connected to any layers', bbox=BoundingBox(l=134.76501022108476, t=487.7167326928942, r=480.58676078841245, b=497.649442747737, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='within them. This is analogous to way that neurons are arranged in traditional', bbox=BoundingBox(l=134.76501022108476, t=499.6717227589028, r=480.5867307884124, b=509.60443281374563, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='forms of ANN. (Figure 1)', bbox=BoundingBox(l=134.76501022108476, t=511.62771282491707, r=246.1269404037763, b=521.5604228797598, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=9, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.84336853027344, t=550.3342895507812, r=195.18660032020762, b=562.74267578125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9564018249511719, cells=[Cell(id=52, text='3', bbox=BoundingBox(l=134.76501022108476, t=550.9896830422515, r=140.74261023089116, b=562.6340631065451, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='Recipes', bbox=BoundingBox(l=152.6978102505039, t=550.9896830422515, r=195.18660032020762, b=562.6340631065451, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=10, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87806701660156, t=583.9160766601562, r=480.6416015625, b=666.999755859375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9855859875679016, cells=[Cell(id=54, text='Despite the relatively small number of layers required to form a CNN, there', bbox=BoundingBox(l=134.76501022108476, t=585.0647232303944, r=480.58682078841264, b=594.9974332852372, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='is no set way of formulating a CNN architecture. That being said, it would be', bbox=BoundingBox(l=134.76501022108476, t=597.0197232964031, r=480.58679078841254, b=606.9524333512459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='idiotic to simply throw a few of layers together and expect it to work. Through', bbox=BoundingBox(l=134.76501022108476, t=608.9747333624118, r=480.58682078841264, b=618.9074434172546, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='reading of related literature it is obvious that much like other forms of ANNs,', bbox=BoundingBox(l=134.76501022108476, t=620.9297334284206, r=480.5868507884127, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='CNNs tend to follow a common architecture. This common architecture is illus-', bbox=BoundingBox(l=134.76501022108476, t=632.8857234944347, r=480.58676078841245, b=642.8184335492775, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='trated in Figure 2, where convolutional layers are stacked, followed by pooling', bbox=BoundingBox(l=134.76501022108476, t=644.8407235604434, r=480.58688078841266, b=654.7734336152862, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='layers in a repeated manner before feeding forward to fully-connected layers.', bbox=BoundingBox(l=134.76501022108476, t=656.7957336264523, r=475.0078107792602, b=666.728433681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))])]), tablestructure=TableStructurePrediction(table_map={}), figures_classification=None, equations_prediction=None), assembled=AssembledUnit(elements=[TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=7, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=133.78009033203125, t=93.9936294555664, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7759202718734741, cells=[Cell(id=0, text='8', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='8'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=7, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=167.4324493408203, t=93.5379638671875, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7988561391830444, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea et al.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=2, page_no=7, cluster=Cluster(id=2, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.94790649414062, t=118.79036712646484, r=218.04713439941406, b=129.75680541992188, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9537138938903809, cells=[Cell(id=2, text='2.3', bbox=BoundingBox(l=134.76500022108476, t=119.74151066114439, r=147.21825024151457, b=129.445060714722, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='Pooling layer', bbox=BoundingBox(l=157.18085025785842, t=119.74151066114439, r=217.79330035729438, b=129.445060714722, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2.3 Pooling layer'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=7, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=134.07662963867188, t=147.324462890625, r=480.58676078841245, b=182.26348100635516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9842262864112854, cells=[Cell(id=4, text='Pooling layers aim to gradually reduce the dimensionality of the representa-', bbox=BoundingBox(l=134.76500022108476, t=148.41980081948964, r=480.58676078841245, b=158.3524708743322, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='tion, and thus further reduce the number of parameters and the computational', bbox=BoundingBox(l=134.76500022108476, t=160.37579088550376, r=480.5867307884124, b=170.3084709403464, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='complexity of the model.', bbox=BoundingBox(l=134.76500022108476, t=172.3308109515126, r=244.44325040101418, b=182.26348100635516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Pooling layers aim to gradually reduce the dimensionality of the representation, and thus further reduce the number of parameters and the computational complexity of the model.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=4, page_no=7, cluster=Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.83888244628906, t=190.89772033691406, r=480.61639404296875, b=260.9205314406546, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9867627024650574, cells=[Cell(id=7, text='The pooling layer operates over each activation map in the input, and scales', bbox=BoundingBox(l=134.76500022108476, t=191.21179105576266, r=480.58679078841254, b=201.1444711106052, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='its dimensionality using the “MAX” function. In most CNNs, these come in the', bbox=BoundingBox(l=134.76500022108476, t=203.1668011217714, r=480.58667078841233, b=213.09948117661406, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='form of', bbox=BoundingBox(l=134.76500022108476, t=215.12182118778026, r=168.0400802756732, b=225.0545012426228, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='max-pooling layers', bbox=BoundingBox(l=171.07899028065862, t=215.1915211881651, r=258.78973042454993, b=224.89508124174267, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='with kernels of a dimensionality of', bbox=BoundingBox(l=261.8280004295342, t=215.12182118778026, r=417.961820685675, b=225.0545012426228, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='2', bbox=BoundingBox(l=421.0000006906592, t=215.33099118893517, r=425.9812906988311, b=224.17779123778212, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='×', bbox=BoundingBox(l=428.5979907031238, t=214.77313118585494, r=436.346890715836, b=224.17779123778212, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='2', bbox=BoundingBox(l=438.9629807201278, t=215.33099118893517, r=443.94427072829967, b=224.17779123778212, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='applied', bbox=BoundingBox(l=446.97800073327664, t=215.12182118778026, r=480.59180078842076, b=225.0545012426228, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='with a stride of', bbox=BoundingBox(l=134.76498022108473, t=227.07782125379447, r=205.8581103377145, b=237.01049130863703, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='2', bbox=BoundingBox(l=209.82999034423042, t=227.2869812549493, r=214.81128035240235, b=236.13378130379635, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='along the spatial dimensions of the input. This scales the', bbox=BoundingBox(l=218.78899035892786, t=227.07782125379447, r=480.5960707884277, b=237.01049130863703, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='activation map down to 25% of the original size - whilst maintaining the depth', bbox=BoundingBox(l=134.76498022108473, t=239.03283131980334, r=480.58670078841243, b=248.96551137464587, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='volume to its standard size.', bbox=BoundingBox(l=134.76498022108473, t=250.98785138581206, r=255.8603704197442, b=260.9205314406546, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The pooling layer operates over each activation map in the input, and scales its dimensionality using the “MAX” function. In most CNNs, these come in the form of max-pooling layers with kernels of a dimensionality of 2 × 2 applied with a stride of 2 along the spatial dimensions of the input. This scales the activation map down to 25% of the original size - whilst maintaining the depth volume to its standard size.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=7, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8048095703125, t=269.3749694824219, r=480.6969909667969, b=351.6319580078125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9878619909286499, cells=[Cell(id=21, text='Due to the destructive nature of the pooling layer, there are only two generally', bbox=BoundingBox(l=134.76498022108473, t=269.8688314900621, r=480.58679078841254, b=279.80151154490466, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='observed methods of max-pooling. Usually, the stride and filters of the pooling', bbox=BoundingBox(l=134.76498022108473, t=281.82385155607096, r=480.58682078841264, b=291.7565316109135, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='layers are both set to', bbox=BoundingBox(l=134.76498022108473, t=293.779841622085, r=228.8518103754361, b=303.7125216769276, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='2', bbox=BoundingBox(l=232.17899038089442, t=293.98901162324, r=237.16028038906634, b=302.83581167208695, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='×', bbox=BoundingBox(l=239.99298039371342, t=293.43115162015977, r=247.74190040642566, b=302.83581167208695, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='2', bbox=BoundingBox(l=250.57397041107177, t=293.98901162324, r=255.55527041924367, b=302.83581167208695, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text=', which will allow the layer to extend through the', bbox=BoundingBox(l=255.55498041924324, t=293.779841622085, r=480.590150788418, b=303.7125216769276, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='entirety of the spatial dimensionality of the input. Furthermore', bbox=BoundingBox(l=134.76498022108473, t=305.7348616880938, r=421.52844069152604, b=315.66754174293646, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='overlapping', bbox=BoundingBox(l=425.23898069761333, t=305.80456168847866, r=480.5911907884197, b=315.50811174205614, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='pooling', bbox=BoundingBox(l=134.76498022108473, t=317.7595817544875, r=170.19199027920348, b=327.4631318080651, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='may be utilised, where the stride is set to', bbox=BoundingBox(l=173.6349802848518, t=317.68988175410277, r=360.75253059182194, b=327.6225518089452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='2', bbox=BoundingBox(l=364.1929905974661, t=317.8990417552576, r=369.174290605638, b=326.74584180410454, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='with a kernel size set to', bbox=BoundingBox(l=372.6170006112859, t=317.68988175410277, r=480.5916407884205, b=327.6225518089452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='3', bbox=BoundingBox(l=134.76500022108476, t=329.8540618212663, r=139.74629022925666, b=338.7008318701132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='. Due to the destructive nature of pooling, having a kernel size above', bbox=BoundingBox(l=139.7460002292562, t=329.6448318201111, r=452.0536207416033, b=339.57754187495385, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='3', bbox=BoundingBox(l=455.33002074697833, t=329.8540618212663, r=460.3113107551502, b=338.7008318701132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='will', bbox=BoundingBox(l=463.5860307605225, t=329.6448318201111, r=480.5921907884214, b=339.57754187495385, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='usually greatly decrease the performance of the model.', bbox=BoundingBox(l=134.76501022108476, t=341.59982188611974, r=376.31824061735784, b=351.5325319409625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Due to the destructive nature of the pooling layer, there are only two generally observed methods of max-pooling. Usually, the stride and filters of the pooling layers are both set to 2 × 2 , which will allow the layer to extend through the entirety of the spatial dimensionality of the input. Furthermore overlapping pooling may be utilised, where the stride is set to 2 with a kernel size set to 3 . Due to the destructive nature of pooling, having a kernel size above 3 will usually greatly decrease the performance of the model.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=7, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.9580078125, t=359.5020751953125, r=480.59521078842636, b=418.52410888671875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9869884252548218, cells=[Cell(id=39, text='It is also important to understand that beyond max-pooling, CNN architectures', bbox=BoundingBox(l=134.76501022108476, t=360.4818119903754, r=480.58679078841254, b=370.4145220452181, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='may contain general-pooling.', bbox=BoundingBox(l=134.76501022108476, t=372.436792056384, r=263.6810304325742, b=382.3695021112268, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='General pooling', bbox=BoundingBox(l=265.9370104362751, t=372.506532056769, r=340.13846055800417, b=382.21011211034676, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='layers are comprised of pooling', bbox=BoundingBox(l=342.3940105617044, t=372.436792056384, r=480.59521078842636, b=382.3695021112268, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='neurons that are able to perform a multitude of common operations including', bbox=BoundingBox(l=134.76501022108476, t=384.39178212239267, r=480.58679078841254, b=394.3244921772354, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='L1/L2-normalisation, and average pooling. However, this tutorial will primar-', bbox=BoundingBox(l=134.76501022108476, t=396.34677218840125, r=480.5868507884127, b=406.2794822432441, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='ily focus on the use of max-pooling.', bbox=BoundingBox(l=134.76501022108476, t=408.30175225440985, r=292.00470047903974, b=418.23446230925265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='It is also important to understand that beyond max-pooling, CNN architectures may contain general-pooling. General pooling layers are comprised of pooling neurons that are able to perform a multitude of common operations including L1/L2-normalisation, and average pooling. However, this tutorial will primarily focus on the use of max-pooling.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=7, page_no=7, cluster=Cluster(id=7, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.02223205566406, t=446.0529479980469, r=255.7871551513672, b=456.9926452636719, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9582642912864685, cells=[Cell(id=46, text='2.4', bbox=BoundingBox(l=134.76501022108476, t=447.08248246853503, r=147.21826024151457, b=456.78607252211265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='Fully-connected layer', bbox=BoundingBox(l=157.18086025785843, t=447.08248246853503, r=255.41211041900885, b=456.78607252211265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2.4 Fully-connected layer'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=7, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.71995544433594, t=474.6971435546875, r=480.58682078841264, b=521.6184692382812, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9859198331832886, cells=[Cell(id=48, text='The fully-connected layer contains neurons of which are directly connected to', bbox=BoundingBox(l=134.76501022108476, t=475.76174262688556, r=480.58682078841264, b=485.6944526817283, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='the neurons in the two adjacent layers, without being connected to any layers', bbox=BoundingBox(l=134.76501022108476, t=487.7167326928942, r=480.58676078841245, b=497.649442747737, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='within them. This is analogous to way that neurons are arranged in traditional', bbox=BoundingBox(l=134.76501022108476, t=499.6717227589028, r=480.5867307884124, b=509.60443281374563, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='forms of ANN. (Figure 1)', bbox=BoundingBox(l=134.76501022108476, t=511.62771282491707, r=246.1269404037763, b=521.5604228797598, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The fully-connected layer contains neurons of which are directly connected to the neurons in the two adjacent layers, without being connected to any layers within them. This is analogous to way that neurons are arranged in traditional forms of ANN. (Figure 1)'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=9, page_no=7, cluster=Cluster(id=9, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.84336853027344, t=550.3342895507812, r=195.18660032020762, b=562.74267578125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9564018249511719, cells=[Cell(id=52, text='3', bbox=BoundingBox(l=134.76501022108476, t=550.9896830422515, r=140.74261023089116, b=562.6340631065451, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='Recipes', bbox=BoundingBox(l=152.6978102505039, t=550.9896830422515, r=195.18660032020762, b=562.6340631065451, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='3 Recipes'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=10, page_no=7, cluster=Cluster(id=10, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87806701660156, t=583.9160766601562, r=480.6416015625, b=666.999755859375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9855859875679016, cells=[Cell(id=54, text='Despite the relatively small number of layers required to form a CNN, there', bbox=BoundingBox(l=134.76501022108476, t=585.0647232303944, r=480.58682078841264, b=594.9974332852372, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='is no set way of formulating a CNN architecture. That being said, it would be', bbox=BoundingBox(l=134.76501022108476, t=597.0197232964031, r=480.58679078841254, b=606.9524333512459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='idiotic to simply throw a few of layers together and expect it to work. Through', bbox=BoundingBox(l=134.76501022108476, t=608.9747333624118, r=480.58682078841264, b=618.9074434172546, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='reading of related literature it is obvious that much like other forms of ANNs,', bbox=BoundingBox(l=134.76501022108476, t=620.9297334284206, r=480.5868507884127, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='CNNs tend to follow a common architecture. This common architecture is illus-', bbox=BoundingBox(l=134.76501022108476, t=632.8857234944347, r=480.58676078841245, b=642.8184335492775, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='trated in Figure 2, where convolutional layers are stacked, followed by pooling', bbox=BoundingBox(l=134.76501022108476, t=644.8407235604434, r=480.58688078841266, b=654.7734336152862, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='layers in a repeated manner before feeding forward to fully-connected layers.', bbox=BoundingBox(l=134.76501022108476, t=656.7957336264523, r=475.0078107792602, b=666.728433681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Despite the relatively small number of layers required to form a CNN, there is no set way of formulating a CNN architecture. That being said, it would be idiotic to simply throw a few of layers together and expect it to work. Through reading of related literature it is obvious that much like other forms of ANNs, CNNs tend to follow a common architecture. This common architecture is illustrated in Figure 2, where convolutional layers are stacked, followed by pooling layers in a repeated manner before feeding forward to fully-connected layers.')], body=[TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=2, page_no=7, cluster=Cluster(id=2, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.94790649414062, t=118.79036712646484, r=218.04713439941406, b=129.75680541992188, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9537138938903809, cells=[Cell(id=2, text='2.3', bbox=BoundingBox(l=134.76500022108476, t=119.74151066114439, r=147.21825024151457, b=129.445060714722, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='Pooling layer', bbox=BoundingBox(l=157.18085025785842, t=119.74151066114439, r=217.79330035729438, b=129.445060714722, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2.3 Pooling layer'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=7, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=134.07662963867188, t=147.324462890625, r=480.58676078841245, b=182.26348100635516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9842262864112854, cells=[Cell(id=4, text='Pooling layers aim to gradually reduce the dimensionality of the representa-', bbox=BoundingBox(l=134.76500022108476, t=148.41980081948964, r=480.58676078841245, b=158.3524708743322, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='tion, and thus further reduce the number of parameters and the computational', bbox=BoundingBox(l=134.76500022108476, t=160.37579088550376, r=480.5867307884124, b=170.3084709403464, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='complexity of the model.', bbox=BoundingBox(l=134.76500022108476, t=172.3308109515126, r=244.44325040101418, b=182.26348100635516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Pooling layers aim to gradually reduce the dimensionality of the representation, and thus further reduce the number of parameters and the computational complexity of the model.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=4, page_no=7, cluster=Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.83888244628906, t=190.89772033691406, r=480.61639404296875, b=260.9205314406546, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9867627024650574, cells=[Cell(id=7, text='The pooling layer operates over each activation map in the input, and scales', bbox=BoundingBox(l=134.76500022108476, t=191.21179105576266, r=480.58679078841254, b=201.1444711106052, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='its dimensionality using the “MAX” function. In most CNNs, these come in the', bbox=BoundingBox(l=134.76500022108476, t=203.1668011217714, r=480.58667078841233, b=213.09948117661406, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='form of', bbox=BoundingBox(l=134.76500022108476, t=215.12182118778026, r=168.0400802756732, b=225.0545012426228, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='max-pooling layers', bbox=BoundingBox(l=171.07899028065862, t=215.1915211881651, r=258.78973042454993, b=224.89508124174267, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='with kernels of a dimensionality of', bbox=BoundingBox(l=261.8280004295342, t=215.12182118778026, r=417.961820685675, b=225.0545012426228, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='2', bbox=BoundingBox(l=421.0000006906592, t=215.33099118893517, r=425.9812906988311, b=224.17779123778212, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='×', bbox=BoundingBox(l=428.5979907031238, t=214.77313118585494, r=436.346890715836, b=224.17779123778212, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='2', bbox=BoundingBox(l=438.9629807201278, t=215.33099118893517, r=443.94427072829967, b=224.17779123778212, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='applied', bbox=BoundingBox(l=446.97800073327664, t=215.12182118778026, r=480.59180078842076, b=225.0545012426228, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='with a stride of', bbox=BoundingBox(l=134.76498022108473, t=227.07782125379447, r=205.8581103377145, b=237.01049130863703, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='2', bbox=BoundingBox(l=209.82999034423042, t=227.2869812549493, r=214.81128035240235, b=236.13378130379635, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='along the spatial dimensions of the input. This scales the', bbox=BoundingBox(l=218.78899035892786, t=227.07782125379447, r=480.5960707884277, b=237.01049130863703, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='activation map down to 25% of the original size - whilst maintaining the depth', bbox=BoundingBox(l=134.76498022108473, t=239.03283131980334, r=480.58670078841243, b=248.96551137464587, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='volume to its standard size.', bbox=BoundingBox(l=134.76498022108473, t=250.98785138581206, r=255.8603704197442, b=260.9205314406546, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The pooling layer operates over each activation map in the input, and scales its dimensionality using the “MAX” function. In most CNNs, these come in the form of max-pooling layers with kernels of a dimensionality of 2 × 2 applied with a stride of 2 along the spatial dimensions of the input. This scales the activation map down to 25% of the original size - whilst maintaining the depth volume to its standard size.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=7, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8048095703125, t=269.3749694824219, r=480.6969909667969, b=351.6319580078125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9878619909286499, cells=[Cell(id=21, text='Due to the destructive nature of the pooling layer, there are only two generally', bbox=BoundingBox(l=134.76498022108473, t=269.8688314900621, r=480.58679078841254, b=279.80151154490466, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='observed methods of max-pooling. Usually, the stride and filters of the pooling', bbox=BoundingBox(l=134.76498022108473, t=281.82385155607096, r=480.58682078841264, b=291.7565316109135, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='layers are both set to', bbox=BoundingBox(l=134.76498022108473, t=293.779841622085, r=228.8518103754361, b=303.7125216769276, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='2', bbox=BoundingBox(l=232.17899038089442, t=293.98901162324, r=237.16028038906634, b=302.83581167208695, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='×', bbox=BoundingBox(l=239.99298039371342, t=293.43115162015977, r=247.74190040642566, b=302.83581167208695, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='2', bbox=BoundingBox(l=250.57397041107177, t=293.98901162324, r=255.55527041924367, b=302.83581167208695, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text=', which will allow the layer to extend through the', bbox=BoundingBox(l=255.55498041924324, t=293.779841622085, r=480.590150788418, b=303.7125216769276, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='entirety of the spatial dimensionality of the input. Furthermore', bbox=BoundingBox(l=134.76498022108473, t=305.7348616880938, r=421.52844069152604, b=315.66754174293646, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='overlapping', bbox=BoundingBox(l=425.23898069761333, t=305.80456168847866, r=480.5911907884197, b=315.50811174205614, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='pooling', bbox=BoundingBox(l=134.76498022108473, t=317.7595817544875, r=170.19199027920348, b=327.4631318080651, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='may be utilised, where the stride is set to', bbox=BoundingBox(l=173.6349802848518, t=317.68988175410277, r=360.75253059182194, b=327.6225518089452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='2', bbox=BoundingBox(l=364.1929905974661, t=317.8990417552576, r=369.174290605638, b=326.74584180410454, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='with a kernel size set to', bbox=BoundingBox(l=372.6170006112859, t=317.68988175410277, r=480.5916407884205, b=327.6225518089452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='3', bbox=BoundingBox(l=134.76500022108476, t=329.8540618212663, r=139.74629022925666, b=338.7008318701132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='. Due to the destructive nature of pooling, having a kernel size above', bbox=BoundingBox(l=139.7460002292562, t=329.6448318201111, r=452.0536207416033, b=339.57754187495385, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='3', bbox=BoundingBox(l=455.33002074697833, t=329.8540618212663, r=460.3113107551502, b=338.7008318701132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='will', bbox=BoundingBox(l=463.5860307605225, t=329.6448318201111, r=480.5921907884214, b=339.57754187495385, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='usually greatly decrease the performance of the model.', bbox=BoundingBox(l=134.76501022108476, t=341.59982188611974, r=376.31824061735784, b=351.5325319409625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Due to the destructive nature of the pooling layer, there are only two generally observed methods of max-pooling. Usually, the stride and filters of the pooling layers are both set to 2 × 2 , which will allow the layer to extend through the entirety of the spatial dimensionality of the input. Furthermore overlapping pooling may be utilised, where the stride is set to 2 with a kernel size set to 3 . Due to the destructive nature of pooling, having a kernel size above 3 will usually greatly decrease the performance of the model.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=7, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.9580078125, t=359.5020751953125, r=480.59521078842636, b=418.52410888671875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9869884252548218, cells=[Cell(id=39, text='It is also important to understand that beyond max-pooling, CNN architectures', bbox=BoundingBox(l=134.76501022108476, t=360.4818119903754, r=480.58679078841254, b=370.4145220452181, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='may contain general-pooling.', bbox=BoundingBox(l=134.76501022108476, t=372.436792056384, r=263.6810304325742, b=382.3695021112268, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='General pooling', bbox=BoundingBox(l=265.9370104362751, t=372.506532056769, r=340.13846055800417, b=382.21011211034676, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='layers are comprised of pooling', bbox=BoundingBox(l=342.3940105617044, t=372.436792056384, r=480.59521078842636, b=382.3695021112268, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='neurons that are able to perform a multitude of common operations including', bbox=BoundingBox(l=134.76501022108476, t=384.39178212239267, r=480.58679078841254, b=394.3244921772354, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='L1/L2-normalisation, and average pooling. However, this tutorial will primar-', bbox=BoundingBox(l=134.76501022108476, t=396.34677218840125, r=480.5868507884127, b=406.2794822432441, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='ily focus on the use of max-pooling.', bbox=BoundingBox(l=134.76501022108476, t=408.30175225440985, r=292.00470047903974, b=418.23446230925265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='It is also important to understand that beyond max-pooling, CNN architectures may contain general-pooling. General pooling layers are comprised of pooling neurons that are able to perform a multitude of common operations including L1/L2-normalisation, and average pooling. However, this tutorial will primarily focus on the use of max-pooling.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=7, page_no=7, cluster=Cluster(id=7, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.02223205566406, t=446.0529479980469, r=255.7871551513672, b=456.9926452636719, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9582642912864685, cells=[Cell(id=46, text='2.4', bbox=BoundingBox(l=134.76501022108476, t=447.08248246853503, r=147.21826024151457, b=456.78607252211265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='Fully-connected layer', bbox=BoundingBox(l=157.18086025785843, t=447.08248246853503, r=255.41211041900885, b=456.78607252211265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2.4 Fully-connected layer'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=7, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.71995544433594, t=474.6971435546875, r=480.58682078841264, b=521.6184692382812, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9859198331832886, cells=[Cell(id=48, text='The fully-connected layer contains neurons of which are directly connected to', bbox=BoundingBox(l=134.76501022108476, t=475.76174262688556, r=480.58682078841264, b=485.6944526817283, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='the neurons in the two adjacent layers, without being connected to any layers', bbox=BoundingBox(l=134.76501022108476, t=487.7167326928942, r=480.58676078841245, b=497.649442747737, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='within them. This is analogous to way that neurons are arranged in traditional', bbox=BoundingBox(l=134.76501022108476, t=499.6717227589028, r=480.5867307884124, b=509.60443281374563, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='forms of ANN. (Figure 1)', bbox=BoundingBox(l=134.76501022108476, t=511.62771282491707, r=246.1269404037763, b=521.5604228797598, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The fully-connected layer contains neurons of which are directly connected to the neurons in the two adjacent layers, without being connected to any layers within them. This is analogous to way that neurons are arranged in traditional forms of ANN. (Figure 1)'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=9, page_no=7, cluster=Cluster(id=9, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.84336853027344, t=550.3342895507812, r=195.18660032020762, b=562.74267578125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9564018249511719, cells=[Cell(id=52, text='3', bbox=BoundingBox(l=134.76501022108476, t=550.9896830422515, r=140.74261023089116, b=562.6340631065451, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='Recipes', bbox=BoundingBox(l=152.6978102505039, t=550.9896830422515, r=195.18660032020762, b=562.6340631065451, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='3 Recipes'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=10, page_no=7, cluster=Cluster(id=10, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87806701660156, t=583.9160766601562, r=480.6416015625, b=666.999755859375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9855859875679016, cells=[Cell(id=54, text='Despite the relatively small number of layers required to form a CNN, there', bbox=BoundingBox(l=134.76501022108476, t=585.0647232303944, r=480.58682078841264, b=594.9974332852372, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='is no set way of formulating a CNN architecture. That being said, it would be', bbox=BoundingBox(l=134.76501022108476, t=597.0197232964031, r=480.58679078841254, b=606.9524333512459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='idiotic to simply throw a few of layers together and expect it to work. Through', bbox=BoundingBox(l=134.76501022108476, t=608.9747333624118, r=480.58682078841264, b=618.9074434172546, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='reading of related literature it is obvious that much like other forms of ANNs,', bbox=BoundingBox(l=134.76501022108476, t=620.9297334284206, r=480.5868507884127, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='CNNs tend to follow a common architecture. This common architecture is illus-', bbox=BoundingBox(l=134.76501022108476, t=632.8857234944347, r=480.58676078841245, b=642.8184335492775, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='trated in Figure 2, where convolutional layers are stacked, followed by pooling', bbox=BoundingBox(l=134.76501022108476, t=644.8407235604434, r=480.58688078841266, b=654.7734336152862, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='layers in a repeated manner before feeding forward to fully-connected layers.', bbox=BoundingBox(l=134.76501022108476, t=656.7957336264523, r=475.0078107792602, b=666.728433681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Despite the relatively small number of layers required to form a CNN, there is no set way of formulating a CNN architecture. That being said, it would be idiotic to simply throw a few of layers together and expect it to work. Through reading of related literature it is obvious that much like other forms of ANNs, CNNs tend to follow a common architecture. This common architecture is illustrated in Figure 2, where convolutional layers are stacked, followed by pooling layers in a repeated manner before feeding forward to fully-connected layers.')], headers=[TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=7, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=133.78009033203125, t=93.9936294555664, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7759202718734741, cells=[Cell(id=0, text='8', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='8'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=7, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=167.4324493408203, t=93.5379638671875, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7988561391830444, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea et al.')])), Page(page_no=8, size=Size(width=595.2760009765625, height=841.8900146484375), cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=1, text='9', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=2, text='Another common CNN architecture is to stack two convolutional layers before', bbox=BoundingBox(l=134.76501022108476, t=119.67181066075966, r=480.58676078841245, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='each pooling layer, as illustrated in Figure 5. This is strongly recommended as', bbox=BoundingBox(l=134.76501022108476, t=131.6268307267684, r=480.58682078841264, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='stacking multiple convolutional layers allows for more complex features of the', bbox=BoundingBox(l=134.76501022108476, t=143.58184079277714, r=480.5868507884127, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='input vector to be selected.', bbox=BoundingBox(l=134.76501022108476, t=155.53686085878599, r=252.77200041467768, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='input', bbox=BoundingBox(l=141.0171202313415, t=254.84454140710648, r=152.0187102493898, b=260.8498514402644, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='convolution w/ ReLu', bbox=BoundingBox(l=175.69994028823936, t=190.0086610491196, r=221.95847036412746, b=196.01397108227752, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='pooling', bbox=BoundingBox(l=232.13184038081707, t=190.21557105026216, r=248.78818040814215, b=196.2333910834891, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='convolution', bbox=BoundingBox(l=350.42499057487936, t=264.6906114614709, r=376.18756061714345, b=270.6959214946288, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='w/ ReLu', bbox=BoundingBox(l=353.9647205806864, t=271.1903614973588, r=372.95895061184683, b=277.20819153058585, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='pooling', bbox=BoundingBox(l=380.33847062395307, t=189.10449104412737, r=396.9493106512036, b=195.10980107728528, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='fully-connected', bbox=BoundingBox(l=399.56500065549466, t=262.8054814510623, r=433.6465807114061, b=268.8107914842202, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='w/ ReLu', bbox=BoundingBox(l=407.1688206679688, t=269.30072148692534, r=426.1585706991219, b=275.3185415201523, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='fully-connected', bbox=BoundingBox(l=420.5636606899434, t=189.10449104412737, r=454.6497807458623, b=195.10980107728528, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='convolution w/ ReLu', bbox=BoundingBox(l=268.3968804403106, t=188.21289103920446, r=314.67807051623583, b=194.2307110724313, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='pooling', bbox=BoundingBox(l=324.8378305329031, t=188.4489710405079, r=341.4487005601536, b=194.45434107366623, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='0', bbox=BoundingBox(l=472.1900607746376, t=219.15814121006656, r=474.9025007790873, b=225.16345124322447, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='9', bbox=BoundingBox(l=472.1900607746376, t=243.45690134423035, r=474.9025007790873, b=249.46221137738826, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='output ', bbox=BoundingBox(l=466.57080076541905, t=253.0985113974658, r=481.8245507904431, b=259.11633143069287, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='...', bbox=BoundingBox(l=471.47577077346574, t=230.8848212748146, r=475.54437078014035, b=236.89013130797252, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='Fig. 5: A common form of CNN architecture in which convolutional layers are', bbox=BoundingBox(l=134.76500022108476, t=288.8927615951013, r=480.58682078841264, b=298.825431649944, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='stacked between ReLus continuously before being passed through the pooling', bbox=BoundingBox(l=134.76500022108476, t=300.8477716611102, r=480.5867307884124, b=310.7804517159527, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='layer, before going between one or many fully connected ReLus.', bbox=BoundingBox(l=134.76500022108476, t=312.8027917271189, r=417.1848806844004, b=322.73547178196156, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='It is also advised to split large convolutional layers up into many smaller sized', bbox=BoundingBox(l=134.76500022108476, t=349.1437619277732, r=480.58682078841264, b=359.0764719826159, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='convolutional layers. This is to reduce the amount of computational complexity', bbox=BoundingBox(l=134.76500022108476, t=361.0987519937818, r=480.5867307884124, b=371.0314620486245, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='within a given convolutional layer. For example, if you were to stack three con-', bbox=BoundingBox(l=134.76500022108476, t=373.0537420597904, r=480.58679078841254, b=382.9864521146332, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='volutional layers on top of each other with a receptive field of', bbox=BoundingBox(l=134.76500022108476, t=385.00973212580453, r=400.48746065700794, b=394.9424421806474, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='3', bbox=BoundingBox(l=402.51202066032926, t=385.2189621269598, r=407.4933206685012, b=394.0657321758067, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='×', bbox=BoundingBox(l=407.9780306692964, t=384.6610421238793, r=415.7269306820086, b=394.0657321758067, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='3', bbox=BoundingBox(l=416.2110306828028, t=385.2189621269598, r=421.19232069097467, b=394.0657321758067, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='. Each neuron', bbox=BoundingBox(l=421.19202069097423, t=385.00973212580453, r=480.5890507884163, b=394.9424421806474, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='of the first convolutional layer will have a', bbox=BoundingBox(l=134.76501022108476, t=396.9647221918132, r=316.8913005198667, b=406.897432246656, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='3', bbox=BoundingBox(l=319.1640005235951, t=397.1739521929685, r=324.145290531767, b=406.02072224181535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='×', bbox=BoundingBox(l=325.55801053408464, t=396.6160221898879, r=333.30692054679685, b=406.02072224181535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='3', bbox=BoundingBox(l=334.72000054911507, t=397.1739521929685, r=339.70129055728694, b=406.02072224181535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='view of the input vector. A neu-', bbox=BoundingBox(l=341.97601056101865, t=396.9647221918132, r=480.59567078842707, b=406.897432246656, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='ron on the second convolutional layer will then have a', bbox=BoundingBox(l=134.76501022108476, t=408.9197022578219, r=376.1587806170962, b=418.8524123126646, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='5', bbox=BoundingBox(l=378.9180306216228, t=409.1289322589771, r=383.8993206297947, b=417.9757023078239, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='×', bbox=BoundingBox(l=386.3090206337479, t=408.5710122558965, r=394.05792064646016, b=417.9757023078239, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='5', bbox=BoundingBox(l=396.4670106504123, t=409.1289322589771, r=401.44830065858423, b=417.9757023078239, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='view of the input', bbox=BoundingBox(l=404.2030006631033, t=408.9197022578219, r=480.5961907884279, b=418.8524123126646, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='vector. A neuron on the third convolutional layer will then have a', bbox=BoundingBox(l=134.76501022108476, t=420.8746923238305, r=421.9369506921963, b=430.8074023786733, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='7', bbox=BoundingBox(l=424.30200069607616, t=421.0839223249858, r=429.2832907042481, b=429.9306923738326, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='×', bbox=BoundingBox(l=431.026000707107, t=420.5260023219052, r=438.7749007198193, b=429.9306923738326, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='7', bbox=BoundingBox(l=440.51700072267727, t=421.0839223249858, r=445.49829073084913, b=429.9306923738326, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='view of', bbox=BoundingBox(l=447.86200073472685, t=420.8746923238305, r=480.5891407884164, b=430.8074023786733, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='the input vector. As these stacks feature non-linearities which in turn allows us', bbox=BoundingBox(l=134.76501022108476, t=432.82968238983915, r=480.58679078841254, b=442.76239244468195, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='to express stronger features of the input with fewer parameters. However, it is', bbox=BoundingBox(l=134.76501022108476, t=444.78466245584775, r=480.58682078841264, b=454.7173725106905, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='important to understand that this does come with a distinct memory allocation', bbox=BoundingBox(l=134.76501022108476, t=456.74066252186196, r=480.58679078841254, b=466.67337257670476, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='problem - especially when making use of the backpropagation algorithm.', bbox=BoundingBox(l=134.76501022108476, t=468.69564258787057, r=457.4038107503804, b=478.62835264271337, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='The input layer should be recursively divisible by two. Common numbers in-', bbox=BoundingBox(l=134.76501022108476, t=487.5596626920269, r=480.58676078841245, b=497.4923727468697, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='clude', bbox=BoundingBox(l=134.76501022108476, t=499.51464275803556, r=158.95421026076767, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='32', bbox=BoundingBox(l=161.44402026485224, t=499.72387275919084, r=171.4066202811961, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='×', bbox=BoundingBox(l=173.62102028482886, t=499.16595275611024, r=181.36993029754112, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='32', bbox=BoundingBox(l=183.58302030117173, t=499.72387275919084, r=193.54562031751558, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text=',', bbox=BoundingBox(l=193.54602031751622, t=499.51464275803556, r=196.0366703216022, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='64', bbox=BoundingBox(l=198.52702032568766, t=499.72387275919084, r=208.48962034203151, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='×', bbox=BoundingBox(l=210.7040303456643, t=499.16595275611024, r=218.45294035837657, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='64', bbox=BoundingBox(l=220.66603036200718, t=499.72387275919084, r=230.628630378351, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text=',', bbox=BoundingBox(l=230.6290303783517, t=499.51464275803556, r=233.11967038243762, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='96', bbox=BoundingBox(l=235.6100303865231, t=499.72387275919084, r=245.572630402867, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='×', bbox=BoundingBox(l=247.7870304064997, t=499.16595275611024, r=255.535950419212, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='96', bbox=BoundingBox(l=257.7500304228442, t=499.72387275919084, r=267.7126204391881, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text=',', bbox=BoundingBox(l=267.7120404391871, t=499.51464275803556, r=270.20270044327305, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=65, text='128', bbox=BoundingBox(l=272.69403044736015, t=499.72387275919084, r=287.637940471876, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='×', bbox=BoundingBox(l=289.85104047550664, t=499.16595275611024, r=297.5999504882188, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='128', bbox=BoundingBox(l=299.81406049185114, t=499.72387275919084, r=314.7579705163669, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=68, text='and', bbox=BoundingBox(l=317.2490505204536, t=499.51464275803556, r=334.1157205481237, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=69, text='224', bbox=BoundingBox(l=336.6060505522091, t=499.72387275919084, r=351.54996057672497, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=70, text='×', bbox=BoundingBox(l=353.76404058035723, t=499.16595275611024, r=361.51294059306946, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=71, text='224', bbox=BoundingBox(l=363.72705059670176, t=499.72387275919084, r=378.6709606212175, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=72, text='.', bbox=BoundingBox(l=378.67105062121766, t=499.51464275803556, r=381.16171062530367, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=73, text='Whilst using small filters, set stride to one and make use of zero-padding as to', bbox=BoundingBox(l=134.76505022108483, t=518.3796328621972, r=480.5867307884124, b=528.31234291704, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=74, text='ensure that the convolutional layers do not reconfigure any of the dimension-', bbox=BoundingBox(l=134.76505022108483, t=530.3346229282059, r=480.58682078841264, b=540.2673329830487, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=75, text='ality of the input. The amount of zero-padding to be used should be calculated', bbox=BoundingBox(l=134.76505022108483, t=542.2896129942146, r=480.5868507884127, b=552.2223230490574, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=76, text='by taking one away from the receptive field size and dividing by two.activation', bbox=BoundingBox(l=134.76505022108483, t=554.2445930602232, r=480.58688078841266, b=564.177303115066, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=77, text='CNNs are extremely powerful machine learning algorithms, however they can', bbox=BoundingBox(l=134.76505022108483, t=573.109583164385, r=480.5868507884127, b=583.0422932192278, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=78, text='be horrendously resource-heavy. An example of this problem could be in filter-', bbox=BoundingBox(l=134.76505022108483, t=585.0646032303937, r=480.58698078841286, b=594.9972932852364, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=79, text='ing a large image (anything over', bbox=BoundingBox(l=134.76505022108483, t=597.0195932964024, r=277.987370456044, b=606.9523033512452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=80, text='128', bbox=BoundingBox(l=280.46405046010705, t=597.2288032975575, r=295.40796048462283, b=606.0755933464045, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=81, text='×', bbox=BoundingBox(l=297.56506048816163, t=596.6708932944771, r=305.31396050087386, b=606.0755933464045, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=82, text='128', bbox=BoundingBox(l=307.470060504411, t=597.2288032975575, r=322.41397052892677, b=606.0755933464045, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=83, text='could be considered large), so if the', bbox=BoundingBox(l=324.8900805329889, t=597.0195932964024, r=480.59555078842686, b=606.9523033512452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=84, text='input is', bbox=BoundingBox(l=134.7650802210849, t=608.974593362411, r=168.67776027671934, b=618.9073034172538, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=85, text='227', bbox=BoundingBox(l=171.53407028140518, t=609.1838033635662, r=186.47797030592096, b=618.0305934124132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=86, text='×', bbox=BoundingBox(l=188.9600703099929, t=608.6259033604858, r=196.70898032270514, b=618.0305934124132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=87, text='227', bbox=BoundingBox(l=199.19107032677704, t=609.1838033635662, r=214.13496035129282, b=618.0305934124132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=88, text='(as seen with ImageNet) and we’re filtering with 64 kernels', bbox=BoundingBox(l=216.98807035597338, t=608.974593362411, r=480.5884707884153, b=618.9073034172538, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=89, text='each with a zero padding of then the result will be three activation vectors of', bbox=BoundingBox(l=134.7650802210849, t=620.9295934284198, r=480.5868507884127, b=630.8623034832626, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=90, text='size', bbox=BoundingBox(l=134.7650802210849, t=632.885583494434, r=151.64172024877135, b=642.8182935492767, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=91, text='227', bbox=BoundingBox(l=154.46307025339985, t=633.0948034955892, r=169.4069702779156, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=92, text='×', bbox=BoundingBox(l=171.86607028194985, t=632.5368934925087, r=179.6149902946621, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=93, text='227', bbox=BoundingBox(l=182.07407029869626, t=633.0948034955892, r=197.017960323212, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=94, text='×', bbox=BoundingBox(l=199.47707032724625, t=632.5368934925087, r=207.22598033995848, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=95, text='64', bbox=BoundingBox(l=209.68506034399266, t=633.0948034955892, r=219.64766036033652, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=96, text='- which calculates to roughly 10 million activations - or an', bbox=BoundingBox(l=222.4690603649651, t=632.885583494434, r=480.590120788418, b=642.8182935492767, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=97, text='enormous 70 megabytes of memory per image. In this case you have two op-', bbox=BoundingBox(l=134.76505022108483, t=644.8405935604427, r=480.58679078841254, b=654.7733036152855, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=98, text='tions. Firstly, you can reduce the spatial dimensionality of the input images by', bbox=BoundingBox(l=134.76505022108483, t=656.7955936264514, r=480.58676078841245, b=666.7283036812942, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))], predictions=PagePredictions(layout=LayoutPrediction(clusters=[Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=255.96002197265625, t=93.55046844482422, r=447.56597900390625, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9279627799987793, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=474.8158874511719, t=93.79475402832031, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8714832067489624, cells=[Cell(id=1, text='9', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87208557128906, t=118.63634490966797, r=480.66082763671875, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9839420914649963, cells=[Cell(id=2, text='Another common CNN architecture is to stack two convolutional layers before', bbox=BoundingBox(l=134.76501022108476, t=119.67181066075966, r=480.58676078841245, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='each pooling layer, as illustrated in Figure 5. This is strongly recommended as', bbox=BoundingBox(l=134.76501022108476, t=131.6268307267684, r=480.58682078841264, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='stacking multiple convolutional layers allows for more complex features of the', bbox=BoundingBox(l=134.76501022108476, t=143.58184079277714, r=480.5868507884127, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='input vector to be selected.', bbox=BoundingBox(l=134.76501022108476, t=155.53686085878599, r=252.77200041467768, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=3, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=133.3032989501953, t=188.10928344726562, r=481.8245507904431, b=277.20819153058585, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9117528200149536, cells=[Cell(id=6, text='input', bbox=BoundingBox(l=141.0171202313415, t=254.84454140710648, r=152.0187102493898, b=260.8498514402644, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='convolution w/ ReLu', bbox=BoundingBox(l=175.69994028823936, t=190.0086610491196, r=221.95847036412746, b=196.01397108227752, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='pooling', bbox=BoundingBox(l=232.13184038081707, t=190.21557105026216, r=248.78818040814215, b=196.2333910834891, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='convolution', bbox=BoundingBox(l=350.42499057487936, t=264.6906114614709, r=376.18756061714345, b=270.6959214946288, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='w/ ReLu', bbox=BoundingBox(l=353.9647205806864, t=271.1903614973588, r=372.95895061184683, b=277.20819153058585, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='pooling', bbox=BoundingBox(l=380.33847062395307, t=189.10449104412737, r=396.9493106512036, b=195.10980107728528, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='fully-connected', bbox=BoundingBox(l=399.56500065549466, t=262.8054814510623, r=433.6465807114061, b=268.8107914842202, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='w/ ReLu', bbox=BoundingBox(l=407.1688206679688, t=269.30072148692534, r=426.1585706991219, b=275.3185415201523, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='fully-connected', bbox=BoundingBox(l=420.5636606899434, t=189.10449104412737, r=454.6497807458623, b=195.10980107728528, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='convolution w/ ReLu', bbox=BoundingBox(l=268.3968804403106, t=188.21289103920446, r=314.67807051623583, b=194.2307110724313, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='pooling', bbox=BoundingBox(l=324.8378305329031, t=188.4489710405079, r=341.4487005601536, b=194.45434107366623, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='0', bbox=BoundingBox(l=472.1900607746376, t=219.15814121006656, r=474.9025007790873, b=225.16345124322447, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='9', bbox=BoundingBox(l=472.1900607746376, t=243.45690134423035, r=474.9025007790873, b=249.46221137738826, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='output ', bbox=BoundingBox(l=466.57080076541905, t=253.0985113974658, r=481.8245507904431, b=259.11633143069287, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='...', bbox=BoundingBox(l=471.47577077346574, t=230.8848212748146, r=475.54437078014035, b=236.89013130797252, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=4, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=134.1597137451172, t=287.998046875, r=480.640380859375, b=322.92626953125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9685500264167786, cells=[Cell(id=21, text='Fig. 5: A common form of CNN architecture in which convolutional layers are', bbox=BoundingBox(l=134.76500022108476, t=288.8927615951013, r=480.58682078841264, b=298.825431649944, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='stacked between ReLus continuously before being passed through the pooling', bbox=BoundingBox(l=134.76500022108476, t=300.8477716611102, r=480.5867307884124, b=310.7804517159527, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='layer, before going between one or many fully connected ReLus.', bbox=BoundingBox(l=134.76500022108476, t=312.8027917271189, r=417.1848806844004, b=322.73547178196156, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.85641479492188, t=348.0907897949219, r=480.6982727050781, b=478.8450622558594, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9880061745643616, cells=[Cell(id=24, text='It is also advised to split large convolutional layers up into many smaller sized', bbox=BoundingBox(l=134.76500022108476, t=349.1437619277732, r=480.58682078841264, b=359.0764719826159, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='convolutional layers. This is to reduce the amount of computational complexity', bbox=BoundingBox(l=134.76500022108476, t=361.0987519937818, r=480.5867307884124, b=371.0314620486245, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='within a given convolutional layer. For example, if you were to stack three con-', bbox=BoundingBox(l=134.76500022108476, t=373.0537420597904, r=480.58679078841254, b=382.9864521146332, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='volutional layers on top of each other with a receptive field of', bbox=BoundingBox(l=134.76500022108476, t=385.00973212580453, r=400.48746065700794, b=394.9424421806474, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='3', bbox=BoundingBox(l=402.51202066032926, t=385.2189621269598, r=407.4933206685012, b=394.0657321758067, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='×', bbox=BoundingBox(l=407.9780306692964, t=384.6610421238793, r=415.7269306820086, b=394.0657321758067, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='3', bbox=BoundingBox(l=416.2110306828028, t=385.2189621269598, r=421.19232069097467, b=394.0657321758067, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='. Each neuron', bbox=BoundingBox(l=421.19202069097423, t=385.00973212580453, r=480.5890507884163, b=394.9424421806474, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='of the first convolutional layer will have a', bbox=BoundingBox(l=134.76501022108476, t=396.9647221918132, r=316.8913005198667, b=406.897432246656, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='3', bbox=BoundingBox(l=319.1640005235951, t=397.1739521929685, r=324.145290531767, b=406.02072224181535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='×', bbox=BoundingBox(l=325.55801053408464, t=396.6160221898879, r=333.30692054679685, b=406.02072224181535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='3', bbox=BoundingBox(l=334.72000054911507, t=397.1739521929685, r=339.70129055728694, b=406.02072224181535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='view of the input vector. A neu-', bbox=BoundingBox(l=341.97601056101865, t=396.9647221918132, r=480.59567078842707, b=406.897432246656, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='ron on the second convolutional layer will then have a', bbox=BoundingBox(l=134.76501022108476, t=408.9197022578219, r=376.1587806170962, b=418.8524123126646, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='5', bbox=BoundingBox(l=378.9180306216228, t=409.1289322589771, r=383.8993206297947, b=417.9757023078239, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='×', bbox=BoundingBox(l=386.3090206337479, t=408.5710122558965, r=394.05792064646016, b=417.9757023078239, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='5', bbox=BoundingBox(l=396.4670106504123, t=409.1289322589771, r=401.44830065858423, b=417.9757023078239, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='view of the input', bbox=BoundingBox(l=404.2030006631033, t=408.9197022578219, r=480.5961907884279, b=418.8524123126646, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='vector. A neuron on the third convolutional layer will then have a', bbox=BoundingBox(l=134.76501022108476, t=420.8746923238305, r=421.9369506921963, b=430.8074023786733, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='7', bbox=BoundingBox(l=424.30200069607616, t=421.0839223249858, r=429.2832907042481, b=429.9306923738326, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='×', bbox=BoundingBox(l=431.026000707107, t=420.5260023219052, r=438.7749007198193, b=429.9306923738326, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='7', bbox=BoundingBox(l=440.51700072267727, t=421.0839223249858, r=445.49829073084913, b=429.9306923738326, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='view of', bbox=BoundingBox(l=447.86200073472685, t=420.8746923238305, r=480.5891407884164, b=430.8074023786733, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='the input vector. As these stacks feature non-linearities which in turn allows us', bbox=BoundingBox(l=134.76501022108476, t=432.82968238983915, r=480.58679078841254, b=442.76239244468195, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='to express stronger features of the input with fewer parameters. However, it is', bbox=BoundingBox(l=134.76501022108476, t=444.78466245584775, r=480.58682078841264, b=454.7173725106905, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='important to understand that this does come with a distinct memory allocation', bbox=BoundingBox(l=134.76501022108476, t=456.74066252186196, r=480.58679078841254, b=466.67337257670476, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='problem - especially when making use of the backpropagation algorithm.', bbox=BoundingBox(l=134.76501022108476, t=468.69564258787057, r=457.4038107503804, b=478.62835264271337, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.77923583984375, t=486.6978454589844, r=480.58676078841245, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.966922402381897, cells=[Cell(id=51, text='The input layer should be recursively divisible by two. Common numbers in-', bbox=BoundingBox(l=134.76501022108476, t=487.5596626920269, r=480.58676078841245, b=497.4923727468697, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='clude', bbox=BoundingBox(l=134.76501022108476, t=499.51464275803556, r=158.95421026076767, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='32', bbox=BoundingBox(l=161.44402026485224, t=499.72387275919084, r=171.4066202811961, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='×', bbox=BoundingBox(l=173.62102028482886, t=499.16595275611024, r=181.36993029754112, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='32', bbox=BoundingBox(l=183.58302030117173, t=499.72387275919084, r=193.54562031751558, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text=',', bbox=BoundingBox(l=193.54602031751622, t=499.51464275803556, r=196.0366703216022, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='64', bbox=BoundingBox(l=198.52702032568766, t=499.72387275919084, r=208.48962034203151, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='×', bbox=BoundingBox(l=210.7040303456643, t=499.16595275611024, r=218.45294035837657, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='64', bbox=BoundingBox(l=220.66603036200718, t=499.72387275919084, r=230.628630378351, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text=',', bbox=BoundingBox(l=230.6290303783517, t=499.51464275803556, r=233.11967038243762, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='96', bbox=BoundingBox(l=235.6100303865231, t=499.72387275919084, r=245.572630402867, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='×', bbox=BoundingBox(l=247.7870304064997, t=499.16595275611024, r=255.535950419212, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='96', bbox=BoundingBox(l=257.7500304228442, t=499.72387275919084, r=267.7126204391881, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text=',', bbox=BoundingBox(l=267.7120404391871, t=499.51464275803556, r=270.20270044327305, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=65, text='128', bbox=BoundingBox(l=272.69403044736015, t=499.72387275919084, r=287.637940471876, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='×', bbox=BoundingBox(l=289.85104047550664, t=499.16595275611024, r=297.5999504882188, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='128', bbox=BoundingBox(l=299.81406049185114, t=499.72387275919084, r=314.7579705163669, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=68, text='and', bbox=BoundingBox(l=317.2490505204536, t=499.51464275803556, r=334.1157205481237, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=69, text='224', bbox=BoundingBox(l=336.6060505522091, t=499.72387275919084, r=351.54996057672497, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=70, text='×', bbox=BoundingBox(l=353.76404058035723, t=499.16595275611024, r=361.51294059306946, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=71, text='224', bbox=BoundingBox(l=363.72705059670176, t=499.72387275919084, r=378.6709606212175, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=72, text='.', bbox=BoundingBox(l=378.67105062121766, t=499.51464275803556, r=381.16171062530367, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.83213806152344, t=517.3685302734375, r=480.58688078841266, b=564.4635009765625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.982406735420227, cells=[Cell(id=73, text='Whilst using small filters, set stride to one and make use of zero-padding as to', bbox=BoundingBox(l=134.76505022108483, t=518.3796328621972, r=480.5867307884124, b=528.31234291704, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=74, text='ensure that the convolutional layers do not reconfigure any of the dimension-', bbox=BoundingBox(l=134.76505022108483, t=530.3346229282059, r=480.58682078841264, b=540.2673329830487, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=75, text='ality of the input. The amount of zero-padding to be used should be calculated', bbox=BoundingBox(l=134.76505022108483, t=542.2896129942146, r=480.5868507884127, b=552.2223230490574, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=76, text='by taking one away from the receptive field size and dividing by two.activation', bbox=BoundingBox(l=134.76505022108483, t=554.2445930602232, r=480.58688078841266, b=564.177303115066, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.7228240966797, t=571.9942626953125, r=480.7103271484375, b=667.2128295898438, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9852842688560486, cells=[Cell(id=77, text='CNNs are extremely powerful machine learning algorithms, however they can', bbox=BoundingBox(l=134.76505022108483, t=573.109583164385, r=480.5868507884127, b=583.0422932192278, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=78, text='be horrendously resource-heavy. An example of this problem could be in filter-', bbox=BoundingBox(l=134.76505022108483, t=585.0646032303937, r=480.58698078841286, b=594.9972932852364, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=79, text='ing a large image (anything over', bbox=BoundingBox(l=134.76505022108483, t=597.0195932964024, r=277.987370456044, b=606.9523033512452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=80, text='128', bbox=BoundingBox(l=280.46405046010705, t=597.2288032975575, r=295.40796048462283, b=606.0755933464045, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=81, text='×', bbox=BoundingBox(l=297.56506048816163, t=596.6708932944771, r=305.31396050087386, b=606.0755933464045, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=82, text='128', bbox=BoundingBox(l=307.470060504411, t=597.2288032975575, r=322.41397052892677, b=606.0755933464045, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=83, text='could be considered large), so if the', bbox=BoundingBox(l=324.8900805329889, t=597.0195932964024, r=480.59555078842686, b=606.9523033512452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=84, text='input is', bbox=BoundingBox(l=134.7650802210849, t=608.974593362411, r=168.67776027671934, b=618.9073034172538, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=85, text='227', bbox=BoundingBox(l=171.53407028140518, t=609.1838033635662, r=186.47797030592096, b=618.0305934124132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=86, text='×', bbox=BoundingBox(l=188.9600703099929, t=608.6259033604858, r=196.70898032270514, b=618.0305934124132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=87, text='227', bbox=BoundingBox(l=199.19107032677704, t=609.1838033635662, r=214.13496035129282, b=618.0305934124132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=88, text='(as seen with ImageNet) and we’re filtering with 64 kernels', bbox=BoundingBox(l=216.98807035597338, t=608.974593362411, r=480.5884707884153, b=618.9073034172538, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=89, text='each with a zero padding of then the result will be three activation vectors of', bbox=BoundingBox(l=134.7650802210849, t=620.9295934284198, r=480.5868507884127, b=630.8623034832626, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=90, text='size', bbox=BoundingBox(l=134.7650802210849, t=632.885583494434, r=151.64172024877135, b=642.8182935492767, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=91, text='227', bbox=BoundingBox(l=154.46307025339985, t=633.0948034955892, r=169.4069702779156, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=92, text='×', bbox=BoundingBox(l=171.86607028194985, t=632.5368934925087, r=179.6149902946621, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=93, text='227', bbox=BoundingBox(l=182.07407029869626, t=633.0948034955892, r=197.017960323212, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=94, text='×', bbox=BoundingBox(l=199.47707032724625, t=632.5368934925087, r=207.22598033995848, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=95, text='64', bbox=BoundingBox(l=209.68506034399266, t=633.0948034955892, r=219.64766036033652, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=96, text='- which calculates to roughly 10 million activations - or an', bbox=BoundingBox(l=222.4690603649651, t=632.885583494434, r=480.590120788418, b=642.8182935492767, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=97, text='enormous 70 megabytes of memory per image. In this case you have two op-', bbox=BoundingBox(l=134.76505022108483, t=644.8405935604427, r=480.58679078841254, b=654.7733036152855, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=98, text='tions. Firstly, you can reduce the spatial dimensionality of the input images by', bbox=BoundingBox(l=134.76505022108483, t=656.7955936264514, r=480.58676078841245, b=666.7283036812942, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))])]), tablestructure=TableStructurePrediction(table_map={}), figures_classification=None, equations_prediction=None), assembled=AssembledUnit(elements=[TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=8, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=255.96002197265625, t=93.55046844482422, r=447.56597900390625, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9279627799987793, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=8, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=474.8158874511719, t=93.79475402832031, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8714832067489624, cells=[Cell(id=1, text='9', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='9'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=8, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87208557128906, t=118.63634490966797, r=480.66082763671875, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9839420914649963, cells=[Cell(id=2, text='Another common CNN architecture is to stack two convolutional layers before', bbox=BoundingBox(l=134.76501022108476, t=119.67181066075966, r=480.58676078841245, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='each pooling layer, as illustrated in Figure 5. This is strongly recommended as', bbox=BoundingBox(l=134.76501022108476, t=131.6268307267684, r=480.58682078841264, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='stacking multiple convolutional layers allows for more complex features of the', bbox=BoundingBox(l=134.76501022108476, t=143.58184079277714, r=480.5868507884127, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='input vector to be selected.', bbox=BoundingBox(l=134.76501022108476, t=155.53686085878599, r=252.77200041467768, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Another common CNN architecture is to stack two convolutional layers before each pooling layer, as illustrated in Figure 5. This is strongly recommended as stacking multiple convolutional layers allows for more complex features of the input vector to be selected.'), FigureElement(label=<DocItemLabel.PICTURE: 'picture'>, id=3, page_no=8, cluster=Cluster(id=3, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=133.3032989501953, t=188.10928344726562, r=481.8245507904431, b=277.20819153058585, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9117528200149536, cells=[Cell(id=6, text='input', bbox=BoundingBox(l=141.0171202313415, t=254.84454140710648, r=152.0187102493898, b=260.8498514402644, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='convolution w/ ReLu', bbox=BoundingBox(l=175.69994028823936, t=190.0086610491196, r=221.95847036412746, b=196.01397108227752, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='pooling', bbox=BoundingBox(l=232.13184038081707, t=190.21557105026216, r=248.78818040814215, b=196.2333910834891, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='convolution', bbox=BoundingBox(l=350.42499057487936, t=264.6906114614709, r=376.18756061714345, b=270.6959214946288, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='w/ ReLu', bbox=BoundingBox(l=353.9647205806864, t=271.1903614973588, r=372.95895061184683, b=277.20819153058585, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='pooling', bbox=BoundingBox(l=380.33847062395307, t=189.10449104412737, r=396.9493106512036, b=195.10980107728528, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='fully-connected', bbox=BoundingBox(l=399.56500065549466, t=262.8054814510623, r=433.6465807114061, b=268.8107914842202, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='w/ ReLu', bbox=BoundingBox(l=407.1688206679688, t=269.30072148692534, r=426.1585706991219, b=275.3185415201523, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='fully-connected', bbox=BoundingBox(l=420.5636606899434, t=189.10449104412737, r=454.6497807458623, b=195.10980107728528, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='convolution w/ ReLu', bbox=BoundingBox(l=268.3968804403106, t=188.21289103920446, r=314.67807051623583, b=194.2307110724313, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='pooling', bbox=BoundingBox(l=324.8378305329031, t=188.4489710405079, r=341.4487005601536, b=194.45434107366623, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='0', bbox=BoundingBox(l=472.1900607746376, t=219.15814121006656, r=474.9025007790873, b=225.16345124322447, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='9', bbox=BoundingBox(l=472.1900607746376, t=243.45690134423035, r=474.9025007790873, b=249.46221137738826, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='output ', bbox=BoundingBox(l=466.57080076541905, t=253.0985113974658, r=481.8245507904431, b=259.11633143069287, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='...', bbox=BoundingBox(l=471.47577077346574, t=230.8848212748146, r=475.54437078014035, b=236.89013130797252, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='', annotations=[], provenance=None, predicted_class=None, confidence=None), TextElement(label=<DocItemLabel.CAPTION: 'caption'>, id=4, page_no=8, cluster=Cluster(id=4, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=134.1597137451172, t=287.998046875, r=480.640380859375, b=322.92626953125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9685500264167786, cells=[Cell(id=21, text='Fig. 5: A common form of CNN architecture in which convolutional layers are', bbox=BoundingBox(l=134.76500022108476, t=288.8927615951013, r=480.58682078841264, b=298.825431649944, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='stacked between ReLus continuously before being passed through the pooling', bbox=BoundingBox(l=134.76500022108476, t=300.8477716611102, r=480.5867307884124, b=310.7804517159527, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='layer, before going between one or many fully connected ReLus.', bbox=BoundingBox(l=134.76500022108476, t=312.8027917271189, r=417.1848806844004, b=322.73547178196156, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Fig. 5: A common form of CNN architecture in which convolutional layers are stacked between ReLus continuously before being passed through the pooling layer, before going between one or many fully connected ReLus.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=8, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.85641479492188, t=348.0907897949219, r=480.6982727050781, b=478.8450622558594, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9880061745643616, cells=[Cell(id=24, text='It is also advised to split large convolutional layers up into many smaller sized', bbox=BoundingBox(l=134.76500022108476, t=349.1437619277732, r=480.58682078841264, b=359.0764719826159, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='convolutional layers. This is to reduce the amount of computational complexity', bbox=BoundingBox(l=134.76500022108476, t=361.0987519937818, r=480.5867307884124, b=371.0314620486245, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='within a given convolutional layer. For example, if you were to stack three con-', bbox=BoundingBox(l=134.76500022108476, t=373.0537420597904, r=480.58679078841254, b=382.9864521146332, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='volutional layers on top of each other with a receptive field of', bbox=BoundingBox(l=134.76500022108476, t=385.00973212580453, r=400.48746065700794, b=394.9424421806474, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='3', bbox=BoundingBox(l=402.51202066032926, t=385.2189621269598, r=407.4933206685012, b=394.0657321758067, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='×', bbox=BoundingBox(l=407.9780306692964, t=384.6610421238793, r=415.7269306820086, b=394.0657321758067, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='3', bbox=BoundingBox(l=416.2110306828028, t=385.2189621269598, r=421.19232069097467, b=394.0657321758067, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='. Each neuron', bbox=BoundingBox(l=421.19202069097423, t=385.00973212580453, r=480.5890507884163, b=394.9424421806474, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='of the first convolutional layer will have a', bbox=BoundingBox(l=134.76501022108476, t=396.9647221918132, r=316.8913005198667, b=406.897432246656, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='3', bbox=BoundingBox(l=319.1640005235951, t=397.1739521929685, r=324.145290531767, b=406.02072224181535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='×', bbox=BoundingBox(l=325.55801053408464, t=396.6160221898879, r=333.30692054679685, b=406.02072224181535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='3', bbox=BoundingBox(l=334.72000054911507, t=397.1739521929685, r=339.70129055728694, b=406.02072224181535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='view of the input vector. A neu-', bbox=BoundingBox(l=341.97601056101865, t=396.9647221918132, r=480.59567078842707, b=406.897432246656, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='ron on the second convolutional layer will then have a', bbox=BoundingBox(l=134.76501022108476, t=408.9197022578219, r=376.1587806170962, b=418.8524123126646, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='5', bbox=BoundingBox(l=378.9180306216228, t=409.1289322589771, r=383.8993206297947, b=417.9757023078239, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='×', bbox=BoundingBox(l=386.3090206337479, t=408.5710122558965, r=394.05792064646016, b=417.9757023078239, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='5', bbox=BoundingBox(l=396.4670106504123, t=409.1289322589771, r=401.44830065858423, b=417.9757023078239, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='view of the input', bbox=BoundingBox(l=404.2030006631033, t=408.9197022578219, r=480.5961907884279, b=418.8524123126646, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='vector. A neuron on the third convolutional layer will then have a', bbox=BoundingBox(l=134.76501022108476, t=420.8746923238305, r=421.9369506921963, b=430.8074023786733, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='7', bbox=BoundingBox(l=424.30200069607616, t=421.0839223249858, r=429.2832907042481, b=429.9306923738326, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='×', bbox=BoundingBox(l=431.026000707107, t=420.5260023219052, r=438.7749007198193, b=429.9306923738326, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='7', bbox=BoundingBox(l=440.51700072267727, t=421.0839223249858, r=445.49829073084913, b=429.9306923738326, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='view of', bbox=BoundingBox(l=447.86200073472685, t=420.8746923238305, r=480.5891407884164, b=430.8074023786733, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='the input vector. As these stacks feature non-linearities which in turn allows us', bbox=BoundingBox(l=134.76501022108476, t=432.82968238983915, r=480.58679078841254, b=442.76239244468195, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='to express stronger features of the input with fewer parameters. However, it is', bbox=BoundingBox(l=134.76501022108476, t=444.78466245584775, r=480.58682078841264, b=454.7173725106905, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='important to understand that this does come with a distinct memory allocation', bbox=BoundingBox(l=134.76501022108476, t=456.74066252186196, r=480.58679078841254, b=466.67337257670476, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='problem - especially when making use of the backpropagation algorithm.', bbox=BoundingBox(l=134.76501022108476, t=468.69564258787057, r=457.4038107503804, b=478.62835264271337, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='It is also advised to split large convolutional layers up into many smaller sized convolutional layers. This is to reduce the amount of computational complexity within a given convolutional layer. For example, if you were to stack three convolutional layers on top of each other with a receptive field of 3 × 3 . Each neuron of the first convolutional layer will have a 3 × 3 view of the input vector. A neuron on the second convolutional layer will then have a 5 × 5 view of the input vector. A neuron on the third convolutional layer will then have a 7 × 7 view of the input vector. As these stacks feature non-linearities which in turn allows us to express stronger features of the input with fewer parameters. However, it is important to understand that this does come with a distinct memory allocation problem - especially when making use of the backpropagation algorithm.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=8, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.77923583984375, t=486.6978454589844, r=480.58676078841245, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.966922402381897, cells=[Cell(id=51, text='The input layer should be recursively divisible by two. Common numbers in-', bbox=BoundingBox(l=134.76501022108476, t=487.5596626920269, r=480.58676078841245, b=497.4923727468697, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='clude', bbox=BoundingBox(l=134.76501022108476, t=499.51464275803556, r=158.95421026076767, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='32', bbox=BoundingBox(l=161.44402026485224, t=499.72387275919084, r=171.4066202811961, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='×', bbox=BoundingBox(l=173.62102028482886, t=499.16595275611024, r=181.36993029754112, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='32', bbox=BoundingBox(l=183.58302030117173, t=499.72387275919084, r=193.54562031751558, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text=',', bbox=BoundingBox(l=193.54602031751622, t=499.51464275803556, r=196.0366703216022, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='64', bbox=BoundingBox(l=198.52702032568766, t=499.72387275919084, r=208.48962034203151, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='×', bbox=BoundingBox(l=210.7040303456643, t=499.16595275611024, r=218.45294035837657, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='64', bbox=BoundingBox(l=220.66603036200718, t=499.72387275919084, r=230.628630378351, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text=',', bbox=BoundingBox(l=230.6290303783517, t=499.51464275803556, r=233.11967038243762, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='96', bbox=BoundingBox(l=235.6100303865231, t=499.72387275919084, r=245.572630402867, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='×', bbox=BoundingBox(l=247.7870304064997, t=499.16595275611024, r=255.535950419212, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='96', bbox=BoundingBox(l=257.7500304228442, t=499.72387275919084, r=267.7126204391881, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text=',', bbox=BoundingBox(l=267.7120404391871, t=499.51464275803556, r=270.20270044327305, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=65, text='128', bbox=BoundingBox(l=272.69403044736015, t=499.72387275919084, r=287.637940471876, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='×', bbox=BoundingBox(l=289.85104047550664, t=499.16595275611024, r=297.5999504882188, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='128', bbox=BoundingBox(l=299.81406049185114, t=499.72387275919084, r=314.7579705163669, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=68, text='and', bbox=BoundingBox(l=317.2490505204536, t=499.51464275803556, r=334.1157205481237, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=69, text='224', bbox=BoundingBox(l=336.6060505522091, t=499.72387275919084, r=351.54996057672497, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=70, text='×', bbox=BoundingBox(l=353.76404058035723, t=499.16595275611024, r=361.51294059306946, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=71, text='224', bbox=BoundingBox(l=363.72705059670176, t=499.72387275919084, r=378.6709606212175, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=72, text='.', bbox=BoundingBox(l=378.67105062121766, t=499.51464275803556, r=381.16171062530367, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The input layer should be recursively divisible by two. Common numbers include 32 × 32 , 64 × 64 , 96 × 96 , 128 × 128 and 224 × 224 .'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=8, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.83213806152344, t=517.3685302734375, r=480.58688078841266, b=564.4635009765625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.982406735420227, cells=[Cell(id=73, text='Whilst using small filters, set stride to one and make use of zero-padding as to', bbox=BoundingBox(l=134.76505022108483, t=518.3796328621972, r=480.5867307884124, b=528.31234291704, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=74, text='ensure that the convolutional layers do not reconfigure any of the dimension-', bbox=BoundingBox(l=134.76505022108483, t=530.3346229282059, r=480.58682078841264, b=540.2673329830487, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=75, text='ality of the input. The amount of zero-padding to be used should be calculated', bbox=BoundingBox(l=134.76505022108483, t=542.2896129942146, r=480.5868507884127, b=552.2223230490574, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=76, text='by taking one away from the receptive field size and dividing by two.activation', bbox=BoundingBox(l=134.76505022108483, t=554.2445930602232, r=480.58688078841266, b=564.177303115066, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Whilst using small filters, set stride to one and make use of zero-padding as to ensure that the convolutional layers do not reconfigure any of the dimensionality of the input. The amount of zero-padding to be used should be calculated by taking one away from the receptive field size and dividing by two.activation'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=8, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.7228240966797, t=571.9942626953125, r=480.7103271484375, b=667.2128295898438, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9852842688560486, cells=[Cell(id=77, text='CNNs are extremely powerful machine learning algorithms, however they can', bbox=BoundingBox(l=134.76505022108483, t=573.109583164385, r=480.5868507884127, b=583.0422932192278, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=78, text='be horrendously resource-heavy. An example of this problem could be in filter-', bbox=BoundingBox(l=134.76505022108483, t=585.0646032303937, r=480.58698078841286, b=594.9972932852364, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=79, text='ing a large image (anything over', bbox=BoundingBox(l=134.76505022108483, t=597.0195932964024, r=277.987370456044, b=606.9523033512452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=80, text='128', bbox=BoundingBox(l=280.46405046010705, t=597.2288032975575, r=295.40796048462283, b=606.0755933464045, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=81, text='×', bbox=BoundingBox(l=297.56506048816163, t=596.6708932944771, r=305.31396050087386, b=606.0755933464045, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=82, text='128', bbox=BoundingBox(l=307.470060504411, t=597.2288032975575, r=322.41397052892677, b=606.0755933464045, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=83, text='could be considered large), so if the', bbox=BoundingBox(l=324.8900805329889, t=597.0195932964024, r=480.59555078842686, b=606.9523033512452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=84, text='input is', bbox=BoundingBox(l=134.7650802210849, t=608.974593362411, r=168.67776027671934, b=618.9073034172538, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=85, text='227', bbox=BoundingBox(l=171.53407028140518, t=609.1838033635662, r=186.47797030592096, b=618.0305934124132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=86, text='×', bbox=BoundingBox(l=188.9600703099929, t=608.6259033604858, r=196.70898032270514, b=618.0305934124132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=87, text='227', bbox=BoundingBox(l=199.19107032677704, t=609.1838033635662, r=214.13496035129282, b=618.0305934124132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=88, text='(as seen with ImageNet) and we’re filtering with 64 kernels', bbox=BoundingBox(l=216.98807035597338, t=608.974593362411, r=480.5884707884153, b=618.9073034172538, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=89, text='each with a zero padding of then the result will be three activation vectors of', bbox=BoundingBox(l=134.7650802210849, t=620.9295934284198, r=480.5868507884127, b=630.8623034832626, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=90, text='size', bbox=BoundingBox(l=134.7650802210849, t=632.885583494434, r=151.64172024877135, b=642.8182935492767, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=91, text='227', bbox=BoundingBox(l=154.46307025339985, t=633.0948034955892, r=169.4069702779156, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=92, text='×', bbox=BoundingBox(l=171.86607028194985, t=632.5368934925087, r=179.6149902946621, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=93, text='227', bbox=BoundingBox(l=182.07407029869626, t=633.0948034955892, r=197.017960323212, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=94, text='×', bbox=BoundingBox(l=199.47707032724625, t=632.5368934925087, r=207.22598033995848, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=95, text='64', bbox=BoundingBox(l=209.68506034399266, t=633.0948034955892, r=219.64766036033652, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=96, text='- which calculates to roughly 10 million activations - or an', bbox=BoundingBox(l=222.4690603649651, t=632.885583494434, r=480.590120788418, b=642.8182935492767, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=97, text='enormous 70 megabytes of memory per image. In this case you have two op-', bbox=BoundingBox(l=134.76505022108483, t=644.8405935604427, r=480.58679078841254, b=654.7733036152855, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=98, text='tions. Firstly, you can reduce the spatial dimensionality of the input images by', bbox=BoundingBox(l=134.76505022108483, t=656.7955936264514, r=480.58676078841245, b=666.7283036812942, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='CNNs are extremely powerful machine learning algorithms, however they can be horrendously resource-heavy. An example of this problem could be in filtering a large image (anything over 128 × 128 could be considered large), so if the input is 227 × 227 (as seen with ImageNet) and we’re filtering with 64 kernels each with a zero padding of then the result will be three activation vectors of size 227 × 227 × 64 - which calculates to roughly 10 million activations - or an enormous 70 megabytes of memory per image. In this case you have two options. Firstly, you can reduce the spatial dimensionality of the input images by')], body=[TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=8, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87208557128906, t=118.63634490966797, r=480.66082763671875, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9839420914649963, cells=[Cell(id=2, text='Another common CNN architecture is to stack two convolutional layers before', bbox=BoundingBox(l=134.76501022108476, t=119.67181066075966, r=480.58676078841245, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='each pooling layer, as illustrated in Figure 5. This is strongly recommended as', bbox=BoundingBox(l=134.76501022108476, t=131.6268307267684, r=480.58682078841264, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='stacking multiple convolutional layers allows for more complex features of the', bbox=BoundingBox(l=134.76501022108476, t=143.58184079277714, r=480.5868507884127, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='input vector to be selected.', bbox=BoundingBox(l=134.76501022108476, t=155.53686085878599, r=252.77200041467768, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Another common CNN architecture is to stack two convolutional layers before each pooling layer, as illustrated in Figure 5. This is strongly recommended as stacking multiple convolutional layers allows for more complex features of the input vector to be selected.'), FigureElement(label=<DocItemLabel.PICTURE: 'picture'>, id=3, page_no=8, cluster=Cluster(id=3, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=133.3032989501953, t=188.10928344726562, r=481.8245507904431, b=277.20819153058585, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9117528200149536, cells=[Cell(id=6, text='input', bbox=BoundingBox(l=141.0171202313415, t=254.84454140710648, r=152.0187102493898, b=260.8498514402644, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='convolution w/ ReLu', bbox=BoundingBox(l=175.69994028823936, t=190.0086610491196, r=221.95847036412746, b=196.01397108227752, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='pooling', bbox=BoundingBox(l=232.13184038081707, t=190.21557105026216, r=248.78818040814215, b=196.2333910834891, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='convolution', bbox=BoundingBox(l=350.42499057487936, t=264.6906114614709, r=376.18756061714345, b=270.6959214946288, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='w/ ReLu', bbox=BoundingBox(l=353.9647205806864, t=271.1903614973588, r=372.95895061184683, b=277.20819153058585, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='pooling', bbox=BoundingBox(l=380.33847062395307, t=189.10449104412737, r=396.9493106512036, b=195.10980107728528, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='fully-connected', bbox=BoundingBox(l=399.56500065549466, t=262.8054814510623, r=433.6465807114061, b=268.8107914842202, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='w/ ReLu', bbox=BoundingBox(l=407.1688206679688, t=269.30072148692534, r=426.1585706991219, b=275.3185415201523, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='fully-connected', bbox=BoundingBox(l=420.5636606899434, t=189.10449104412737, r=454.6497807458623, b=195.10980107728528, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='convolution w/ ReLu', bbox=BoundingBox(l=268.3968804403106, t=188.21289103920446, r=314.67807051623583, b=194.2307110724313, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='pooling', bbox=BoundingBox(l=324.8378305329031, t=188.4489710405079, r=341.4487005601536, b=194.45434107366623, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='0', bbox=BoundingBox(l=472.1900607746376, t=219.15814121006656, r=474.9025007790873, b=225.16345124322447, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='9', bbox=BoundingBox(l=472.1900607746376, t=243.45690134423035, r=474.9025007790873, b=249.46221137738826, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='output ', bbox=BoundingBox(l=466.57080076541905, t=253.0985113974658, r=481.8245507904431, b=259.11633143069287, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='...', bbox=BoundingBox(l=471.47577077346574, t=230.8848212748146, r=475.54437078014035, b=236.89013130797252, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='', annotations=[], provenance=None, predicted_class=None, confidence=None), TextElement(label=<DocItemLabel.CAPTION: 'caption'>, id=4, page_no=8, cluster=Cluster(id=4, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=134.1597137451172, t=287.998046875, r=480.640380859375, b=322.92626953125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9685500264167786, cells=[Cell(id=21, text='Fig. 5: A common form of CNN architecture in which convolutional layers are', bbox=BoundingBox(l=134.76500022108476, t=288.8927615951013, r=480.58682078841264, b=298.825431649944, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='stacked between ReLus continuously before being passed through the pooling', bbox=BoundingBox(l=134.76500022108476, t=300.8477716611102, r=480.5867307884124, b=310.7804517159527, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='layer, before going between one or many fully connected ReLus.', bbox=BoundingBox(l=134.76500022108476, t=312.8027917271189, r=417.1848806844004, b=322.73547178196156, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Fig. 5: A common form of CNN architecture in which convolutional layers are stacked between ReLus continuously before being passed through the pooling layer, before going between one or many fully connected ReLus.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=8, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.85641479492188, t=348.0907897949219, r=480.6982727050781, b=478.8450622558594, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9880061745643616, cells=[Cell(id=24, text='It is also advised to split large convolutional layers up into many smaller sized', bbox=BoundingBox(l=134.76500022108476, t=349.1437619277732, r=480.58682078841264, b=359.0764719826159, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='convolutional layers. This is to reduce the amount of computational complexity', bbox=BoundingBox(l=134.76500022108476, t=361.0987519937818, r=480.5867307884124, b=371.0314620486245, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='within a given convolutional layer. For example, if you were to stack three con-', bbox=BoundingBox(l=134.76500022108476, t=373.0537420597904, r=480.58679078841254, b=382.9864521146332, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='volutional layers on top of each other with a receptive field of', bbox=BoundingBox(l=134.76500022108476, t=385.00973212580453, r=400.48746065700794, b=394.9424421806474, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='3', bbox=BoundingBox(l=402.51202066032926, t=385.2189621269598, r=407.4933206685012, b=394.0657321758067, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='×', bbox=BoundingBox(l=407.9780306692964, t=384.6610421238793, r=415.7269306820086, b=394.0657321758067, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='3', bbox=BoundingBox(l=416.2110306828028, t=385.2189621269598, r=421.19232069097467, b=394.0657321758067, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='. Each neuron', bbox=BoundingBox(l=421.19202069097423, t=385.00973212580453, r=480.5890507884163, b=394.9424421806474, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='of the first convolutional layer will have a', bbox=BoundingBox(l=134.76501022108476, t=396.9647221918132, r=316.8913005198667, b=406.897432246656, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='3', bbox=BoundingBox(l=319.1640005235951, t=397.1739521929685, r=324.145290531767, b=406.02072224181535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='×', bbox=BoundingBox(l=325.55801053408464, t=396.6160221898879, r=333.30692054679685, b=406.02072224181535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='3', bbox=BoundingBox(l=334.72000054911507, t=397.1739521929685, r=339.70129055728694, b=406.02072224181535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='view of the input vector. A neu-', bbox=BoundingBox(l=341.97601056101865, t=396.9647221918132, r=480.59567078842707, b=406.897432246656, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='ron on the second convolutional layer will then have a', bbox=BoundingBox(l=134.76501022108476, t=408.9197022578219, r=376.1587806170962, b=418.8524123126646, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='5', bbox=BoundingBox(l=378.9180306216228, t=409.1289322589771, r=383.8993206297947, b=417.9757023078239, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='×', bbox=BoundingBox(l=386.3090206337479, t=408.5710122558965, r=394.05792064646016, b=417.9757023078239, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='5', bbox=BoundingBox(l=396.4670106504123, t=409.1289322589771, r=401.44830065858423, b=417.9757023078239, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='view of the input', bbox=BoundingBox(l=404.2030006631033, t=408.9197022578219, r=480.5961907884279, b=418.8524123126646, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='vector. A neuron on the third convolutional layer will then have a', bbox=BoundingBox(l=134.76501022108476, t=420.8746923238305, r=421.9369506921963, b=430.8074023786733, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='7', bbox=BoundingBox(l=424.30200069607616, t=421.0839223249858, r=429.2832907042481, b=429.9306923738326, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='×', bbox=BoundingBox(l=431.026000707107, t=420.5260023219052, r=438.7749007198193, b=429.9306923738326, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='7', bbox=BoundingBox(l=440.51700072267727, t=421.0839223249858, r=445.49829073084913, b=429.9306923738326, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='view of', bbox=BoundingBox(l=447.86200073472685, t=420.8746923238305, r=480.5891407884164, b=430.8074023786733, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='the input vector. As these stacks feature non-linearities which in turn allows us', bbox=BoundingBox(l=134.76501022108476, t=432.82968238983915, r=480.58679078841254, b=442.76239244468195, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='to express stronger features of the input with fewer parameters. However, it is', bbox=BoundingBox(l=134.76501022108476, t=444.78466245584775, r=480.58682078841264, b=454.7173725106905, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='important to understand that this does come with a distinct memory allocation', bbox=BoundingBox(l=134.76501022108476, t=456.74066252186196, r=480.58679078841254, b=466.67337257670476, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='problem - especially when making use of the backpropagation algorithm.', bbox=BoundingBox(l=134.76501022108476, t=468.69564258787057, r=457.4038107503804, b=478.62835264271337, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='It is also advised to split large convolutional layers up into many smaller sized convolutional layers. This is to reduce the amount of computational complexity within a given convolutional layer. For example, if you were to stack three convolutional layers on top of each other with a receptive field of 3 × 3 . Each neuron of the first convolutional layer will have a 3 × 3 view of the input vector. A neuron on the second convolutional layer will then have a 5 × 5 view of the input vector. A neuron on the third convolutional layer will then have a 7 × 7 view of the input vector. As these stacks feature non-linearities which in turn allows us to express stronger features of the input with fewer parameters. However, it is important to understand that this does come with a distinct memory allocation problem - especially when making use of the backpropagation algorithm.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=8, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.77923583984375, t=486.6978454589844, r=480.58676078841245, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.966922402381897, cells=[Cell(id=51, text='The input layer should be recursively divisible by two. Common numbers in-', bbox=BoundingBox(l=134.76501022108476, t=487.5596626920269, r=480.58676078841245, b=497.4923727468697, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='clude', bbox=BoundingBox(l=134.76501022108476, t=499.51464275803556, r=158.95421026076767, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='32', bbox=BoundingBox(l=161.44402026485224, t=499.72387275919084, r=171.4066202811961, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='×', bbox=BoundingBox(l=173.62102028482886, t=499.16595275611024, r=181.36993029754112, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='32', bbox=BoundingBox(l=183.58302030117173, t=499.72387275919084, r=193.54562031751558, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text=',', bbox=BoundingBox(l=193.54602031751622, t=499.51464275803556, r=196.0366703216022, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='64', bbox=BoundingBox(l=198.52702032568766, t=499.72387275919084, r=208.48962034203151, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='×', bbox=BoundingBox(l=210.7040303456643, t=499.16595275611024, r=218.45294035837657, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='64', bbox=BoundingBox(l=220.66603036200718, t=499.72387275919084, r=230.628630378351, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text=',', bbox=BoundingBox(l=230.6290303783517, t=499.51464275803556, r=233.11967038243762, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='96', bbox=BoundingBox(l=235.6100303865231, t=499.72387275919084, r=245.572630402867, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='×', bbox=BoundingBox(l=247.7870304064997, t=499.16595275611024, r=255.535950419212, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='96', bbox=BoundingBox(l=257.7500304228442, t=499.72387275919084, r=267.7126204391881, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text=',', bbox=BoundingBox(l=267.7120404391871, t=499.51464275803556, r=270.20270044327305, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=65, text='128', bbox=BoundingBox(l=272.69403044736015, t=499.72387275919084, r=287.637940471876, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='×', bbox=BoundingBox(l=289.85104047550664, t=499.16595275611024, r=297.5999504882188, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='128', bbox=BoundingBox(l=299.81406049185114, t=499.72387275919084, r=314.7579705163669, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=68, text='and', bbox=BoundingBox(l=317.2490505204536, t=499.51464275803556, r=334.1157205481237, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=69, text='224', bbox=BoundingBox(l=336.6060505522091, t=499.72387275919084, r=351.54996057672497, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=70, text='×', bbox=BoundingBox(l=353.76404058035723, t=499.16595275611024, r=361.51294059306946, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=71, text='224', bbox=BoundingBox(l=363.72705059670176, t=499.72387275919084, r=378.6709606212175, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=72, text='.', bbox=BoundingBox(l=378.67105062121766, t=499.51464275803556, r=381.16171062530367, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The input layer should be recursively divisible by two. Common numbers include 32 × 32 , 64 × 64 , 96 × 96 , 128 × 128 and 224 × 224 .'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=8, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.83213806152344, t=517.3685302734375, r=480.58688078841266, b=564.4635009765625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.982406735420227, cells=[Cell(id=73, text='Whilst using small filters, set stride to one and make use of zero-padding as to', bbox=BoundingBox(l=134.76505022108483, t=518.3796328621972, r=480.5867307884124, b=528.31234291704, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=74, text='ensure that the convolutional layers do not reconfigure any of the dimension-', bbox=BoundingBox(l=134.76505022108483, t=530.3346229282059, r=480.58682078841264, b=540.2673329830487, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=75, text='ality of the input. The amount of zero-padding to be used should be calculated', bbox=BoundingBox(l=134.76505022108483, t=542.2896129942146, r=480.5868507884127, b=552.2223230490574, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=76, text='by taking one away from the receptive field size and dividing by two.activation', bbox=BoundingBox(l=134.76505022108483, t=554.2445930602232, r=480.58688078841266, b=564.177303115066, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Whilst using small filters, set stride to one and make use of zero-padding as to ensure that the convolutional layers do not reconfigure any of the dimensionality of the input. The amount of zero-padding to be used should be calculated by taking one away from the receptive field size and dividing by two.activation'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=8, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.7228240966797, t=571.9942626953125, r=480.7103271484375, b=667.2128295898438, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9852842688560486, cells=[Cell(id=77, text='CNNs are extremely powerful machine learning algorithms, however they can', bbox=BoundingBox(l=134.76505022108483, t=573.109583164385, r=480.5868507884127, b=583.0422932192278, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=78, text='be horrendously resource-heavy. An example of this problem could be in filter-', bbox=BoundingBox(l=134.76505022108483, t=585.0646032303937, r=480.58698078841286, b=594.9972932852364, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=79, text='ing a large image (anything over', bbox=BoundingBox(l=134.76505022108483, t=597.0195932964024, r=277.987370456044, b=606.9523033512452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=80, text='128', bbox=BoundingBox(l=280.46405046010705, t=597.2288032975575, r=295.40796048462283, b=606.0755933464045, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=81, text='×', bbox=BoundingBox(l=297.56506048816163, t=596.6708932944771, r=305.31396050087386, b=606.0755933464045, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=82, text='128', bbox=BoundingBox(l=307.470060504411, t=597.2288032975575, r=322.41397052892677, b=606.0755933464045, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=83, text='could be considered large), so if the', bbox=BoundingBox(l=324.8900805329889, t=597.0195932964024, r=480.59555078842686, b=606.9523033512452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=84, text='input is', bbox=BoundingBox(l=134.7650802210849, t=608.974593362411, r=168.67776027671934, b=618.9073034172538, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=85, text='227', bbox=BoundingBox(l=171.53407028140518, t=609.1838033635662, r=186.47797030592096, b=618.0305934124132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=86, text='×', bbox=BoundingBox(l=188.9600703099929, t=608.6259033604858, r=196.70898032270514, b=618.0305934124132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=87, text='227', bbox=BoundingBox(l=199.19107032677704, t=609.1838033635662, r=214.13496035129282, b=618.0305934124132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=88, text='(as seen with ImageNet) and we’re filtering with 64 kernels', bbox=BoundingBox(l=216.98807035597338, t=608.974593362411, r=480.5884707884153, b=618.9073034172538, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=89, text='each with a zero padding of then the result will be three activation vectors of', bbox=BoundingBox(l=134.7650802210849, t=620.9295934284198, r=480.5868507884127, b=630.8623034832626, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=90, text='size', bbox=BoundingBox(l=134.7650802210849, t=632.885583494434, r=151.64172024877135, b=642.8182935492767, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=91, text='227', bbox=BoundingBox(l=154.46307025339985, t=633.0948034955892, r=169.4069702779156, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=92, text='×', bbox=BoundingBox(l=171.86607028194985, t=632.5368934925087, r=179.6149902946621, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=93, text='227', bbox=BoundingBox(l=182.07407029869626, t=633.0948034955892, r=197.017960323212, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=94, text='×', bbox=BoundingBox(l=199.47707032724625, t=632.5368934925087, r=207.22598033995848, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=95, text='64', bbox=BoundingBox(l=209.68506034399266, t=633.0948034955892, r=219.64766036033652, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=96, text='- which calculates to roughly 10 million activations - or an', bbox=BoundingBox(l=222.4690603649651, t=632.885583494434, r=480.590120788418, b=642.8182935492767, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=97, text='enormous 70 megabytes of memory per image. In this case you have two op-', bbox=BoundingBox(l=134.76505022108483, t=644.8405935604427, r=480.58679078841254, b=654.7733036152855, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=98, text='tions. Firstly, you can reduce the spatial dimensionality of the input images by', bbox=BoundingBox(l=134.76505022108483, t=656.7955936264514, r=480.58676078841245, b=666.7283036812942, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='CNNs are extremely powerful machine learning algorithms, however they can be horrendously resource-heavy. An example of this problem could be in filtering a large image (anything over 128 × 128 could be considered large), so if the input is 227 × 227 (as seen with ImageNet) and we’re filtering with 64 kernels each with a zero padding of then the result will be three activation vectors of size 227 × 227 × 64 - which calculates to roughly 10 million activations - or an enormous 70 megabytes of memory per image. In this case you have two options. Firstly, you can reduce the spatial dimensionality of the input images by')], headers=[TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=8, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=255.96002197265625, t=93.55046844482422, r=447.56597900390625, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9279627799987793, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=8, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=474.8158874511719, t=93.79475402832031, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8714832067489624, cells=[Cell(id=1, text='9', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='9')])), Page(page_no=9, size=Size(width=595.2760009765625, height=841.8900146484375), cells=[Cell(id=0, text='10', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=143.73140023579433, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=2, text='resizing the raw images to something a little less heavy. Alternatively, you can', bbox=BoundingBox(l=134.76500022108476, t=119.67181066075966, r=480.58682078841264, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='go against everything we stated earlier in this document and opt for larger filter', bbox=BoundingBox(l=134.76500022108476, t=131.6268307267684, r=480.58688078841266, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='sizes with a larger stride (2, as opposed to 1).', bbox=BoundingBox(l=134.76500022108476, t=143.58184079277714, r=331.9746705446113, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='In addition to the few rules-of-thumb outlined above, it is also important to ac-', bbox=BoundingBox(l=134.76500022108476, t=161.51483089179294, r=480.58679078841254, b=171.4475109466356, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='knowledge a few ’tricks’ about generalised ANN training techniques. The au-', bbox=BoundingBox(l=134.76500022108476, t=173.4698409578017, r=480.5867307884124, b=183.40252101264423, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='thors suggest a read of Geoffrey Hinton’s excellent “Practical Guide to Training', bbox=BoundingBox(l=134.76500022108476, t=185.42486102381054, r=480.58670078841243, b=195.35754107865318, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='Restricted Boltzmann Machines”.', bbox=BoundingBox(l=134.76500022108476, t=197.37988108981926, r=281.8328904623527, b=207.31256114466203, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='4', bbox=BoundingBox(l=134.76500022108476, t=231.85083128014833, r=140.74260023089116, b=243.4951713444417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='Conclusion', bbox=BoundingBox(l=152.6978002505039, t=231.85083128014833, r=215.1159103529021, b=243.4951713444417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='Convolutional Neural Networks differ to other forms of Artifical Neural Net-', bbox=BoundingBox(l=134.76500022108476, t=261.03387144128044, r=480.58676078841245, b=270.9665514961231, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='work in that instead of focusing on the entirety of the problem domain, knowl-', bbox=BoundingBox(l=134.76500022108476, t=272.98986150729456, r=480.58682078841264, b=282.9225415621372, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='edge about the specific type of input is exploited. This in turn allows for a much', bbox=BoundingBox(l=134.76500022108476, t=284.9448815733035, r=480.5867307884124, b=294.87756162814617, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='simpler network architecture to be set up.', bbox=BoundingBox(l=134.76500022108476, t=296.89990163931225, r=318.04703052176274, b=306.8325816941549, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='This paper has outlined the basic concepts of Convolutional Neural Networks,', bbox=BoundingBox(l=134.76500022108476, t=314.83288173832796, r=480.58679078841254, b=324.7655617931705, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='explaining the layers required to build one and detailing how best to structure', bbox=BoundingBox(l=134.76500022108476, t=326.7878418043364, r=480.5867307884124, b=336.7205518591792, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='the network in most image analysis tasks.', bbox=BoundingBox(l=134.76500022108476, t=338.74282187034504, r=318.21628052204034, b=348.67553192518784, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='Research in the field of image analysis using neural networks has somewhat', bbox=BoundingBox(l=134.76500022108476, t=356.675841969361, r=480.58667078841233, b=366.60855202420373, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='slowed in recent times. This is partly due to the incorrect belief surrounding the', bbox=BoundingBox(l=134.76500022108476, t=368.6308220353696, r=480.58676078841245, b=378.5635320902124, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='level of complexity and knowledge required to begin modelling these superbly', bbox=BoundingBox(l=134.76500022108476, t=380.5858121013783, r=480.58676078841245, b=390.518522156221, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='powerful machine learning algorithms. The authors hope that this paper has', bbox=BoundingBox(l=134.76500022108476, t=392.54080216738686, r=480.58679078841254, b=402.47351222222966, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='in some way reduced this confusion, and made the field more accessible to', bbox=BoundingBox(l=134.76500022108476, t=404.49679223340104, r=480.58667078841233, b=414.4295022882438, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='beginners.', bbox=BoundingBox(l=134.76500022108476, t=416.4517822994097, r=180.5033002961194, b=426.3844923542525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='Acknowledgements', bbox=BoundingBox(l=134.76500022108476, t=450.9227624897389, r=243.67688039975695, b=462.5671325540324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='The authors would like to thank Dr. Chuan Lu and Nicholas Dimonaco for', bbox=BoundingBox(l=134.76500022108476, t=480.105802650871, r=480.58667078841233, b=490.03851270571374, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='useful discussion and suggestions.', bbox=BoundingBox(l=134.76500022108476, t=492.0607927168796, r=286.96362047076974, b=501.99350277172243, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='References', bbox=BoundingBox(l=134.76500022108476, t=526.5317629072088, r=194.5290503191289, b=538.1761429715024, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='1.', bbox=BoundingBox(l=139.2480002284392, t=548.9420730309457, r=146.00546023952495, b=557.8815930803046, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for im-', bbox=BoundingBox(l=148.25793024322016, t=548.9420730309457, r=480.5899007884176, b=557.8815930803046, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='age classification. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE', bbox=BoundingBox(l=150.95399024764313, t=559.9010930914551, r=480.59470078842554, b=568.840603140814, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='Conference on. pp. 3642-3649. IEEE (2012)', bbox=BoundingBox(l=150.95399024764313, t=570.8600731519643, r=318.41046052235896, b=579.7995932013234, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='2.', bbox=BoundingBox(l=139.2480002284392, t=581.3070932096468, r=146.07829023964445, b=590.2465832590057, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='Cires¸an, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in', bbox=BoundingBox(l=148.35506024337954, t=581.3070932096468, r=480.5899007884176, b=590.2465832590057, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='breast cancer histology images with deep neural networks. In: Medical Image Com-', bbox=BoundingBox(l=150.95399024764313, t=592.2660832701563, r=480.59467078842545, b=601.2055833195151, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='puting and Computer-Assisted Intervention-MICCAI 2013, pp. 411-418. Springer', bbox=BoundingBox(l=150.95399024764313, t=603.2250833306656, r=480.59479078842566, b=612.1645833800244, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='(2013)', bbox=BoundingBox(l=150.95399024764313, t=614.1840833911749, r=174.85841028685883, b=623.1235834405338, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='3.', bbox=BoundingBox(l=139.2480002284392, t=624.6310834488573, r=146.0001202395162, b=633.5705834982161, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible,', bbox=BoundingBox(l=148.25081024320852, t=624.6310834488573, r=480.5899007884176, b=633.5705834982161, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='high performance convolutional neural networks for image classification. In: IJCAI', bbox=BoundingBox(l=150.95399024764313, t=635.5900835093667, r=480.5948507884258, b=644.5295835587256, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='Proceedings-International Joint Conference on Artificial Intelligence. vol. 22, p. 1237', bbox=BoundingBox(l=150.95399024764313, t=646.5490835698761, r=480.59467078842545, b=655.4885836192349, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='(2011)', bbox=BoundingBox(l=150.95399024764313, t=657.5080836303854, r=174.85841028685883, b=666.4475836797442, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))], predictions=PagePredictions(layout=LayoutPrediction(clusters=[Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=134.48631286621094, t=93.9991683959961, r=143.73849487304688, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8442774415016174, cells=[Cell(id=0, text='10', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=143.73140023579433, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=167.15977478027344, t=93.5068588256836, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8268627524375916, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8414764404297, t=118.74977111816406, r=480.58688078841266, b=153.6955108642578, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9780817627906799, cells=[Cell(id=2, text='resizing the raw images to something a little less heavy. Alternatively, you can', bbox=BoundingBox(l=134.76500022108476, t=119.67181066075966, r=480.58682078841264, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='go against everything we stated earlier in this document and opt for larger filter', bbox=BoundingBox(l=134.76500022108476, t=131.6268307267684, r=480.58688078841266, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='sizes with a larger stride (2, as opposed to 1).', bbox=BoundingBox(l=134.76500022108476, t=143.58184079277714, r=331.9746705446113, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=134.21324157714844, t=160.7611083984375, r=480.58679078841254, b=207.31256114466203, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9785709977149963, cells=[Cell(id=5, text='In addition to the few rules-of-thumb outlined above, it is also important to ac-', bbox=BoundingBox(l=134.76500022108476, t=161.51483089179294, r=480.58679078841254, b=171.4475109466356, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='knowledge a few ’tricks’ about generalised ANN training techniques. The au-', bbox=BoundingBox(l=134.76500022108476, t=173.4698409578017, r=480.5867307884124, b=183.40252101264423, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='thors suggest a read of Geoffrey Hinton’s excellent “Practical Guide to Training', bbox=BoundingBox(l=134.76500022108476, t=185.42486102381054, r=480.58670078841243, b=195.35754107865318, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='Restricted Boltzmann Machines”.', bbox=BoundingBox(l=134.76500022108476, t=197.37988108981926, r=281.8328904623527, b=207.31256114466203, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=4, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.84384155273438, t=231.3874969482422, r=215.1159103529021, b=243.4951713444417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9349105358123779, cells=[Cell(id=9, text='4', bbox=BoundingBox(l=134.76500022108476, t=231.85083128014833, r=140.74260023089116, b=243.4951713444417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='Conclusion', bbox=BoundingBox(l=152.6978002505039, t=231.85083128014833, r=215.1159103529021, b=243.4951713444417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.68148803710938, t=259.8370056152344, r=480.58682078841264, b=306.9059143066406, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.982923686504364, cells=[Cell(id=11, text='Convolutional Neural Networks differ to other forms of Artifical Neural Net-', bbox=BoundingBox(l=134.76500022108476, t=261.03387144128044, r=480.58676078841245, b=270.9665514961231, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='work in that instead of focusing on the entirety of the problem domain, knowl-', bbox=BoundingBox(l=134.76500022108476, t=272.98986150729456, r=480.58682078841264, b=282.9225415621372, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='edge about the specific type of input is exploited. This in turn allows for a much', bbox=BoundingBox(l=134.76500022108476, t=284.9448815733035, r=480.5867307884124, b=294.87756162814617, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='simpler network architecture to be set up.', bbox=BoundingBox(l=134.76500022108476, t=296.89990163931225, r=318.04703052176274, b=306.8325816941549, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.798095703125, t=313.9832458496094, r=480.58679078841254, b=348.8280944824219, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9790396094322205, cells=[Cell(id=15, text='This paper has outlined the basic concepts of Convolutional Neural Networks,', bbox=BoundingBox(l=134.76500022108476, t=314.83288173832796, r=480.58679078841254, b=324.7655617931705, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='explaining the layers required to build one and detailing how best to structure', bbox=BoundingBox(l=134.76500022108476, t=326.7878418043364, r=480.5867307884124, b=336.7205518591792, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='the network in most image analysis tasks.', bbox=BoundingBox(l=134.76500022108476, t=338.74282187034504, r=318.21628052204034, b=348.67553192518784, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8413543701172, t=356.0538024902344, r=480.8612365722656, b=426.3844923542525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9865900278091431, cells=[Cell(id=18, text='Research in the field of image analysis using neural networks has somewhat', bbox=BoundingBox(l=134.76500022108476, t=356.675841969361, r=480.58667078841233, b=366.60855202420373, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='slowed in recent times. This is partly due to the incorrect belief surrounding the', bbox=BoundingBox(l=134.76500022108476, t=368.6308220353696, r=480.58676078841245, b=378.5635320902124, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='level of complexity and knowledge required to begin modelling these superbly', bbox=BoundingBox(l=134.76500022108476, t=380.5858121013783, r=480.58676078841245, b=390.518522156221, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='powerful machine learning algorithms. The authors hope that this paper has', bbox=BoundingBox(l=134.76500022108476, t=392.54080216738686, r=480.58679078841254, b=402.47351222222966, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='in some way reduced this confusion, and made the field more accessible to', bbox=BoundingBox(l=134.76500022108476, t=404.49679223340104, r=480.58667078841233, b=414.4295022882438, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='beginners.', bbox=BoundingBox(l=134.76500022108476, t=416.4517822994097, r=180.5033002961194, b=426.3844923542525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=8, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.9271240234375, t=450.19580078125, r=243.67688039975695, b=462.5867004394531, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9575116038322449, cells=[Cell(id=24, text='Acknowledgements', bbox=BoundingBox(l=134.76500022108476, t=450.9227624897389, r=243.67688039975695, b=462.5671325540324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.86122131347656, t=479.3764343261719, r=480.82696533203125, b=502.0371398925781, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9725787043571472, cells=[Cell(id=25, text='The authors would like to thank Dr. Chuan Lu and Nicholas Dimonaco for', bbox=BoundingBox(l=134.76500022108476, t=480.105802650871, r=480.58667078841233, b=490.03851270571374, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='useful discussion and suggestions.', bbox=BoundingBox(l=134.76500022108476, t=492.0607927168796, r=286.96362047076974, b=501.99350277172243, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=10, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.29351806640625, t=525.9068603515625, r=194.5290503191289, b=538.1761429715024, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9404545426368713, cells=[Cell(id=27, text='References', bbox=BoundingBox(l=134.76500022108476, t=526.5317629072088, r=194.5290503191289, b=538.1761429715024, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=11, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.7662811279297, t=548.0494995117188, r=480.59470078842554, b=579.7995932013234, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.973379373550415, cells=[Cell(id=28, text='1.', bbox=BoundingBox(l=139.2480002284392, t=548.9420730309457, r=146.00546023952495, b=557.8815930803046, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for im-', bbox=BoundingBox(l=148.25793024322016, t=548.9420730309457, r=480.5899007884176, b=557.8815930803046, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='age classification. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE', bbox=BoundingBox(l=150.95399024764313, t=559.9010930914551, r=480.59470078842554, b=568.840603140814, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='Conference on. pp. 3642-3649. IEEE (2012)', bbox=BoundingBox(l=150.95399024764313, t=570.8600731519643, r=318.41046052235896, b=579.7995932013234, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=12, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.2350311279297, t=580.685791015625, r=480.658203125, b=623.1235834405338, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9770330190658569, cells=[Cell(id=32, text='2.', bbox=BoundingBox(l=139.2480002284392, t=581.3070932096468, r=146.07829023964445, b=590.2465832590057, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='Cires¸an, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in', bbox=BoundingBox(l=148.35506024337954, t=581.3070932096468, r=480.5899007884176, b=590.2465832590057, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='breast cancer histology images with deep neural networks. In: Medical Image Com-', bbox=BoundingBox(l=150.95399024764313, t=592.2660832701563, r=480.59467078842545, b=601.2055833195151, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='puting and Computer-Assisted Intervention-MICCAI 2013, pp. 411-418. Springer', bbox=BoundingBox(l=150.95399024764313, t=603.2250833306656, r=480.59479078842566, b=612.1645833800244, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='(2013)', bbox=BoundingBox(l=150.95399024764313, t=614.1840833911749, r=174.85841028685883, b=623.1235834405338, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=13, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.2299041748047, t=624.0062255859375, r=480.5948507884258, b=666.5886840820312, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9793604612350464, cells=[Cell(id=37, text='3.', bbox=BoundingBox(l=139.2480002284392, t=624.6310834488573, r=146.0001202395162, b=633.5705834982161, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible,', bbox=BoundingBox(l=148.25081024320852, t=624.6310834488573, r=480.5899007884176, b=633.5705834982161, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='high performance convolutional neural networks for image classification. In: IJCAI', bbox=BoundingBox(l=150.95399024764313, t=635.5900835093667, r=480.5948507884258, b=644.5295835587256, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='Proceedings-International Joint Conference on Artificial Intelligence. vol. 22, p. 1237', bbox=BoundingBox(l=150.95399024764313, t=646.5490835698761, r=480.59467078842545, b=655.4885836192349, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='(2011)', bbox=BoundingBox(l=150.95399024764313, t=657.5080836303854, r=174.85841028685883, b=666.4475836797442, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))])]), tablestructure=TableStructurePrediction(table_map={}), figures_classification=None, equations_prediction=None), assembled=AssembledUnit(elements=[TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=9, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=134.48631286621094, t=93.9991683959961, r=143.73849487304688, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8442774415016174, cells=[Cell(id=0, text='10', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=143.73140023579433, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='10'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=9, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=167.15977478027344, t=93.5068588256836, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8268627524375916, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea et al.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=9, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8414764404297, t=118.74977111816406, r=480.58688078841266, b=153.6955108642578, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9780817627906799, cells=[Cell(id=2, text='resizing the raw images to something a little less heavy. Alternatively, you can', bbox=BoundingBox(l=134.76500022108476, t=119.67181066075966, r=480.58682078841264, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='go against everything we stated earlier in this document and opt for larger filter', bbox=BoundingBox(l=134.76500022108476, t=131.6268307267684, r=480.58688078841266, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='sizes with a larger stride (2, as opposed to 1).', bbox=BoundingBox(l=134.76500022108476, t=143.58184079277714, r=331.9746705446113, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='resizing the raw images to something a little less heavy. Alternatively, you can go against everything we stated earlier in this document and opt for larger filter sizes with a larger stride (2, as opposed to 1).'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=9, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=134.21324157714844, t=160.7611083984375, r=480.58679078841254, b=207.31256114466203, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9785709977149963, cells=[Cell(id=5, text='In addition to the few rules-of-thumb outlined above, it is also important to ac-', bbox=BoundingBox(l=134.76500022108476, t=161.51483089179294, r=480.58679078841254, b=171.4475109466356, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='knowledge a few ’tricks’ about generalised ANN training techniques. The au-', bbox=BoundingBox(l=134.76500022108476, t=173.4698409578017, r=480.5867307884124, b=183.40252101264423, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='thors suggest a read of Geoffrey Hinton’s excellent “Practical Guide to Training', bbox=BoundingBox(l=134.76500022108476, t=185.42486102381054, r=480.58670078841243, b=195.35754107865318, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='Restricted Boltzmann Machines”.', bbox=BoundingBox(l=134.76500022108476, t=197.37988108981926, r=281.8328904623527, b=207.31256114466203, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='In addition to the few rules-of-thumb outlined above, it is also important to acknowledge a few ’tricks’ about generalised ANN training techniques. The authors suggest a read of Geoffrey Hinton’s excellent “Practical Guide to Training Restricted Boltzmann Machines”.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=4, page_no=9, cluster=Cluster(id=4, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.84384155273438, t=231.3874969482422, r=215.1159103529021, b=243.4951713444417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9349105358123779, cells=[Cell(id=9, text='4', bbox=BoundingBox(l=134.76500022108476, t=231.85083128014833, r=140.74260023089116, b=243.4951713444417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='Conclusion', bbox=BoundingBox(l=152.6978002505039, t=231.85083128014833, r=215.1159103529021, b=243.4951713444417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='4 Conclusion'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=9, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.68148803710938, t=259.8370056152344, r=480.58682078841264, b=306.9059143066406, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.982923686504364, cells=[Cell(id=11, text='Convolutional Neural Networks differ to other forms of Artifical Neural Net-', bbox=BoundingBox(l=134.76500022108476, t=261.03387144128044, r=480.58676078841245, b=270.9665514961231, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='work in that instead of focusing on the entirety of the problem domain, knowl-', bbox=BoundingBox(l=134.76500022108476, t=272.98986150729456, r=480.58682078841264, b=282.9225415621372, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='edge about the specific type of input is exploited. This in turn allows for a much', bbox=BoundingBox(l=134.76500022108476, t=284.9448815733035, r=480.5867307884124, b=294.87756162814617, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='simpler network architecture to be set up.', bbox=BoundingBox(l=134.76500022108476, t=296.89990163931225, r=318.04703052176274, b=306.8325816941549, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Convolutional Neural Networks differ to other forms of Artifical Neural Network in that instead of focusing on the entirety of the problem domain, knowledge about the specific type of input is exploited. This in turn allows for a much simpler network architecture to be set up.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=9, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.798095703125, t=313.9832458496094, r=480.58679078841254, b=348.8280944824219, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9790396094322205, cells=[Cell(id=15, text='This paper has outlined the basic concepts of Convolutional Neural Networks,', bbox=BoundingBox(l=134.76500022108476, t=314.83288173832796, r=480.58679078841254, b=324.7655617931705, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='explaining the layers required to build one and detailing how best to structure', bbox=BoundingBox(l=134.76500022108476, t=326.7878418043364, r=480.5867307884124, b=336.7205518591792, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='the network in most image analysis tasks.', bbox=BoundingBox(l=134.76500022108476, t=338.74282187034504, r=318.21628052204034, b=348.67553192518784, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='This paper has outlined the basic concepts of Convolutional Neural Networks, explaining the layers required to build one and detailing how best to structure the network in most image analysis tasks.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=9, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8413543701172, t=356.0538024902344, r=480.8612365722656, b=426.3844923542525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9865900278091431, cells=[Cell(id=18, text='Research in the field of image analysis using neural networks has somewhat', bbox=BoundingBox(l=134.76500022108476, t=356.675841969361, r=480.58667078841233, b=366.60855202420373, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='slowed in recent times. This is partly due to the incorrect belief surrounding the', bbox=BoundingBox(l=134.76500022108476, t=368.6308220353696, r=480.58676078841245, b=378.5635320902124, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='level of complexity and knowledge required to begin modelling these superbly', bbox=BoundingBox(l=134.76500022108476, t=380.5858121013783, r=480.58676078841245, b=390.518522156221, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='powerful machine learning algorithms. The authors hope that this paper has', bbox=BoundingBox(l=134.76500022108476, t=392.54080216738686, r=480.58679078841254, b=402.47351222222966, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='in some way reduced this confusion, and made the field more accessible to', bbox=BoundingBox(l=134.76500022108476, t=404.49679223340104, r=480.58667078841233, b=414.4295022882438, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='beginners.', bbox=BoundingBox(l=134.76500022108476, t=416.4517822994097, r=180.5033002961194, b=426.3844923542525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Research in the field of image analysis using neural networks has somewhat slowed in recent times. This is partly due to the incorrect belief surrounding the level of complexity and knowledge required to begin modelling these superbly powerful machine learning algorithms. The authors hope that this paper has in some way reduced this confusion, and made the field more accessible to beginners.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=8, page_no=9, cluster=Cluster(id=8, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.9271240234375, t=450.19580078125, r=243.67688039975695, b=462.5867004394531, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9575116038322449, cells=[Cell(id=24, text='Acknowledgements', bbox=BoundingBox(l=134.76500022108476, t=450.9227624897389, r=243.67688039975695, b=462.5671325540324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Acknowledgements'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=9, page_no=9, cluster=Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.86122131347656, t=479.3764343261719, r=480.82696533203125, b=502.0371398925781, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9725787043571472, cells=[Cell(id=25, text='The authors would like to thank Dr. Chuan Lu and Nicholas Dimonaco for', bbox=BoundingBox(l=134.76500022108476, t=480.105802650871, r=480.58667078841233, b=490.03851270571374, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='useful discussion and suggestions.', bbox=BoundingBox(l=134.76500022108476, t=492.0607927168796, r=286.96362047076974, b=501.99350277172243, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The authors would like to thank Dr. Chuan Lu and Nicholas Dimonaco for useful discussion and suggestions.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=10, page_no=9, cluster=Cluster(id=10, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.29351806640625, t=525.9068603515625, r=194.5290503191289, b=538.1761429715024, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9404545426368713, cells=[Cell(id=27, text='References', bbox=BoundingBox(l=134.76500022108476, t=526.5317629072088, r=194.5290503191289, b=538.1761429715024, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='References'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=11, page_no=9, cluster=Cluster(id=11, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.7662811279297, t=548.0494995117188, r=480.59470078842554, b=579.7995932013234, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.973379373550415, cells=[Cell(id=28, text='1.', bbox=BoundingBox(l=139.2480002284392, t=548.9420730309457, r=146.00546023952495, b=557.8815930803046, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for im-', bbox=BoundingBox(l=148.25793024322016, t=548.9420730309457, r=480.5899007884176, b=557.8815930803046, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='age classification. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE', bbox=BoundingBox(l=150.95399024764313, t=559.9010930914551, r=480.59470078842554, b=568.840603140814, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='Conference on. pp. 3642-3649. IEEE (2012)', bbox=BoundingBox(l=150.95399024764313, t=570.8600731519643, r=318.41046052235896, b=579.7995932013234, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='1. Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for image classification. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. pp. 3642-3649. IEEE (2012)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=12, page_no=9, cluster=Cluster(id=12, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.2350311279297, t=580.685791015625, r=480.658203125, b=623.1235834405338, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9770330190658569, cells=[Cell(id=32, text='2.', bbox=BoundingBox(l=139.2480002284392, t=581.3070932096468, r=146.07829023964445, b=590.2465832590057, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='Cires¸an, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in', bbox=BoundingBox(l=148.35506024337954, t=581.3070932096468, r=480.5899007884176, b=590.2465832590057, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='breast cancer histology images with deep neural networks. In: Medical Image Com-', bbox=BoundingBox(l=150.95399024764313, t=592.2660832701563, r=480.59467078842545, b=601.2055833195151, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='puting and Computer-Assisted Intervention-MICCAI 2013, pp. 411-418. Springer', bbox=BoundingBox(l=150.95399024764313, t=603.2250833306656, r=480.59479078842566, b=612.1645833800244, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='(2013)', bbox=BoundingBox(l=150.95399024764313, t=614.1840833911749, r=174.85841028685883, b=623.1235834405338, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2. Cires¸an, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in breast cancer histology images with deep neural networks. In: Medical Image Computing and Computer-Assisted Intervention-MICCAI 2013, pp. 411-418. Springer (2013)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=13, page_no=9, cluster=Cluster(id=13, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.2299041748047, t=624.0062255859375, r=480.5948507884258, b=666.5886840820312, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9793604612350464, cells=[Cell(id=37, text='3.', bbox=BoundingBox(l=139.2480002284392, t=624.6310834488573, r=146.0001202395162, b=633.5705834982161, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible,', bbox=BoundingBox(l=148.25081024320852, t=624.6310834488573, r=480.5899007884176, b=633.5705834982161, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='high performance convolutional neural networks for image classification. In: IJCAI', bbox=BoundingBox(l=150.95399024764313, t=635.5900835093667, r=480.5948507884258, b=644.5295835587256, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='Proceedings-International Joint Conference on Artificial Intelligence. vol. 22, p. 1237', bbox=BoundingBox(l=150.95399024764313, t=646.5490835698761, r=480.59467078842545, b=655.4885836192349, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='(2011)', bbox=BoundingBox(l=150.95399024764313, t=657.5080836303854, r=174.85841028685883, b=666.4475836797442, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='3. Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible, high performance convolutional neural networks for image classification. In: IJCAI Proceedings-International Joint Conference on Artificial Intelligence. vol. 22, p. 1237 (2011)')], body=[TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=9, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8414764404297, t=118.74977111816406, r=480.58688078841266, b=153.6955108642578, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9780817627906799, cells=[Cell(id=2, text='resizing the raw images to something a little less heavy. Alternatively, you can', bbox=BoundingBox(l=134.76500022108476, t=119.67181066075966, r=480.58682078841264, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='go against everything we stated earlier in this document and opt for larger filter', bbox=BoundingBox(l=134.76500022108476, t=131.6268307267684, r=480.58688078841266, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='sizes with a larger stride (2, as opposed to 1).', bbox=BoundingBox(l=134.76500022108476, t=143.58184079277714, r=331.9746705446113, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='resizing the raw images to something a little less heavy. Alternatively, you can go against everything we stated earlier in this document and opt for larger filter sizes with a larger stride (2, as opposed to 1).'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=9, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=134.21324157714844, t=160.7611083984375, r=480.58679078841254, b=207.31256114466203, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9785709977149963, cells=[Cell(id=5, text='In addition to the few rules-of-thumb outlined above, it is also important to ac-', bbox=BoundingBox(l=134.76500022108476, t=161.51483089179294, r=480.58679078841254, b=171.4475109466356, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='knowledge a few ’tricks’ about generalised ANN training techniques. The au-', bbox=BoundingBox(l=134.76500022108476, t=173.4698409578017, r=480.5867307884124, b=183.40252101264423, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='thors suggest a read of Geoffrey Hinton’s excellent “Practical Guide to Training', bbox=BoundingBox(l=134.76500022108476, t=185.42486102381054, r=480.58670078841243, b=195.35754107865318, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='Restricted Boltzmann Machines”.', bbox=BoundingBox(l=134.76500022108476, t=197.37988108981926, r=281.8328904623527, b=207.31256114466203, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='In addition to the few rules-of-thumb outlined above, it is also important to acknowledge a few ’tricks’ about generalised ANN training techniques. The authors suggest a read of Geoffrey Hinton’s excellent “Practical Guide to Training Restricted Boltzmann Machines”.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=4, page_no=9, cluster=Cluster(id=4, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.84384155273438, t=231.3874969482422, r=215.1159103529021, b=243.4951713444417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9349105358123779, cells=[Cell(id=9, text='4', bbox=BoundingBox(l=134.76500022108476, t=231.85083128014833, r=140.74260023089116, b=243.4951713444417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='Conclusion', bbox=BoundingBox(l=152.6978002505039, t=231.85083128014833, r=215.1159103529021, b=243.4951713444417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='4 Conclusion'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=9, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.68148803710938, t=259.8370056152344, r=480.58682078841264, b=306.9059143066406, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.982923686504364, cells=[Cell(id=11, text='Convolutional Neural Networks differ to other forms of Artifical Neural Net-', bbox=BoundingBox(l=134.76500022108476, t=261.03387144128044, r=480.58676078841245, b=270.9665514961231, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='work in that instead of focusing on the entirety of the problem domain, knowl-', bbox=BoundingBox(l=134.76500022108476, t=272.98986150729456, r=480.58682078841264, b=282.9225415621372, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='edge about the specific type of input is exploited. This in turn allows for a much', bbox=BoundingBox(l=134.76500022108476, t=284.9448815733035, r=480.5867307884124, b=294.87756162814617, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='simpler network architecture to be set up.', bbox=BoundingBox(l=134.76500022108476, t=296.89990163931225, r=318.04703052176274, b=306.8325816941549, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Convolutional Neural Networks differ to other forms of Artifical Neural Network in that instead of focusing on the entirety of the problem domain, knowledge about the specific type of input is exploited. This in turn allows for a much simpler network architecture to be set up.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=9, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.798095703125, t=313.9832458496094, r=480.58679078841254, b=348.8280944824219, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9790396094322205, cells=[Cell(id=15, text='This paper has outlined the basic concepts of Convolutional Neural Networks,', bbox=BoundingBox(l=134.76500022108476, t=314.83288173832796, r=480.58679078841254, b=324.7655617931705, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='explaining the layers required to build one and detailing how best to structure', bbox=BoundingBox(l=134.76500022108476, t=326.7878418043364, r=480.5867307884124, b=336.7205518591792, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='the network in most image analysis tasks.', bbox=BoundingBox(l=134.76500022108476, t=338.74282187034504, r=318.21628052204034, b=348.67553192518784, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='This paper has outlined the basic concepts of Convolutional Neural Networks, explaining the layers required to build one and detailing how best to structure the network in most image analysis tasks.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=9, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8413543701172, t=356.0538024902344, r=480.8612365722656, b=426.3844923542525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9865900278091431, cells=[Cell(id=18, text='Research in the field of image analysis using neural networks has somewhat', bbox=BoundingBox(l=134.76500022108476, t=356.675841969361, r=480.58667078841233, b=366.60855202420373, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='slowed in recent times. This is partly due to the incorrect belief surrounding the', bbox=BoundingBox(l=134.76500022108476, t=368.6308220353696, r=480.58676078841245, b=378.5635320902124, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='level of complexity and knowledge required to begin modelling these superbly', bbox=BoundingBox(l=134.76500022108476, t=380.5858121013783, r=480.58676078841245, b=390.518522156221, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='powerful machine learning algorithms. The authors hope that this paper has', bbox=BoundingBox(l=134.76500022108476, t=392.54080216738686, r=480.58679078841254, b=402.47351222222966, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='in some way reduced this confusion, and made the field more accessible to', bbox=BoundingBox(l=134.76500022108476, t=404.49679223340104, r=480.58667078841233, b=414.4295022882438, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='beginners.', bbox=BoundingBox(l=134.76500022108476, t=416.4517822994097, r=180.5033002961194, b=426.3844923542525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Research in the field of image analysis using neural networks has somewhat slowed in recent times. This is partly due to the incorrect belief surrounding the level of complexity and knowledge required to begin modelling these superbly powerful machine learning algorithms. The authors hope that this paper has in some way reduced this confusion, and made the field more accessible to beginners.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=8, page_no=9, cluster=Cluster(id=8, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.9271240234375, t=450.19580078125, r=243.67688039975695, b=462.5867004394531, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9575116038322449, cells=[Cell(id=24, text='Acknowledgements', bbox=BoundingBox(l=134.76500022108476, t=450.9227624897389, r=243.67688039975695, b=462.5671325540324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Acknowledgements'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=9, page_no=9, cluster=Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.86122131347656, t=479.3764343261719, r=480.82696533203125, b=502.0371398925781, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9725787043571472, cells=[Cell(id=25, text='The authors would like to thank Dr. Chuan Lu and Nicholas Dimonaco for', bbox=BoundingBox(l=134.76500022108476, t=480.105802650871, r=480.58667078841233, b=490.03851270571374, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='useful discussion and suggestions.', bbox=BoundingBox(l=134.76500022108476, t=492.0607927168796, r=286.96362047076974, b=501.99350277172243, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The authors would like to thank Dr. Chuan Lu and Nicholas Dimonaco for useful discussion and suggestions.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=10, page_no=9, cluster=Cluster(id=10, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.29351806640625, t=525.9068603515625, r=194.5290503191289, b=538.1761429715024, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9404545426368713, cells=[Cell(id=27, text='References', bbox=BoundingBox(l=134.76500022108476, t=526.5317629072088, r=194.5290503191289, b=538.1761429715024, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='References'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=11, page_no=9, cluster=Cluster(id=11, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.7662811279297, t=548.0494995117188, r=480.59470078842554, b=579.7995932013234, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.973379373550415, cells=[Cell(id=28, text='1.', bbox=BoundingBox(l=139.2480002284392, t=548.9420730309457, r=146.00546023952495, b=557.8815930803046, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for im-', bbox=BoundingBox(l=148.25793024322016, t=548.9420730309457, r=480.5899007884176, b=557.8815930803046, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='age classification. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE', bbox=BoundingBox(l=150.95399024764313, t=559.9010930914551, r=480.59470078842554, b=568.840603140814, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='Conference on. pp. 3642-3649. IEEE (2012)', bbox=BoundingBox(l=150.95399024764313, t=570.8600731519643, r=318.41046052235896, b=579.7995932013234, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='1. Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for image classification. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. pp. 3642-3649. IEEE (2012)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=12, page_no=9, cluster=Cluster(id=12, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.2350311279297, t=580.685791015625, r=480.658203125, b=623.1235834405338, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9770330190658569, cells=[Cell(id=32, text='2.', bbox=BoundingBox(l=139.2480002284392, t=581.3070932096468, r=146.07829023964445, b=590.2465832590057, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='Cires¸an, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in', bbox=BoundingBox(l=148.35506024337954, t=581.3070932096468, r=480.5899007884176, b=590.2465832590057, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='breast cancer histology images with deep neural networks. In: Medical Image Com-', bbox=BoundingBox(l=150.95399024764313, t=592.2660832701563, r=480.59467078842545, b=601.2055833195151, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='puting and Computer-Assisted Intervention-MICCAI 2013, pp. 411-418. Springer', bbox=BoundingBox(l=150.95399024764313, t=603.2250833306656, r=480.59479078842566, b=612.1645833800244, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='(2013)', bbox=BoundingBox(l=150.95399024764313, t=614.1840833911749, r=174.85841028685883, b=623.1235834405338, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2. Cires¸an, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in breast cancer histology images with deep neural networks. In: Medical Image Computing and Computer-Assisted Intervention-MICCAI 2013, pp. 411-418. Springer (2013)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=13, page_no=9, cluster=Cluster(id=13, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.2299041748047, t=624.0062255859375, r=480.5948507884258, b=666.5886840820312, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9793604612350464, cells=[Cell(id=37, text='3.', bbox=BoundingBox(l=139.2480002284392, t=624.6310834488573, r=146.0001202395162, b=633.5705834982161, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible,', bbox=BoundingBox(l=148.25081024320852, t=624.6310834488573, r=480.5899007884176, b=633.5705834982161, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='high performance convolutional neural networks for image classification. In: IJCAI', bbox=BoundingBox(l=150.95399024764313, t=635.5900835093667, r=480.5948507884258, b=644.5295835587256, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='Proceedings-International Joint Conference on Artificial Intelligence. vol. 22, p. 1237', bbox=BoundingBox(l=150.95399024764313, t=646.5490835698761, r=480.59467078842545, b=655.4885836192349, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='(2011)', bbox=BoundingBox(l=150.95399024764313, t=657.5080836303854, r=174.85841028685883, b=666.4475836797442, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='3. Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible, high performance convolutional neural networks for image classification. In: IJCAI Proceedings-International Joint Conference on Artificial Intelligence. vol. 22, p. 1237 (2011)')], headers=[TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=9, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=134.48631286621094, t=93.9991683959961, r=143.73849487304688, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8442774415016174, cells=[Cell(id=0, text='10', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=143.73140023579433, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='10'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=9, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=167.15977478027344, t=93.5068588256836, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8268627524375916, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea et al.')])), Page(page_no=10, size=Size(width=595.2760009765625, height=841.8900146484375), cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=1, text='11', bbox=BoundingBox(l=471.6257307737117, t=94.48107052167063, r=480.5921307884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=2, text='4.', bbox=BoundingBox(l=139.24802022843926, t=120.3840906646924, r=145.95924023944914, b=129.32354071405098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='Cires¸an, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural', bbox=BoundingBox(l=148.19632024311912, t=120.3840906646924, r=480.58994078841766, b=129.32354071405098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='network committees for handwritten character classification. In: Document Analysis', bbox=BoundingBox(l=150.95401024764317, t=131.34307072520176, r=480.5947307884256, b=140.2825307745602, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='and Recognition (ICDAR), 2011 International Conference on. pp. 1135-1139. IEEE', bbox=BoundingBox(l=150.95401024764317, t=142.30206078571098, r=480.5947307884256, b=151.24151083506956, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='(2011)', bbox=BoundingBox(l=150.95401024764317, t=153.26007084621483, r=174.85843028685886, b=162.19952089557341, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='5.', bbox=BoundingBox(l=139.24802022843926, t=164.21905090672408, r=145.96364023945637, b=173.15850095608266, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural net-', bbox=BoundingBox(l=148.20218024312874, t=164.21905090672408, r=480.58978078841744, b=173.15850095608266, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='worksa review. Pattern recognition 35(10), 2279-2301 (2002)', bbox=BoundingBox(l=150.95401024764317, t=175.1780309672332, r=386.3400006337987, b=184.11749101659188, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='6.', bbox=BoundingBox(l=139.24802022843926, t=186.13702102774266, r=146.00636023952646, b=195.07647107710125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware', bbox=BoundingBox(l=148.25916024322223, t=186.13702102774266, r=480.58987078841767, b=195.07647107710125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='accelerated convolutional neural networks for synthetic vision systems. In: Circuits', bbox=BoundingBox(l=150.95401024764317, t=197.0960010882519, r=480.59467078842545, b=206.03546113761047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp.', bbox=BoundingBox(l=150.95401024764317, t=208.05499114876113, r=480.5948507884258, b=216.99444119811972, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='257-260. IEEE (2010)', bbox=BoundingBox(l=150.95401024764317, t=219.0139712092705, r=232.4227303812943, b=227.95343125862905, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='7.', bbox=BoundingBox(l=139.24802022843926, t=229.97296126977972, r=145.9334702394069, b=238.9124113191383, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum', bbox=BoundingBox(l=148.16196024306274, t=229.97296126977972, r=480.5898107884175, b=238.9124113191383, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='9(1), 926 (2010)', bbox=BoundingBox(l=150.95401024764317, t=240.93194133028896, r=209.97086034446153, b=249.87139137964755, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='8.', bbox=BoundingBox(l=139.24802022843926, t=251.8909313907983, r=146.05434023960515, b=260.8303814401569, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Im-', bbox=BoundingBox(l=148.3231202433271, t=251.8909313907983, r=480.5898107884175, b=260.8303814401569, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='proving neural networks by preventing co-adaptation of feature detectors. arXiv', bbox=BoundingBox(l=150.95401024764317, t=262.84991145130755, r=480.5947307884256, b=271.78936150066613, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='preprint arXiv:1207.0580 (2012)', bbox=BoundingBox(l=150.95401024764317, t=273.8079215118115, r=274.11649044969374, b=282.74737156117, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='9.', bbox=BoundingBox(l=139.24802022843926, t=284.76690157232076, r=146.0757602396403, b=293.70636162167943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action', bbox=BoundingBox(l=148.35168024337398, t=284.76690157232076, r=480.5898107884175, b=293.70636162167943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1),', bbox=BoundingBox(l=150.95401024764317, t=295.72589163283, r=480.59470078842554, b=304.66534168218857, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='221-231 (2013)', bbox=BoundingBox(l=150.95401024764317, t=306.68487169333923, r=208.4824203420197, b=315.62432174269793, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='10.', bbox=BoundingBox(l=134.76501022108476, t=317.6438617538487, r=146.3162402400348, b=326.58331180320715, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-', bbox=BoundingBox(l=148.62648024382477, t=317.6438617538487, r=480.590120788418, b=326.58331180320715, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='scale video classification with convolutional neural networks. In: Computer Vision', bbox=BoundingBox(l=150.95401024764317, t=328.6028418143578, r=480.59476078842556, b=337.54232186371655, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725-1732. IEEE', bbox=BoundingBox(l=150.95401024764317, t=339.56179187486697, r=480.5947307884256, b=348.5013119242259, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='(2014)', bbox=BoundingBox(l=150.95401024764317, t=350.5207819353762, r=174.85843028685886, b=359.46029198473514, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='11.', bbox=BoundingBox(l=134.76501022108476, t=361.4797619958855, r=145.9576402394465, b=370.4192820452444, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convo-', bbox=BoundingBox(l=148.19617024311887, t=361.4797619958855, r=480.5902407884182, b=370.4192820452444, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='lutional neural networks. In: Advances in neural information processing systems.', bbox=BoundingBox(l=150.95401024764317, t=372.4387520563948, r=480.59479078842566, b=381.37826210575366, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='pp. 1097-1105 (2012)', bbox=BoundingBox(l=150.95401024764317, t=383.39675211689865, r=232.70963038176492, b=392.3362721662576, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='12.', bbox=BoundingBox(l=134.76501022108476, t=394.355742177408, r=145.86594023929607, b=403.2952522267668, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel,', bbox=BoundingBox(l=148.08614024293834, t=394.355742177408, r=480.5900307884179, b=403.2952522267668, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='L.D.: Backpropagation applied to handwritten zip code recognition. Neural compu-', bbox=BoundingBox(l=150.95401024764317, t=405.3147222379172, r=480.59467078842545, b=414.25424228727616, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='tation 1(4), 541-551 (1989)', bbox=BoundingBox(l=150.95401024764317, t=416.2737122984265, r=253.19788041537635, b=425.21322234778535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='13.', bbox=BoundingBox(l=134.76501022108476, t=427.2326923589357, r=145.78894023916976, b=436.1722124082947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to doc-', bbox=BoundingBox(l=147.99373024278677, t=427.2326923589357, r=480.59009078841797, b=436.1722124082947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='ument recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998)', bbox=BoundingBox(l=150.95401024764317, t=438.1916824194451, r=420.4302706897245, b=447.1311924688039, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='14.', bbox=BoundingBox(l=134.76501022108476, t=449.1506624799543, r=146.421830240208, b=458.0901725293132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='Nebauer, C.: Evaluation of convolutional neural networks for visual recognition.', bbox=BoundingBox(l=148.75319024403268, t=449.1506624799543, r=480.5900607884179, b=458.0901725293132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='Neural Networks, IEEE Transactions on 9(4), 685-696 (1998)', bbox=BoundingBox(l=150.95401024764317, t=460.1096525404636, r=387.7297706360787, b=469.0491625898225, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='15.', bbox=BoundingBox(l=134.76501022108476, t=471.0686326009729, r=146.22498023988507, b=480.00814265033176, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural net-', bbox=BoundingBox(l=148.51697024364515, t=471.0686326009729, r=480.590120788418, b=480.00814265033176, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='works applied to visual document analysis. In: null. p. 958. IEEE (2003)', bbox=BoundingBox(l=150.95401024764317, t=482.0276126614821, r=431.96100070864094, b=490.96713271084104, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='16.', bbox=BoundingBox(l=134.76501022108476, t=492.98562272198603, r=145.95445023944131, b=501.9251427713449, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of', bbox=BoundingBox(l=148.1923402431126, t=492.98562272198603, r=480.5900607884179, b=501.9251427713449, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='Toronto (2013)', bbox=BoundingBox(l=150.95401024764317, t=503.9446127824953, r=207.980320341196, b=512.8841228318543, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='17.', bbox=BoundingBox(l=134.76501022108476, t=514.9035928430046, r=145.9241202393915, b=523.8431028923634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with con-', bbox=BoundingBox(l=148.15594024305284, t=514.9035928430046, r=480.590150788418, b=523.8431028923634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='volutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings.', bbox=BoundingBox(l=150.95401024764317, t=525.8625729035139, r=480.5948207884257, b=534.8020929528727, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='IEEE. pp. 224-229. IEEE (2005)', bbox=BoundingBox(l=150.95401024764317, t=536.8215629640231, r=271.6238104456045, b=545.761073013382, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='18.', bbox=BoundingBox(l=134.76501022108476, t=547.7805430245323, r=146.3605302401075, b=556.7200630738914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In:', bbox=BoundingBox(l=148.67964024391202, t=547.7805430245323, r=480.5900307884179, b=556.7200630738914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='Advances in Neural Information Processing Systems. pp. 2553-2561 (2013)', bbox=BoundingBox(l=150.95401024764317, t=558.7395330850417, r=445.8409707314113, b=567.6790431344007, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='19.', bbox=BoundingBox(l=134.76501022108476, t=569.6985131455509, r=146.7078102406772, b=578.6380331949099, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks', bbox=BoundingBox(l=149.09637024459568, t=569.6985131455509, r=480.590150788418, b=578.6380331949099, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='(siconnets) and their application of face detection. In: Neural Networks, 2003. Pro-', bbox=BoundingBox(l=150.95401024764317, t=580.6575332060604, r=480.59479078842566, b=589.5970132554191, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='ceedings of the International Joint Conference on. vol. 3, pp. 2157-2162. IEEE (2003)', bbox=BoundingBox(l=150.95401024764317, t=591.6165132665697, r=480.59479078842566, b=600.5560133159285, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='20.', bbox=BoundingBox(l=134.76501022108476, t=602.5745233270735, r=146.06206023961784, b=611.5140233764323, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional', bbox=BoundingBox(l=148.32147024332446, t=602.5745233270735, r=480.59009078841797, b=611.5140233764323, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='neural networks. arXiv preprint arXiv:1301.3557 (2013)', bbox=BoundingBox(l=150.95401024764317, t=613.5335233875829, r=367.45676060282034, b=622.4730234369417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=65, text='21.', bbox=BoundingBox(l=134.76501022108476, t=624.4925234480922, r=146.0039102395224, b=633.4320234974512, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In:', bbox=BoundingBox(l=148.25168024320993, t=624.4925234480922, r=480.590150788418, b=633.4320234974512, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='Computer Vision-ECCV 2014, pp. 818-833. Springer (2014)', bbox=BoundingBox(l=150.95401024764317, t=635.4515235086017, r=384.4032306306214, b=644.3910235579605, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))], predictions=PagePredictions(layout=LayoutPrediction(clusters=[Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=256.0866394042969, t=93.61783599853516, r=447.5473937988281, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9234234094619751, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=471.0235290527344, t=93.66058349609375, r=480.5921307884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.889333963394165, cells=[Cell(id=1, text='11', bbox=BoundingBox(l=471.6257307737117, t=94.48107052167063, r=480.5921307884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=2, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.15142822265625, t=119.36505126953125, r=480.60302734375, b=162.19952089557341, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.970146894454956, cells=[Cell(id=2, text='4.', bbox=BoundingBox(l=139.24802022843926, t=120.3840906646924, r=145.95924023944914, b=129.32354071405098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='Cires¸an, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural', bbox=BoundingBox(l=148.19632024311912, t=120.3840906646924, r=480.58994078841766, b=129.32354071405098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='network committees for handwritten character classification. In: Document Analysis', bbox=BoundingBox(l=150.95401024764317, t=131.34307072520176, r=480.5947307884256, b=140.2825307745602, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='and Recognition (ICDAR), 2011 International Conference on. pp. 1135-1139. IEEE', bbox=BoundingBox(l=150.95401024764317, t=142.30206078571098, r=480.5947307884256, b=151.24151083506956, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='(2011)', bbox=BoundingBox(l=150.95401024764317, t=153.26007084621483, r=174.85843028685886, b=162.19952089557341, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=3, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.33560180664062, t=163.44866943359375, r=480.71539306640625, b=184.11749101659188, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9617682695388794, cells=[Cell(id=7, text='5.', bbox=BoundingBox(l=139.24802022843926, t=164.21905090672408, r=145.96364023945637, b=173.15850095608266, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural net-', bbox=BoundingBox(l=148.20218024312874, t=164.21905090672408, r=480.58978078841744, b=173.15850095608266, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='worksa review. Pattern recognition 35(10), 2279-2301 (2002)', bbox=BoundingBox(l=150.95401024764317, t=175.1780309672332, r=386.3400006337987, b=184.11749101659188, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=4, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.4720458984375, t=185.40130615234375, r=480.6998291015625, b=227.95343125862905, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9742252230644226, cells=[Cell(id=10, text='6.', bbox=BoundingBox(l=139.24802022843926, t=186.13702102774266, r=146.00636023952646, b=195.07647107710125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware', bbox=BoundingBox(l=148.25916024322223, t=186.13702102774266, r=480.58987078841767, b=195.07647107710125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='accelerated convolutional neural networks for synthetic vision systems. In: Circuits', bbox=BoundingBox(l=150.95401024764317, t=197.0960010882519, r=480.59467078842545, b=206.03546113761047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp.', bbox=BoundingBox(l=150.95401024764317, t=208.05499114876113, r=480.5948507884258, b=216.99444119811972, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='257-260. IEEE (2010)', bbox=BoundingBox(l=150.95401024764317, t=219.0139712092705, r=232.4227303812943, b=227.95343125862905, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=5, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.4396514892578, t=229.38987731933594, r=480.5898107884175, b=249.87139137964755, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9618682861328125, cells=[Cell(id=15, text='7.', bbox=BoundingBox(l=139.24802022843926, t=229.97296126977972, r=145.9334702394069, b=238.9124113191383, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum', bbox=BoundingBox(l=148.16196024306274, t=229.97296126977972, r=480.5898107884175, b=238.9124113191383, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='9(1), 926 (2010)', bbox=BoundingBox(l=150.95401024764317, t=240.93194133028896, r=209.97086034446153, b=249.87139137964755, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=6, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.49429321289062, t=251.51890563964844, r=480.5947307884256, b=282.74737156117, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.974453330039978, cells=[Cell(id=18, text='8.', bbox=BoundingBox(l=139.24802022843926, t=251.8909313907983, r=146.05434023960515, b=260.8303814401569, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Im-', bbox=BoundingBox(l=148.3231202433271, t=251.8909313907983, r=480.5898107884175, b=260.8303814401569, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='proving neural networks by preventing co-adaptation of feature detectors. arXiv', bbox=BoundingBox(l=150.95401024764317, t=262.84991145130755, r=480.5947307884256, b=271.78936150066613, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='preprint arXiv:1207.0580 (2012)', bbox=BoundingBox(l=150.95401024764317, t=273.8079215118115, r=274.11649044969374, b=282.74737156117, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=7, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.10630798339844, t=284.2738342285156, r=480.59470078842554, b=315.62432174269793, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9755162000656128, cells=[Cell(id=22, text='9.', bbox=BoundingBox(l=139.24802022843926, t=284.76690157232076, r=146.0757602396403, b=293.70636162167943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action', bbox=BoundingBox(l=148.35168024337398, t=284.76690157232076, r=480.5898107884175, b=293.70636162167943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1),', bbox=BoundingBox(l=150.95401024764317, t=295.72589163283, r=480.59470078842554, b=304.66534168218857, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='221-231 (2013)', bbox=BoundingBox(l=150.95401024764317, t=306.68487169333923, r=208.4824203420197, b=315.62432174269793, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=8, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.7255096435547, t=316.7909851074219, r=480.77935791015625, b=359.46029198473514, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9749436378479004, cells=[Cell(id=26, text='10.', bbox=BoundingBox(l=134.76501022108476, t=317.6438617538487, r=146.3162402400348, b=326.58331180320715, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-', bbox=BoundingBox(l=148.62648024382477, t=317.6438617538487, r=480.590120788418, b=326.58331180320715, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='scale video classification with convolutional neural networks. In: Computer Vision', bbox=BoundingBox(l=150.95401024764317, t=328.6028418143578, r=480.59476078842556, b=337.54232186371655, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725-1732. IEEE', bbox=BoundingBox(l=150.95401024764317, t=339.56179187486697, r=480.5947307884256, b=348.5013119242259, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='(2014)', bbox=BoundingBox(l=150.95401024764317, t=350.5207819353762, r=174.85843028685886, b=359.46029198473514, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=9, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.47573852539062, t=360.74285888671875, r=480.59479078842566, b=392.3362721662576, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9674268960952759, cells=[Cell(id=31, text='11.', bbox=BoundingBox(l=134.76501022108476, t=361.4797619958855, r=145.9576402394465, b=370.4192820452444, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convo-', bbox=BoundingBox(l=148.19617024311887, t=361.4797619958855, r=480.5902407884182, b=370.4192820452444, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='lutional neural networks. In: Advances in neural information processing systems.', bbox=BoundingBox(l=150.95401024764317, t=372.4387520563948, r=480.59479078842566, b=381.37826210575366, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='pp. 1097-1105 (2012)', bbox=BoundingBox(l=150.95401024764317, t=383.39675211689865, r=232.70963038176492, b=392.3362721662576, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=10, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.62811279296875, t=393.55804443359375, r=480.59467078842545, b=425.21322234778535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9672126173973083, cells=[Cell(id=35, text='12.', bbox=BoundingBox(l=134.76501022108476, t=394.355742177408, r=145.86594023929607, b=403.2952522267668, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel,', bbox=BoundingBox(l=148.08614024293834, t=394.355742177408, r=480.5900307884179, b=403.2952522267668, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='L.D.: Backpropagation applied to handwritten zip code recognition. Neural compu-', bbox=BoundingBox(l=150.95401024764317, t=405.3147222379172, r=480.59467078842545, b=414.25424228727616, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='tation 1(4), 541-551 (1989)', bbox=BoundingBox(l=150.95401024764317, t=416.2737122984265, r=253.19788041537635, b=425.21322234778535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=11, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.47447204589844, t=426.6608581542969, r=480.59009078841797, b=447.1311924688039, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9590675234794617, cells=[Cell(id=39, text='13.', bbox=BoundingBox(l=134.76501022108476, t=427.2326923589357, r=145.78894023916976, b=436.1722124082947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to doc-', bbox=BoundingBox(l=147.99373024278677, t=427.2326923589357, r=480.59009078841797, b=436.1722124082947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='ument recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998)', bbox=BoundingBox(l=150.95401024764317, t=438.1916824194451, r=420.4302706897245, b=447.1311924688039, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=12, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.4318084716797, t=448.2698059082031, r=480.5900607884179, b=469.0491625898225, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9524788856506348, cells=[Cell(id=42, text='14.', bbox=BoundingBox(l=134.76501022108476, t=449.1506624799543, r=146.421830240208, b=458.0901725293132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='Nebauer, C.: Evaluation of convolutional neural networks for visual recognition.', bbox=BoundingBox(l=148.75319024403268, t=449.1506624799543, r=480.5900607884179, b=458.0901725293132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='Neural Networks, IEEE Transactions on 9(4), 685-696 (1998)', bbox=BoundingBox(l=150.95401024764317, t=460.1096525404636, r=387.7297706360787, b=469.0491625898225, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=13, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.57110595703125, t=470.0508117675781, r=480.6988525390625, b=490.96713271084104, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9601881504058838, cells=[Cell(id=45, text='15.', bbox=BoundingBox(l=134.76501022108476, t=471.0686326009729, r=146.22498023988507, b=480.00814265033176, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural net-', bbox=BoundingBox(l=148.51697024364515, t=471.0686326009729, r=480.590120788418, b=480.00814265033176, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='works applied to visual document analysis. In: null. p. 958. IEEE (2003)', bbox=BoundingBox(l=150.95401024764317, t=482.0276126614821, r=431.96100070864094, b=490.96713271084104, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=14, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.7103729248047, t=492.13946533203125, r=480.5900607884179, b=512.8841228318543, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9577903747558594, cells=[Cell(id=48, text='16.', bbox=BoundingBox(l=134.76501022108476, t=492.98562272198603, r=145.95445023944131, b=501.9251427713449, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of', bbox=BoundingBox(l=148.1923402431126, t=492.98562272198603, r=480.5900607884179, b=501.9251427713449, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='Toronto (2013)', bbox=BoundingBox(l=150.95401024764317, t=503.9446127824953, r=207.980320341196, b=512.8841228318543, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=15, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.64859008789062, t=514.1981811523438, r=480.5948207884257, b=545.761073013382, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9668182134628296, cells=[Cell(id=51, text='17.', bbox=BoundingBox(l=134.76501022108476, t=514.9035928430046, r=145.9241202393915, b=523.8431028923634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with con-', bbox=BoundingBox(l=148.15594024305284, t=514.9035928430046, r=480.590150788418, b=523.8431028923634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='volutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings.', bbox=BoundingBox(l=150.95401024764317, t=525.8625729035139, r=480.5948207884257, b=534.8020929528727, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='IEEE. pp. 224-229. IEEE (2005)', bbox=BoundingBox(l=150.95401024764317, t=536.8215629640231, r=271.6238104456045, b=545.761073013382, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=16, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.76501022108476, t=546.84033203125, r=480.5900307884179, b=567.6790431344007, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9606278538703918, cells=[Cell(id=55, text='18.', bbox=BoundingBox(l=134.76501022108476, t=547.7805430245323, r=146.3605302401075, b=556.7200630738914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In:', bbox=BoundingBox(l=148.67964024391202, t=547.7805430245323, r=480.5900307884179, b=556.7200630738914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='Advances in Neural Information Processing Systems. pp. 2553-2561 (2013)', bbox=BoundingBox(l=150.95401024764317, t=558.7395330850417, r=445.8409707314113, b=567.6790431344007, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=17, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.6785888671875, t=568.6400146484375, r=480.60009765625, b=600.720947265625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9559718370437622, cells=[Cell(id=58, text='19.', bbox=BoundingBox(l=134.76501022108476, t=569.6985131455509, r=146.7078102406772, b=578.6380331949099, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks', bbox=BoundingBox(l=149.09637024459568, t=569.6985131455509, r=480.590150788418, b=578.6380331949099, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='(siconnets) and their application of face detection. In: Neural Networks, 2003. Pro-', bbox=BoundingBox(l=150.95401024764317, t=580.6575332060604, r=480.59479078842566, b=589.5970132554191, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='ceedings of the International Joint Conference on. vol. 3, pp. 2157-2162. IEEE (2003)', bbox=BoundingBox(l=150.95401024764317, t=591.6165132665697, r=480.59479078842566, b=600.5560133159285, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=18, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.3703155517578, t=601.6026611328125, r=480.59009078841797, b=622.4730234369417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.958541750907898, cells=[Cell(id=62, text='20.', bbox=BoundingBox(l=134.76501022108476, t=602.5745233270735, r=146.06206023961784, b=611.5140233764323, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional', bbox=BoundingBox(l=148.32147024332446, t=602.5745233270735, r=480.59009078841797, b=611.5140233764323, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='neural networks. arXiv preprint arXiv:1301.3557 (2013)', bbox=BoundingBox(l=150.95401024764317, t=613.5335233875829, r=367.45676060282034, b=622.4730234369417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), Cluster(id=19, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.26535034179688, t=623.5272216796875, r=480.590150788418, b=644.814208984375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9669718146324158, cells=[Cell(id=65, text='21.', bbox=BoundingBox(l=134.76501022108476, t=624.4925234480922, r=146.0039102395224, b=633.4320234974512, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In:', bbox=BoundingBox(l=148.25168024320993, t=624.4925234480922, r=480.590150788418, b=633.4320234974512, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='Computer Vision-ECCV 2014, pp. 818-833. Springer (2014)', bbox=BoundingBox(l=150.95401024764317, t=635.4515235086017, r=384.4032306306214, b=644.3910235579605, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))])]), tablestructure=TableStructurePrediction(table_map={}), figures_classification=None, equations_prediction=None), assembled=AssembledUnit(elements=[TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=10, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=256.0866394042969, t=93.61783599853516, r=447.5473937988281, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9234234094619751, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=10, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=471.0235290527344, t=93.66058349609375, r=480.5921307884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.889333963394165, cells=[Cell(id=1, text='11', bbox=BoundingBox(l=471.6257307737117, t=94.48107052167063, r=480.5921307884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='11'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=2, page_no=10, cluster=Cluster(id=2, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.15142822265625, t=119.36505126953125, r=480.60302734375, b=162.19952089557341, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.970146894454956, cells=[Cell(id=2, text='4.', bbox=BoundingBox(l=139.24802022843926, t=120.3840906646924, r=145.95924023944914, b=129.32354071405098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='Cires¸an, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural', bbox=BoundingBox(l=148.19632024311912, t=120.3840906646924, r=480.58994078841766, b=129.32354071405098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='network committees for handwritten character classification. In: Document Analysis', bbox=BoundingBox(l=150.95401024764317, t=131.34307072520176, r=480.5947307884256, b=140.2825307745602, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='and Recognition (ICDAR), 2011 International Conference on. pp. 1135-1139. IEEE', bbox=BoundingBox(l=150.95401024764317, t=142.30206078571098, r=480.5947307884256, b=151.24151083506956, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='(2011)', bbox=BoundingBox(l=150.95401024764317, t=153.26007084621483, r=174.85843028685886, b=162.19952089557341, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='4. Cires¸an, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural network committees for handwritten character classification. In: Document Analysis and Recognition (ICDAR), 2011 International Conference on. pp. 1135-1139. IEEE (2011)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=3, page_no=10, cluster=Cluster(id=3, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.33560180664062, t=163.44866943359375, r=480.71539306640625, b=184.11749101659188, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9617682695388794, cells=[Cell(id=7, text='5.', bbox=BoundingBox(l=139.24802022843926, t=164.21905090672408, r=145.96364023945637, b=173.15850095608266, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural net-', bbox=BoundingBox(l=148.20218024312874, t=164.21905090672408, r=480.58978078841744, b=173.15850095608266, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='worksa review. Pattern recognition 35(10), 2279-2301 (2002)', bbox=BoundingBox(l=150.95401024764317, t=175.1780309672332, r=386.3400006337987, b=184.11749101659188, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='5. Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural networksa review. Pattern recognition 35(10), 2279-2301 (2002)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=4, page_no=10, cluster=Cluster(id=4, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.4720458984375, t=185.40130615234375, r=480.6998291015625, b=227.95343125862905, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9742252230644226, cells=[Cell(id=10, text='6.', bbox=BoundingBox(l=139.24802022843926, t=186.13702102774266, r=146.00636023952646, b=195.07647107710125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware', bbox=BoundingBox(l=148.25916024322223, t=186.13702102774266, r=480.58987078841767, b=195.07647107710125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='accelerated convolutional neural networks for synthetic vision systems. In: Circuits', bbox=BoundingBox(l=150.95401024764317, t=197.0960010882519, r=480.59467078842545, b=206.03546113761047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp.', bbox=BoundingBox(l=150.95401024764317, t=208.05499114876113, r=480.5948507884258, b=216.99444119811972, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='257-260. IEEE (2010)', bbox=BoundingBox(l=150.95401024764317, t=219.0139712092705, r=232.4227303812943, b=227.95343125862905, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='6. Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware accelerated convolutional neural networks for synthetic vision systems. In: Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp. 257-260. IEEE (2010)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=5, page_no=10, cluster=Cluster(id=5, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.4396514892578, t=229.38987731933594, r=480.5898107884175, b=249.87139137964755, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9618682861328125, cells=[Cell(id=15, text='7.', bbox=BoundingBox(l=139.24802022843926, t=229.97296126977972, r=145.9334702394069, b=238.9124113191383, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum', bbox=BoundingBox(l=148.16196024306274, t=229.97296126977972, r=480.5898107884175, b=238.9124113191383, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='9(1), 926 (2010)', bbox=BoundingBox(l=150.95401024764317, t=240.93194133028896, r=209.97086034446153, b=249.87139137964755, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='7. Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum 9(1), 926 (2010)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=6, page_no=10, cluster=Cluster(id=6, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.49429321289062, t=251.51890563964844, r=480.5947307884256, b=282.74737156117, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.974453330039978, cells=[Cell(id=18, text='8.', bbox=BoundingBox(l=139.24802022843926, t=251.8909313907983, r=146.05434023960515, b=260.8303814401569, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Im-', bbox=BoundingBox(l=148.3231202433271, t=251.8909313907983, r=480.5898107884175, b=260.8303814401569, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='proving neural networks by preventing co-adaptation of feature detectors. arXiv', bbox=BoundingBox(l=150.95401024764317, t=262.84991145130755, r=480.5947307884256, b=271.78936150066613, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='preprint arXiv:1207.0580 (2012)', bbox=BoundingBox(l=150.95401024764317, t=273.8079215118115, r=274.11649044969374, b=282.74737156117, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='8. Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 (2012)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=7, page_no=10, cluster=Cluster(id=7, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.10630798339844, t=284.2738342285156, r=480.59470078842554, b=315.62432174269793, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9755162000656128, cells=[Cell(id=22, text='9.', bbox=BoundingBox(l=139.24802022843926, t=284.76690157232076, r=146.0757602396403, b=293.70636162167943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action', bbox=BoundingBox(l=148.35168024337398, t=284.76690157232076, r=480.5898107884175, b=293.70636162167943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1),', bbox=BoundingBox(l=150.95401024764317, t=295.72589163283, r=480.59470078842554, b=304.66534168218857, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='221-231 (2013)', bbox=BoundingBox(l=150.95401024764317, t=306.68487169333923, r=208.4824203420197, b=315.62432174269793, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='9. Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1), 221-231 (2013)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=8, page_no=10, cluster=Cluster(id=8, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.7255096435547, t=316.7909851074219, r=480.77935791015625, b=359.46029198473514, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9749436378479004, cells=[Cell(id=26, text='10.', bbox=BoundingBox(l=134.76501022108476, t=317.6438617538487, r=146.3162402400348, b=326.58331180320715, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-', bbox=BoundingBox(l=148.62648024382477, t=317.6438617538487, r=480.590120788418, b=326.58331180320715, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='scale video classification with convolutional neural networks. In: Computer Vision', bbox=BoundingBox(l=150.95401024764317, t=328.6028418143578, r=480.59476078842556, b=337.54232186371655, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725-1732. IEEE', bbox=BoundingBox(l=150.95401024764317, t=339.56179187486697, r=480.5947307884256, b=348.5013119242259, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='(2014)', bbox=BoundingBox(l=150.95401024764317, t=350.5207819353762, r=174.85843028685886, b=359.46029198473514, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='10. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Largescale video classification with convolutional neural networks. In: Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725-1732. IEEE (2014)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=9, page_no=10, cluster=Cluster(id=9, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.47573852539062, t=360.74285888671875, r=480.59479078842566, b=392.3362721662576, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9674268960952759, cells=[Cell(id=31, text='11.', bbox=BoundingBox(l=134.76501022108476, t=361.4797619958855, r=145.9576402394465, b=370.4192820452444, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convo-', bbox=BoundingBox(l=148.19617024311887, t=361.4797619958855, r=480.5902407884182, b=370.4192820452444, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='lutional neural networks. In: Advances in neural information processing systems.', bbox=BoundingBox(l=150.95401024764317, t=372.4387520563948, r=480.59479078842566, b=381.37826210575366, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='pp. 1097-1105 (2012)', bbox=BoundingBox(l=150.95401024764317, t=383.39675211689865, r=232.70963038176492, b=392.3362721662576, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='11. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems. pp. 1097-1105 (2012)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=10, page_no=10, cluster=Cluster(id=10, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.62811279296875, t=393.55804443359375, r=480.59467078842545, b=425.21322234778535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9672126173973083, cells=[Cell(id=35, text='12.', bbox=BoundingBox(l=134.76501022108476, t=394.355742177408, r=145.86594023929607, b=403.2952522267668, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel,', bbox=BoundingBox(l=148.08614024293834, t=394.355742177408, r=480.5900307884179, b=403.2952522267668, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='L.D.: Backpropagation applied to handwritten zip code recognition. Neural compu-', bbox=BoundingBox(l=150.95401024764317, t=405.3147222379172, r=480.59467078842545, b=414.25424228727616, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='tation 1(4), 541-551 (1989)', bbox=BoundingBox(l=150.95401024764317, t=416.2737122984265, r=253.19788041537635, b=425.21322234778535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='12. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural computation 1(4), 541-551 (1989)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=11, page_no=10, cluster=Cluster(id=11, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.47447204589844, t=426.6608581542969, r=480.59009078841797, b=447.1311924688039, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9590675234794617, cells=[Cell(id=39, text='13.', bbox=BoundingBox(l=134.76501022108476, t=427.2326923589357, r=145.78894023916976, b=436.1722124082947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to doc-', bbox=BoundingBox(l=147.99373024278677, t=427.2326923589357, r=480.59009078841797, b=436.1722124082947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='ument recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998)', bbox=BoundingBox(l=150.95401024764317, t=438.1916824194451, r=420.4302706897245, b=447.1311924688039, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='13. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=12, page_no=10, cluster=Cluster(id=12, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.4318084716797, t=448.2698059082031, r=480.5900607884179, b=469.0491625898225, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9524788856506348, cells=[Cell(id=42, text='14.', bbox=BoundingBox(l=134.76501022108476, t=449.1506624799543, r=146.421830240208, b=458.0901725293132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='Nebauer, C.: Evaluation of convolutional neural networks for visual recognition.', bbox=BoundingBox(l=148.75319024403268, t=449.1506624799543, r=480.5900607884179, b=458.0901725293132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='Neural Networks, IEEE Transactions on 9(4), 685-696 (1998)', bbox=BoundingBox(l=150.95401024764317, t=460.1096525404636, r=387.7297706360787, b=469.0491625898225, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='14. Nebauer, C.: Evaluation of convolutional neural networks for visual recognition. Neural Networks, IEEE Transactions on 9(4), 685-696 (1998)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=13, page_no=10, cluster=Cluster(id=13, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.57110595703125, t=470.0508117675781, r=480.6988525390625, b=490.96713271084104, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9601881504058838, cells=[Cell(id=45, text='15.', bbox=BoundingBox(l=134.76501022108476, t=471.0686326009729, r=146.22498023988507, b=480.00814265033176, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural net-', bbox=BoundingBox(l=148.51697024364515, t=471.0686326009729, r=480.590120788418, b=480.00814265033176, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='works applied to visual document analysis. In: null. p. 958. IEEE (2003)', bbox=BoundingBox(l=150.95401024764317, t=482.0276126614821, r=431.96100070864094, b=490.96713271084104, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='15. Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural networks applied to visual document analysis. In: null. p. 958. IEEE (2003)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=14, page_no=10, cluster=Cluster(id=14, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.7103729248047, t=492.13946533203125, r=480.5900607884179, b=512.8841228318543, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9577903747558594, cells=[Cell(id=48, text='16.', bbox=BoundingBox(l=134.76501022108476, t=492.98562272198603, r=145.95445023944131, b=501.9251427713449, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of', bbox=BoundingBox(l=148.1923402431126, t=492.98562272198603, r=480.5900607884179, b=501.9251427713449, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='Toronto (2013)', bbox=BoundingBox(l=150.95401024764317, t=503.9446127824953, r=207.980320341196, b=512.8841228318543, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='16. Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of Toronto (2013)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=15, page_no=10, cluster=Cluster(id=15, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.64859008789062, t=514.1981811523438, r=480.5948207884257, b=545.761073013382, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9668182134628296, cells=[Cell(id=51, text='17.', bbox=BoundingBox(l=134.76501022108476, t=514.9035928430046, r=145.9241202393915, b=523.8431028923634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with con-', bbox=BoundingBox(l=148.15594024305284, t=514.9035928430046, r=480.590150788418, b=523.8431028923634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='volutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings.', bbox=BoundingBox(l=150.95401024764317, t=525.8625729035139, r=480.5948207884257, b=534.8020929528727, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='IEEE. pp. 224-229. IEEE (2005)', bbox=BoundingBox(l=150.95401024764317, t=536.8215629640231, r=271.6238104456045, b=545.761073013382, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='17. Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with convolutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings. IEEE. pp. 224-229. IEEE (2005)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=16, page_no=10, cluster=Cluster(id=16, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.76501022108476, t=546.84033203125, r=480.5900307884179, b=567.6790431344007, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9606278538703918, cells=[Cell(id=55, text='18.', bbox=BoundingBox(l=134.76501022108476, t=547.7805430245323, r=146.3605302401075, b=556.7200630738914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In:', bbox=BoundingBox(l=148.67964024391202, t=547.7805430245323, r=480.5900307884179, b=556.7200630738914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='Advances in Neural Information Processing Systems. pp. 2553-2561 (2013)', bbox=BoundingBox(l=150.95401024764317, t=558.7395330850417, r=445.8409707314113, b=567.6790431344007, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='18. Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In: Advances in Neural Information Processing Systems. pp. 2553-2561 (2013)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=17, page_no=10, cluster=Cluster(id=17, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.6785888671875, t=568.6400146484375, r=480.60009765625, b=600.720947265625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9559718370437622, cells=[Cell(id=58, text='19.', bbox=BoundingBox(l=134.76501022108476, t=569.6985131455509, r=146.7078102406772, b=578.6380331949099, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks', bbox=BoundingBox(l=149.09637024459568, t=569.6985131455509, r=480.590150788418, b=578.6380331949099, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='(siconnets) and their application of face detection. In: Neural Networks, 2003. Pro-', bbox=BoundingBox(l=150.95401024764317, t=580.6575332060604, r=480.59479078842566, b=589.5970132554191, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='ceedings of the International Joint Conference on. vol. 3, pp. 2157-2162. IEEE (2003)', bbox=BoundingBox(l=150.95401024764317, t=591.6165132665697, r=480.59479078842566, b=600.5560133159285, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='19. Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks (siconnets) and their application of face detection. In: Neural Networks, 2003. Proceedings of the International Joint Conference on. vol. 3, pp. 2157-2162. IEEE (2003)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=18, page_no=10, cluster=Cluster(id=18, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.3703155517578, t=601.6026611328125, r=480.59009078841797, b=622.4730234369417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.958541750907898, cells=[Cell(id=62, text='20.', bbox=BoundingBox(l=134.76501022108476, t=602.5745233270735, r=146.06206023961784, b=611.5140233764323, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional', bbox=BoundingBox(l=148.32147024332446, t=602.5745233270735, r=480.59009078841797, b=611.5140233764323, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='neural networks. arXiv preprint arXiv:1301.3557 (2013)', bbox=BoundingBox(l=150.95401024764317, t=613.5335233875829, r=367.45676060282034, b=622.4730234369417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='20. Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional neural networks. arXiv preprint arXiv:1301.3557 (2013)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=19, page_no=10, cluster=Cluster(id=19, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.26535034179688, t=623.5272216796875, r=480.590150788418, b=644.814208984375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9669718146324158, cells=[Cell(id=65, text='21.', bbox=BoundingBox(l=134.76501022108476, t=624.4925234480922, r=146.0039102395224, b=633.4320234974512, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In:', bbox=BoundingBox(l=148.25168024320993, t=624.4925234480922, r=480.590150788418, b=633.4320234974512, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='Computer Vision-ECCV 2014, pp. 818-833. Springer (2014)', bbox=BoundingBox(l=150.95401024764317, t=635.4515235086017, r=384.4032306306214, b=644.3910235579605, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='21. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In: Computer Vision-ECCV 2014, pp. 818-833. Springer (2014)')], body=[TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=2, page_no=10, cluster=Cluster(id=2, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.15142822265625, t=119.36505126953125, r=480.60302734375, b=162.19952089557341, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.970146894454956, cells=[Cell(id=2, text='4.', bbox=BoundingBox(l=139.24802022843926, t=120.3840906646924, r=145.95924023944914, b=129.32354071405098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='Cires¸an, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural', bbox=BoundingBox(l=148.19632024311912, t=120.3840906646924, r=480.58994078841766, b=129.32354071405098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='network committees for handwritten character classification. In: Document Analysis', bbox=BoundingBox(l=150.95401024764317, t=131.34307072520176, r=480.5947307884256, b=140.2825307745602, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='and Recognition (ICDAR), 2011 International Conference on. pp. 1135-1139. IEEE', bbox=BoundingBox(l=150.95401024764317, t=142.30206078571098, r=480.5947307884256, b=151.24151083506956, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='(2011)', bbox=BoundingBox(l=150.95401024764317, t=153.26007084621483, r=174.85843028685886, b=162.19952089557341, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='4. Cires¸an, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural network committees for handwritten character classification. In: Document Analysis and Recognition (ICDAR), 2011 International Conference on. pp. 1135-1139. IEEE (2011)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=3, page_no=10, cluster=Cluster(id=3, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.33560180664062, t=163.44866943359375, r=480.71539306640625, b=184.11749101659188, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9617682695388794, cells=[Cell(id=7, text='5.', bbox=BoundingBox(l=139.24802022843926, t=164.21905090672408, r=145.96364023945637, b=173.15850095608266, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural net-', bbox=BoundingBox(l=148.20218024312874, t=164.21905090672408, r=480.58978078841744, b=173.15850095608266, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='worksa review. Pattern recognition 35(10), 2279-2301 (2002)', bbox=BoundingBox(l=150.95401024764317, t=175.1780309672332, r=386.3400006337987, b=184.11749101659188, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='5. Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural networksa review. Pattern recognition 35(10), 2279-2301 (2002)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=4, page_no=10, cluster=Cluster(id=4, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.4720458984375, t=185.40130615234375, r=480.6998291015625, b=227.95343125862905, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9742252230644226, cells=[Cell(id=10, text='6.', bbox=BoundingBox(l=139.24802022843926, t=186.13702102774266, r=146.00636023952646, b=195.07647107710125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware', bbox=BoundingBox(l=148.25916024322223, t=186.13702102774266, r=480.58987078841767, b=195.07647107710125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='accelerated convolutional neural networks for synthetic vision systems. In: Circuits', bbox=BoundingBox(l=150.95401024764317, t=197.0960010882519, r=480.59467078842545, b=206.03546113761047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp.', bbox=BoundingBox(l=150.95401024764317, t=208.05499114876113, r=480.5948507884258, b=216.99444119811972, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='257-260. IEEE (2010)', bbox=BoundingBox(l=150.95401024764317, t=219.0139712092705, r=232.4227303812943, b=227.95343125862905, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='6. Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware accelerated convolutional neural networks for synthetic vision systems. In: Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp. 257-260. IEEE (2010)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=5, page_no=10, cluster=Cluster(id=5, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.4396514892578, t=229.38987731933594, r=480.5898107884175, b=249.87139137964755, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9618682861328125, cells=[Cell(id=15, text='7.', bbox=BoundingBox(l=139.24802022843926, t=229.97296126977972, r=145.9334702394069, b=238.9124113191383, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum', bbox=BoundingBox(l=148.16196024306274, t=229.97296126977972, r=480.5898107884175, b=238.9124113191383, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='9(1), 926 (2010)', bbox=BoundingBox(l=150.95401024764317, t=240.93194133028896, r=209.97086034446153, b=249.87139137964755, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='7. Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum 9(1), 926 (2010)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=6, page_no=10, cluster=Cluster(id=6, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.49429321289062, t=251.51890563964844, r=480.5947307884256, b=282.74737156117, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.974453330039978, cells=[Cell(id=18, text='8.', bbox=BoundingBox(l=139.24802022843926, t=251.8909313907983, r=146.05434023960515, b=260.8303814401569, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Im-', bbox=BoundingBox(l=148.3231202433271, t=251.8909313907983, r=480.5898107884175, b=260.8303814401569, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='proving neural networks by preventing co-adaptation of feature detectors. arXiv', bbox=BoundingBox(l=150.95401024764317, t=262.84991145130755, r=480.5947307884256, b=271.78936150066613, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='preprint arXiv:1207.0580 (2012)', bbox=BoundingBox(l=150.95401024764317, t=273.8079215118115, r=274.11649044969374, b=282.74737156117, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='8. Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 (2012)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=7, page_no=10, cluster=Cluster(id=7, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.10630798339844, t=284.2738342285156, r=480.59470078842554, b=315.62432174269793, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9755162000656128, cells=[Cell(id=22, text='9.', bbox=BoundingBox(l=139.24802022843926, t=284.76690157232076, r=146.0757602396403, b=293.70636162167943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action', bbox=BoundingBox(l=148.35168024337398, t=284.76690157232076, r=480.5898107884175, b=293.70636162167943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1),', bbox=BoundingBox(l=150.95401024764317, t=295.72589163283, r=480.59470078842554, b=304.66534168218857, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='221-231 (2013)', bbox=BoundingBox(l=150.95401024764317, t=306.68487169333923, r=208.4824203420197, b=315.62432174269793, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='9. Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1), 221-231 (2013)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=8, page_no=10, cluster=Cluster(id=8, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.7255096435547, t=316.7909851074219, r=480.77935791015625, b=359.46029198473514, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9749436378479004, cells=[Cell(id=26, text='10.', bbox=BoundingBox(l=134.76501022108476, t=317.6438617538487, r=146.3162402400348, b=326.58331180320715, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-', bbox=BoundingBox(l=148.62648024382477, t=317.6438617538487, r=480.590120788418, b=326.58331180320715, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='scale video classification with convolutional neural networks. In: Computer Vision', bbox=BoundingBox(l=150.95401024764317, t=328.6028418143578, r=480.59476078842556, b=337.54232186371655, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725-1732. IEEE', bbox=BoundingBox(l=150.95401024764317, t=339.56179187486697, r=480.5947307884256, b=348.5013119242259, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='(2014)', bbox=BoundingBox(l=150.95401024764317, t=350.5207819353762, r=174.85843028685886, b=359.46029198473514, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='10. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Largescale video classification with convolutional neural networks. In: Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725-1732. IEEE (2014)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=9, page_no=10, cluster=Cluster(id=9, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.47573852539062, t=360.74285888671875, r=480.59479078842566, b=392.3362721662576, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9674268960952759, cells=[Cell(id=31, text='11.', bbox=BoundingBox(l=134.76501022108476, t=361.4797619958855, r=145.9576402394465, b=370.4192820452444, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convo-', bbox=BoundingBox(l=148.19617024311887, t=361.4797619958855, r=480.5902407884182, b=370.4192820452444, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='lutional neural networks. In: Advances in neural information processing systems.', bbox=BoundingBox(l=150.95401024764317, t=372.4387520563948, r=480.59479078842566, b=381.37826210575366, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='pp. 1097-1105 (2012)', bbox=BoundingBox(l=150.95401024764317, t=383.39675211689865, r=232.70963038176492, b=392.3362721662576, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='11. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems. pp. 1097-1105 (2012)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=10, page_no=10, cluster=Cluster(id=10, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.62811279296875, t=393.55804443359375, r=480.59467078842545, b=425.21322234778535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9672126173973083, cells=[Cell(id=35, text='12.', bbox=BoundingBox(l=134.76501022108476, t=394.355742177408, r=145.86594023929607, b=403.2952522267668, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel,', bbox=BoundingBox(l=148.08614024293834, t=394.355742177408, r=480.5900307884179, b=403.2952522267668, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='L.D.: Backpropagation applied to handwritten zip code recognition. Neural compu-', bbox=BoundingBox(l=150.95401024764317, t=405.3147222379172, r=480.59467078842545, b=414.25424228727616, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='tation 1(4), 541-551 (1989)', bbox=BoundingBox(l=150.95401024764317, t=416.2737122984265, r=253.19788041537635, b=425.21322234778535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='12. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural computation 1(4), 541-551 (1989)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=11, page_no=10, cluster=Cluster(id=11, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.47447204589844, t=426.6608581542969, r=480.59009078841797, b=447.1311924688039, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9590675234794617, cells=[Cell(id=39, text='13.', bbox=BoundingBox(l=134.76501022108476, t=427.2326923589357, r=145.78894023916976, b=436.1722124082947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to doc-', bbox=BoundingBox(l=147.99373024278677, t=427.2326923589357, r=480.59009078841797, b=436.1722124082947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='ument recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998)', bbox=BoundingBox(l=150.95401024764317, t=438.1916824194451, r=420.4302706897245, b=447.1311924688039, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='13. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=12, page_no=10, cluster=Cluster(id=12, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.4318084716797, t=448.2698059082031, r=480.5900607884179, b=469.0491625898225, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9524788856506348, cells=[Cell(id=42, text='14.', bbox=BoundingBox(l=134.76501022108476, t=449.1506624799543, r=146.421830240208, b=458.0901725293132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='Nebauer, C.: Evaluation of convolutional neural networks for visual recognition.', bbox=BoundingBox(l=148.75319024403268, t=449.1506624799543, r=480.5900607884179, b=458.0901725293132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='Neural Networks, IEEE Transactions on 9(4), 685-696 (1998)', bbox=BoundingBox(l=150.95401024764317, t=460.1096525404636, r=387.7297706360787, b=469.0491625898225, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='14. Nebauer, C.: Evaluation of convolutional neural networks for visual recognition. Neural Networks, IEEE Transactions on 9(4), 685-696 (1998)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=13, page_no=10, cluster=Cluster(id=13, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.57110595703125, t=470.0508117675781, r=480.6988525390625, b=490.96713271084104, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9601881504058838, cells=[Cell(id=45, text='15.', bbox=BoundingBox(l=134.76501022108476, t=471.0686326009729, r=146.22498023988507, b=480.00814265033176, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural net-', bbox=BoundingBox(l=148.51697024364515, t=471.0686326009729, r=480.590120788418, b=480.00814265033176, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='works applied to visual document analysis. In: null. p. 958. IEEE (2003)', bbox=BoundingBox(l=150.95401024764317, t=482.0276126614821, r=431.96100070864094, b=490.96713271084104, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='15. Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural networks applied to visual document analysis. In: null. p. 958. IEEE (2003)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=14, page_no=10, cluster=Cluster(id=14, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.7103729248047, t=492.13946533203125, r=480.5900607884179, b=512.8841228318543, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9577903747558594, cells=[Cell(id=48, text='16.', bbox=BoundingBox(l=134.76501022108476, t=492.98562272198603, r=145.95445023944131, b=501.9251427713449, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of', bbox=BoundingBox(l=148.1923402431126, t=492.98562272198603, r=480.5900607884179, b=501.9251427713449, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='Toronto (2013)', bbox=BoundingBox(l=150.95401024764317, t=503.9446127824953, r=207.980320341196, b=512.8841228318543, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='16. Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of Toronto (2013)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=15, page_no=10, cluster=Cluster(id=15, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.64859008789062, t=514.1981811523438, r=480.5948207884257, b=545.761073013382, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9668182134628296, cells=[Cell(id=51, text='17.', bbox=BoundingBox(l=134.76501022108476, t=514.9035928430046, r=145.9241202393915, b=523.8431028923634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with con-', bbox=BoundingBox(l=148.15594024305284, t=514.9035928430046, r=480.590150788418, b=523.8431028923634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='volutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings.', bbox=BoundingBox(l=150.95401024764317, t=525.8625729035139, r=480.5948207884257, b=534.8020929528727, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='IEEE. pp. 224-229. IEEE (2005)', bbox=BoundingBox(l=150.95401024764317, t=536.8215629640231, r=271.6238104456045, b=545.761073013382, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='17. Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with convolutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings. IEEE. pp. 224-229. IEEE (2005)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=16, page_no=10, cluster=Cluster(id=16, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.76501022108476, t=546.84033203125, r=480.5900307884179, b=567.6790431344007, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9606278538703918, cells=[Cell(id=55, text='18.', bbox=BoundingBox(l=134.76501022108476, t=547.7805430245323, r=146.3605302401075, b=556.7200630738914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In:', bbox=BoundingBox(l=148.67964024391202, t=547.7805430245323, r=480.5900307884179, b=556.7200630738914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='Advances in Neural Information Processing Systems. pp. 2553-2561 (2013)', bbox=BoundingBox(l=150.95401024764317, t=558.7395330850417, r=445.8409707314113, b=567.6790431344007, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='18. Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In: Advances in Neural Information Processing Systems. pp. 2553-2561 (2013)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=17, page_no=10, cluster=Cluster(id=17, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.6785888671875, t=568.6400146484375, r=480.60009765625, b=600.720947265625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9559718370437622, cells=[Cell(id=58, text='19.', bbox=BoundingBox(l=134.76501022108476, t=569.6985131455509, r=146.7078102406772, b=578.6380331949099, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks', bbox=BoundingBox(l=149.09637024459568, t=569.6985131455509, r=480.590150788418, b=578.6380331949099, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='(siconnets) and their application of face detection. In: Neural Networks, 2003. Pro-', bbox=BoundingBox(l=150.95401024764317, t=580.6575332060604, r=480.59479078842566, b=589.5970132554191, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='ceedings of the International Joint Conference on. vol. 3, pp. 2157-2162. IEEE (2003)', bbox=BoundingBox(l=150.95401024764317, t=591.6165132665697, r=480.59479078842566, b=600.5560133159285, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='19. Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks (siconnets) and their application of face detection. In: Neural Networks, 2003. Proceedings of the International Joint Conference on. vol. 3, pp. 2157-2162. IEEE (2003)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=18, page_no=10, cluster=Cluster(id=18, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.3703155517578, t=601.6026611328125, r=480.59009078841797, b=622.4730234369417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.958541750907898, cells=[Cell(id=62, text='20.', bbox=BoundingBox(l=134.76501022108476, t=602.5745233270735, r=146.06206023961784, b=611.5140233764323, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional', bbox=BoundingBox(l=148.32147024332446, t=602.5745233270735, r=480.59009078841797, b=611.5140233764323, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='neural networks. arXiv preprint arXiv:1301.3557 (2013)', bbox=BoundingBox(l=150.95401024764317, t=613.5335233875829, r=367.45676060282034, b=622.4730234369417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='20. Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional neural networks. arXiv preprint arXiv:1301.3557 (2013)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=19, page_no=10, cluster=Cluster(id=19, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.26535034179688, t=623.5272216796875, r=480.590150788418, b=644.814208984375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9669718146324158, cells=[Cell(id=65, text='21.', bbox=BoundingBox(l=134.76501022108476, t=624.4925234480922, r=146.0039102395224, b=633.4320234974512, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In:', bbox=BoundingBox(l=148.25168024320993, t=624.4925234480922, r=480.590150788418, b=633.4320234974512, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='Computer Vision-ECCV 2014, pp. 818-833. Springer (2014)', bbox=BoundingBox(l=150.95401024764317, t=635.4515235086017, r=384.4032306306214, b=644.3910235579605, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='21. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In: Computer Vision-ECCV 2014, pp. 818-833. Springer (2014)')], headers=[TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=10, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=256.0866394042969, t=93.61783599853516, r=447.5473937988281, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9234234094619751, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=10, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=471.0235290527344, t=93.66058349609375, r=480.5921307884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.889333963394165, cells=[Cell(id=1, text='11', bbox=BoundingBox(l=471.6257307737117, t=94.48107052167063, r=480.5921307884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='11')]))] assembled=AssembledUnit(elements=[TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=0, page_no=0, cluster=Cluster(id=0, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=137.64463806152344, t=116.22373962402344, r=476.82874078224734, b=130.95611072306508, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8532029986381531, cells=[Cell(id=0, text='An Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=138.53101022726298, t=116.98291064591297, r=476.82874078224734, b=130.95611072306508, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='An Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=1, page_no=0, cluster=Cluster(id=1, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=235.75559997558594, t=160.30120088509193, r=378.7718811035156, b=171.6171875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7410858869552612, cells=[Cell(id=1, text='Keiron O’Shea', bbox=BoundingBox(l=236.21001038750742, t=161.63281089244435, r=299.9806504921245, b=171.565490947287, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=2, text='1', bbox=BoundingBox(l=299.981020492125, t=160.30120088509193, r=303.9526104986405, b=166.49395091928477, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='and Ryan Nash', bbox=BoundingBox(l=306.94101050354305, t=161.63281089244435, r=374.67673061466496, b=171.565490947287, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='2', bbox=BoundingBox(l=374.6760306146638, t=160.30120088509193, r=378.6476106211792, b=166.49395091928477, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea 1 and Ryan Nash 2'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=2, page_no=0, cluster=Cluster(id=2, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=140.86863708496094, t=181.55941772460938, r=474.033690777662, b=202.1148223876953, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.6404754519462585, cells=[Cell(id=5, text='1', bbox=BoundingBox(l=141.3270302318499, t=181.6796210031315, r=144.97993023784258, b=186.9877310324398, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB', bbox=BoundingBox(l=149.96103024601416, t=183.22613101167042, r=474.033690777662, b=192.1655810610289, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='keo7@aber.ac.uk', bbox=BoundingBox(l=267.3290404385588, t=194.99108107662983, r=348.0266405709449, b=201.9131411148494, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB keo7@aber.ac.uk'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=3, page_no=0, cluster=Cluster(id=3, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=139.36328125, t=203.46005249023438, r=475.54037078013374, b=225.04162124255174, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.6642215251922607, cells=[Cell(id=8, text='2', bbox=BoundingBox(l=139.82004022937767, t=203.5966811241451, r=143.47295023537032, b=208.9047811534533, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='School of Computing and Communications, Lancaster University, Lancashire, LA1', bbox=BoundingBox(l=148.45503024354352, t=205.14318113268382, r=475.54037078013374, b=214.0826411820425, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='4YW', bbox=BoundingBox(l=297.96301048881446, t=216.10217119319316, r=317.39319052069004, b=225.04162124255174, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2 School of Computing and Communications, Lancaster University, Lancashire, LA1 4YW'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=4, page_no=0, cluster=Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=245.39329528808594, t=226.80148315429688, r=369.5463306062484, b=234.79016129637762, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7631615996360779, cells=[Cell(id=11, text='nashrd@live.lancs.ac.uk', bbox=BoundingBox(l=245.8100104032564, t=227.86810125815794, r=369.5463306062484, b=234.79016129637762, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='nashrd@live.lancs.ac.uk'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=0, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=162.13299560546875, t=253.8007049560547, r=452.33795166015625, b=351.5365219409846, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9657419919967651, cells=[Cell(id=12, text='Abstract.', bbox=BoundingBox(l=163.11102026758698, t=254.98889140790345, r=199.71187032763143, b=263.72216145612356, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='The field of machine learning has taken a dramatic twist in re-', bbox=BoundingBox(l=204.6930203358031, t=254.926141407557, r=452.2463107419194, b=263.86560145691567, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='cent times, with the rise of the Artificial Neural Network (ANN). These', bbox=BoundingBox(l=163.11102026758698, t=265.88415146806096, r=452.2416707419117, b=274.82360151741955, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='biologically inspired computational models are able to far exceed the per-', bbox=BoundingBox(l=163.11102026758698, t=276.8431315285702, r=452.2416107419117, b=285.78259157792877, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='formance of previous forms of artificial intelligence in common machine', bbox=BoundingBox(l=163.11102026758698, t=287.80212158907943, r=452.2415207419116, b=296.7415716384379, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='learning tasks. One of the most impressive forms of ANN architecture is', bbox=BoundingBox(l=163.11102026758698, t=298.7611016495888, r=452.2415207419116, b=307.70056169894735, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='that of the Convolutional Neural Network (CNN). CNNs are primarily', bbox=BoundingBox(l=163.11102026758698, t=309.720091710098, r=452.2415807419116, b=318.6595417594566, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='used to solve difficult image-driven pattern recognition tasks and with', bbox=BoundingBox(l=163.11102026758698, t=320.67907177060727, r=452.2415207419116, b=329.6185318199658, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='their precise yet simple architecture, offers a simplified method of getting', bbox=BoundingBox(l=163.11102026758698, t=331.6380318311164, r=452.2415507419116, b=340.5775418804753, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='started with ANNs.', bbox=BoundingBox(l=163.11102026758698, t=342.5970118916257, r=241.19044039567788, b=351.5365219409846, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Abstract. The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=0, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=162.31292724609375, t=359.31500244140625, r=452.2415507419116, b=402.4580993652344, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9703543782234192, cells=[Cell(id=22, text='This document provides a brief introduction to CNNs, discussing recently', bbox=BoundingBox(l=163.11102026758698, t=360.5400019906967, r=452.2414607419114, b=369.47952204005566, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='published papers and newly formed techniques in developing these bril-', bbox=BoundingBox(l=163.11102026758698, t=371.49899205120596, r=452.2415507419116, b=380.43850210056485, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='liantly fantastic image recognition models. This introduction assumes you', bbox=BoundingBox(l=163.11102026758698, t=382.45700211170987, r=452.2415507419116, b=391.3965121610688, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='are familiar with the fundamentals of ANNs and machine learning.', bbox=BoundingBox(l=163.11102026758698, t=393.41598217221906, r=429.83463070515256, b=402.355492221578, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=0, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=162.44024658203125, t=421.299072265625, r=452.4108907421894, b=442.73138427734375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9514586925506592, cells=[Cell(id=26, text='Keywords:', bbox=BoundingBox(l=163.11102026758698, t=422.3807623321461, r=207.19881033991393, b=431.1140423803664, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='Pattern recognition, artificial neural networks, machine learn-', bbox=BoundingBox(l=211.6810303472671, t=422.31799233179953, r=452.4108907421894, b=431.2575023811584, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='ing, image analysis', bbox=BoundingBox(l=163.11102026758698, t=433.27700239230904, r=238.93089039197108, b=442.21652244166796, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keywords: Pattern recognition, artificial neural networks, machine learning, image analysis'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=8, page_no=0, cluster=Cluster(id=8, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.2177734375, t=471.4940490722656, r=221.76300036380678, b=483.745082670965, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9292116761207581, cells=[Cell(id=29, text='1', bbox=BoundingBox(l=134.76501022108476, t=472.10070260667135, r=140.74261023089116, b=483.745082670965, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='Introduction', bbox=BoundingBox(l=152.6978102505039, t=472.10070260667135, r=221.76300036380678, b=483.745082670965, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='1 Introduction'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=9, page_no=0, cluster=Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.661865234375, t=505.0313415527344, r=480.72076416015625, b=576.1156616210938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9834169149398804, cells=[Cell(id=31, text='Artificial Neural Networks', bbox=BoundingBox(l=134.76501022108476, t=506.41946279616, r=257.86292042302944, b=516.1230428497377, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='(ANNs) are computational processing systems of', bbox=BoundingBox(l=261.0410204282432, t=506.349732795775, r=480.58688078841266, b=516.2824428506178, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='which are heavily inspired by way biological nervous systems (such as the hu-', bbox=BoundingBox(l=134.76501022108476, t=518.3047128617836, r=480.58670078841243, b=528.2374229166264, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='man brain) operate. ANNs are mainly comprised of a high number of intercon-', bbox=BoundingBox(l=134.76501022108476, t=530.2597029277922, r=480.58676078841245, b=540.192412982635, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='nected computational nodes (referred to as neurons), of which work entwine in', bbox=BoundingBox(l=134.76501022108476, t=542.2146929938009, r=480.58682078841264, b=552.1474030486437, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='a distributed fashion to collectively learn from the input in order to optimise its', bbox=BoundingBox(l=134.76501022108476, t=554.1696730598095, r=480.58679078841254, b=564.1023831146523, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='final output.', bbox=BoundingBox(l=134.76501022108476, t=566.1256731258238, r=189.3899503106981, b=576.0583831806665, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Artificial Neural Networks (ANNs) are computational processing systems of which are heavily inspired by way biological nervous systems (such as the human brain) operate. ANNs are mainly comprised of a high number of interconnected computational nodes (referred to as neurons), of which work entwine in a distributed fashion to collectively learn from the input in order to optimise its final output.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=10, page_no=0, cluster=Cluster(id=10, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.71791076660156, t=583.977294921875, r=480.8759460449219, b=667.2293701171875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9865064024925232, cells=[Cell(id=38, text='The basic structure of a ANN can be modelled as shown in Figure 1. We would', bbox=BoundingBox(l=134.76501022108476, t=585.064663230394, r=480.5867307884124, b=594.9973732852368, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='load the input, usually in the form of a multidimensional vector to the input', bbox=BoundingBox(l=134.76501022108476, t=597.0196632964028, r=480.5867307884124, b=606.9523733512456, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='layer of which will distribute it to the hidden layers. The hidden layers will then', bbox=BoundingBox(l=134.76501022108476, t=608.9746733624115, r=480.5867307884124, b=618.9073734172542, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='make decisions from the previous layer and weigh up how a stochastic change', bbox=BoundingBox(l=134.76501022108476, t=620.9296734284203, r=480.58679078841254, b=630.8623834832631, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='within itself detriments or improves the final output, and this is referred to as', bbox=BoundingBox(l=134.76501022108476, t=632.884673494429, r=480.5867307884124, b=642.8173835492717, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='the process of learning. Having multiple hidden layers stacked upon each-other', bbox=BoundingBox(l=134.76501022108476, t=644.8406635604431, r=480.5867307884124, b=654.7733736152859, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='is commonly called deep learning.', bbox=BoundingBox(l=134.76501022108476, t=656.7956636264519, r=285.7780804688248, b=666.7283736812946, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The basic structure of a ANN can be modelled as shown in Figure 1. We would load the input, usually in the form of a multidimensional vector to the input layer of which will distribute it to the hidden layers. The hidden layers will then make decisions from the previous layer and weigh up how a stochastic change within itself detriments or improves the final output, and this is referred to as the process of learning. Having multiple hidden layers stacked upon each-other is commonly called deep learning.'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=11, page_no=0, cluster=Cluster(id=11, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=17.202451705932617, t=264.95001146290315, r=36.33979405961618, b=604.8900433398585, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8815839290618896, cells=[Cell(id=45, text='arXiv:1511.08458v2 [cs.NE] 2 Dec 2015', bbox=BoundingBox(l=18.3402140300875, t=264.95001146290315, r=36.33979405961618, b=604.8900433398585, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='arXiv:1511.08458v2 [cs.NE] 2 Dec 2015'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=1, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=133.6807861328125, t=93.64701080322266, r=139.58908081054688, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7250283360481262, cells=[Cell(id=0, text='2', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=1, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=167.17018127441406, t=93.5260238647461, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.6552881002426147, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea et al.'), TextElement(label=<DocItemLabel.CAPTION: 'caption'>, id=2, page_no=1, cluster=Cluster(id=2, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=133.7849884033203, t=278.09375, r=480.58679078841254, b=336.2594918566335, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9529556035995483, cells=[Cell(id=2, text='Fig. 1: A simple three layered feedforward neural network (FNN), comprised', bbox=BoundingBox(l=134.76500022108476, t=278.50677153775587, r=480.58679078841254, b=288.4394515925984, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='of a input layer, a hidden layer and an output layer. This structure is the basis', bbox=BoundingBox(l=134.76500022108476, t=290.4617916037647, r=480.5866407884123, b=300.39447165860736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='of a number of common ANN architectures, included but not limited to Feed-', bbox=BoundingBox(l=134.76500022108476, t=302.41680166977346, r=480.58676078841245, b=312.349481724616, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='forward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and', bbox=BoundingBox(l=134.76500022108476, t=314.3718217357823, r=480.58676078841245, b=324.30450179062484, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='Recurrent Neural Networks (RNNs).', bbox=BoundingBox(l=134.76500022108476, t=326.32678180179073, r=296.7867704868848, b=336.2594918566335, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Fig. 1: A simple three layered feedforward neural network (FNN), comprised of a input layer, a hidden layer and an output layer. This structure is the basis of a number of common ANN architectures, included but not limited to Feedforward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and Recurrent Neural Networks (RNNs).'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=1, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.74380493164062, t=365.6249084472656, r=480.59271078842227, b=447.6732482910156, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9868213534355164, cells=[Cell(id=7, text='The two key learning paradigms in image processing tasks are supervised and', bbox=BoundingBox(l=134.76500022108476, t=365.8777720201687, r=480.58679078841254, b=375.8104820750116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='unsupervised learning.', bbox=BoundingBox(l=134.76500022108476, t=377.8327620861774, r=237.7583603900475, b=387.76547214102015, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='Supervised learning', bbox=BoundingBox(l=241.2870003958363, t=377.90249208656246, r=334.47714054871665, b=387.6060721401401, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='is learning through pre-labelled', bbox=BoundingBox(l=338.00800055450907, t=377.8327620861774, r=480.59271078842227, b=387.76547214102015, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='inputs, which act as targets. For each training example there will be a set of', bbox=BoundingBox(l=134.76500022108476, t=389.78775215218604, r=480.58676078841245, b=399.72045220702887, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='input values (vectors) and one or more associated designated output values.', bbox=BoundingBox(l=134.76500022108476, t=401.7427322181947, r=480.58679078841254, b=411.6754422730375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='The goal of this form of training is to reduce the models overall classification', bbox=BoundingBox(l=134.76500022108476, t=413.69772228420334, r=480.58679078841254, b=423.6304323390461, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='error, through correct calculation of the output value of training example by', bbox=BoundingBox(l=134.76500022108476, t=425.65271235021197, r=480.58676078841245, b=435.58541240505474, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='training.', bbox=BoundingBox(l=134.76500022108476, t=437.6087024162262, r=172.35388028275008, b=447.54141247106895, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The two key learning paradigms in image processing tasks are supervised and unsupervised learning. Supervised learning is learning through pre-labelled inputs, which act as targets. For each training example there will be a set of input values (vectors) and one or more associated designated output values. The goal of this form of training is to reduce the models overall classification error, through correct calculation of the output value of training example by training.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=4, page_no=1, cluster=Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.91444396972656, t=458.1381530761719, r=480.59070078841904, b=517.1390380859375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9838359951972961, cells=[Cell(id=16, text='Unsupervised learning', bbox=BoundingBox(l=134.76500022108476, t=458.9344425339748, r=239.35237039266252, b=468.63803258755246, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='differs in that the training set does not include any la-', bbox=BoundingBox(l=242.10600039717988, t=458.8647125335898, r=480.59070078841904, b=468.79742258843254, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='bels. Success is usually determined by whether the network is able to reduce or', bbox=BoundingBox(l=134.76500022108476, t=470.81970259959843, r=480.5867307884124, b=480.7524126544412, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='increase an associated cost function. However, it is important to note that most', bbox=BoundingBox(l=134.76500022108476, t=482.77569266561255, r=480.58679078841254, b=492.70840272045535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='image-focused pattern-recognition tasks usually depend on classification using', bbox=BoundingBox(l=134.76500022108476, t=494.7306827316212, r=480.5867307884124, b=504.663392786464, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='supervised learning.', bbox=BoundingBox(l=134.76500022108476, t=506.6856627976299, r=224.9065903689639, b=516.6183728524726, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Unsupervised learning differs in that the training set does not include any labels. Success is usually determined by whether the network is able to reduce or increase an associated cost function. However, it is important to note that most image-focused pattern-recognition tasks usually depend on classification using supervised learning.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=1, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.6765899658203, t=527.3146362304688, r=480.7476806640625, b=621.6336669921875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9860644340515137, cells=[Cell(id=22, text='Convolutional Neural Networks', bbox=BoundingBox(l=134.76500022108476, t=528.012392915384, r=283.65607046534365, b=537.7159729689615, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='(CNNs) are analogous to traditional ANNs', bbox=BoundingBox(l=286.989990470813, t=527.9426529149989, r=480.593200788423, b=537.8753629698416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='in that they are comprised of neurons that self-optimise through learning. Each', bbox=BoundingBox(l=134.76498022108473, t=539.8976429810075, r=480.5867307884124, b=549.8303530358503, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='neuron will still receive an input and perform a operation (such as a scalar', bbox=BoundingBox(l=134.76498022108473, t=551.8526330470162, r=480.58667078841233, b=561.7853331018589, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='product followed by a non-linear function) - the basis of countless ANNs. From', bbox=BoundingBox(l=134.76498022108473, t=563.8076131130247, r=480.5866407884123, b=573.7403231678675, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='the input raw image vectors to the final output of the class score, the entire of', bbox=BoundingBox(l=134.76498022108473, t=575.7626031790335, r=480.5866407884123, b=585.695313233876, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='the network will still express a single perceptive score function (the weight).', bbox=BoundingBox(l=134.76498022108473, t=587.7185932450475, r=480.58688078841266, b=597.6513032998903, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='The last layer will contain loss functions associated with the classes, and all of', bbox=BoundingBox(l=134.76498022108473, t=599.6735933110563, r=480.58682078841264, b=609.6063033658991, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='the regular tips and tricks developed for traditional ANNs still apply.', bbox=BoundingBox(l=134.76498022108473, t=611.628603377065, r=439.90955072168066, b=621.5613134319078, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Convolutional Neural Networks (CNNs) are analogous to traditional ANNs in that they are comprised of neurons that self-optimise through learning. Each neuron will still receive an input and perform a operation (such as a scalar product followed by a non-linear function) - the basis of countless ANNs. From the input raw image vectors to the final output of the class score, the entire of the network will still express a single perceptive score function (the weight). The last layer will contain loss functions associated with the classes, and all of the regular tips and tricks developed for traditional ANNs still apply.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=1, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.68446350097656, t=631.9942626953125, r=480.5868507884127, b=666.78271484375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9788795113563538, cells=[Cell(id=31, text='The only notable difference between CNNs and traditional ANNs is that CNNs', bbox=BoundingBox(l=134.76498022108473, t=632.885603494434, r=480.5868507884127, b=642.8183135492768, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='are primarily used in the field of pattern recognition within images. This allows', bbox=BoundingBox(l=134.76498022108473, t=644.8406035604428, r=480.58679078841254, b=654.7733136152856, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='us to encode image-specific features into the architecture, making the network', bbox=BoundingBox(l=134.76498022108473, t=656.7956036264516, r=480.58682078841264, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The only notable difference between CNNs and traditional ANNs is that CNNs are primarily used in the field of pattern recognition within images. This allows us to encode image-specific features into the architecture, making the network'), FigureElement(label=<DocItemLabel.PICTURE: 'picture'>, id=7, page_no=1, cluster=Cluster(id=7, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=193.8206024169922, t=115.9693374633789, r=420.3355407714844, b=267.38494873046875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9722045063972473, cells=[]), text='', annotations=[], provenance=None, predicted_class=None, confidence=None), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=2, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=255.9642333984375, t=93.53244018554688, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9313411116600037, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=2, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=475.16705322265625, t=93.53336334228516, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.873039722442627, cells=[Cell(id=1, text='3', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='3'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=2, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=134.2552032470703, t=118.86754608154297, r=480.58670078841243, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.974609911441803, cells=[Cell(id=2, text='more suited for image-focused tasks - whilst further reducing the parameters', bbox=BoundingBox(l=134.76501022108476, t=119.67181066075966, r=480.58670078841243, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='required to set up the model.', bbox=BoundingBox(l=134.76501022108476, t=131.6268307267684, r=262.07709042994287, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='more suited for image-focused tasks - whilst further reducing the parameters required to set up the model.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=2, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.69602966308594, t=150.32054138183594, r=481.443480789818, b=244.74359135133477, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9850017428398132, cells=[Cell(id=4, text='One of the largest limitations of traditional forms of ANN is that they tend to', bbox=BoundingBox(l=134.76501022108476, t=151.12481083442503, r=480.5867307884124, b=161.0574908892678, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='struggle with the computational complexity required to compute image data.', bbox=BoundingBox(l=134.76501022108476, t=163.079830900434, r=480.58679078841254, b=173.01251095527653, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='Common machine learning benchmarking datasets such as the MNIST database', bbox=BoundingBox(l=134.76501022108476, t=175.03485096644295, r=481.443480789818, b=184.9675210212854, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='of handwritten digits are suitable for most forms of ANN, due to its relatively', bbox=BoundingBox(l=134.76501022108476, t=186.98986103245147, r=480.58670078841243, b=196.92254108729412, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='small image dimensionality of just', bbox=BoundingBox(l=134.76501022108476, t=198.94586109846568, r=286.20648046952766, b=208.87854115330833, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='28', bbox=BoundingBox(l=288.66400047355927, t=199.15502109962063, r=298.6265904899031, b=208.00183114846766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='×', bbox=BoundingBox(l=300.6990104933029, t=198.5971610965405, r=308.44791050601515, b=208.00183114846766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='28', bbox=BoundingBox(l=310.5200205094145, t=199.15502109962063, r=320.4826005257583, b=208.00183114846766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='. With this dataset a single neuron in', bbox=BoundingBox(l=320.48203052575735, t=198.94586109846568, r=480.59103078841946, b=208.87854115330833, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='the first hidden layer will contain', bbox=BoundingBox(l=134.7650302210848, t=210.90087116447455, r=280.3784504599666, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='784', bbox=BoundingBox(l=282.62103046364564, t=211.11004116562947, r=297.56494048816137, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='weights (', bbox=BoundingBox(l=299.80502049183633, t=210.90087116447455, r=340.1535305580289, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='28', bbox=BoundingBox(l=340.15204055802644, t=211.11004116562947, r=350.11462057437024, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='×', bbox=BoundingBox(l=351.40204057648225, t=210.55218116254923, r=359.15094058919453, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='28', bbox=BoundingBox(l=360.438050591306, t=211.11004116562947, r=370.4006306076499, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='×', bbox=BoundingBox(l=371.68704060976023, t=210.55218116254923, r=379.4359406224725, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='1', bbox=BoundingBox(l=380.723050624584, t=211.11004116562947, r=385.7043506327559, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='where', bbox=BoundingBox(l=387.9440606364302, t=210.90087116447455, r=415.35120068139224, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='1', bbox=BoundingBox(l=417.5920706850684, t=211.11004116562947, r=422.5733606932403, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='bare in mind', bbox=BoundingBox(l=424.81409069691625, t=210.90087116447455, r=480.59470078842554, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='that MNIST is normalised to just black and white values), which is manageable', bbox=BoundingBox(l=134.7650802210849, t=222.85589123048328, r=480.58676078841245, b=232.78857128532593, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='for most forms of ANN.', bbox=BoundingBox(l=134.7650802210849, t=234.81091129649212, r=240.3985103943787, b=244.74359135133477, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='One of the largest limitations of traditional forms of ANN is that they tend to struggle with the computational complexity required to compute image data. Common machine learning benchmarking datasets such as the MNIST database of handwritten digits are suitable for most forms of ANN, due to its relatively small image dimensionality of just 28 × 28 . With this dataset a single neuron in the first hidden layer will contain 784 weights ( 28 × 28 × 1 where 1 bare in mind that MNIST is normalised to just black and white values), which is manageable for most forms of ANN.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=4, page_no=2, cluster=Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.93231201171875, t=253.4578094482422, r=480.5915207884203, b=312.1922912597656, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9856253266334534, cells=[Cell(id=26, text='If you consider a more substantial coloured image input of', bbox=BoundingBox(l=134.7650802210849, t=254.30889140414888, r=391.39169064208613, b=264.2415714589914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='64', bbox=BoundingBox(l=393.7400806459387, t=254.5180614053039, r=403.70267066228257, b=263.36486145415074, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='×', bbox=BoundingBox(l=405.381070665036, t=253.96020140222367, r=413.12997067774825, b=263.36486145415074, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='64', bbox=BoundingBox(l=414.80908068050286, t=254.5180614053039, r=424.7716706968467, b=263.36486145415074, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text=', the number', bbox=BoundingBox(l=424.7710906968457, t=254.30889140414888, r=480.5915207884203, b=264.2415714589914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='of weights on just a single neuron of the first layer increases substantially to', bbox=BoundingBox(l=134.7650802210849, t=266.26391147015784, r=480.58682078841264, b=276.1965915250004, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='12', bbox=BoundingBox(l=134.7650802210849, t=278.42907153732676, r=144.72768023742873, b=287.2758715861738, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text=',', bbox=BoundingBox(l=144.72708023742774, t=278.42907153732676, r=147.49469024196807, b=287.2758715861738, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='288', bbox=BoundingBox(l=149.155080244692, t=278.42907153732676, r=164.09897026920777, b=287.2758715861738, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='. Also take into account that to deal with this scale of input, the network', bbox=BoundingBox(l=164.09908026920792, t=278.21991153617194, r=480.5909407884193, b=288.1525815910145, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='will also need to be a lot larger than one used to classify colour-normalised', bbox=BoundingBox(l=134.7650802210849, t=290.1749216021807, r=480.5868507884127, b=300.10760165702334, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='MNIST digits, then you will understand the drawbacks of using such models.', bbox=BoundingBox(l=134.7650802210849, t=302.12994166818953, r=476.6915007820223, b=312.0626217230322, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='If you consider a more substantial coloured image input of 64 × 64 , the number of weights on just a single neuron of the first layer increases substantially to 12 , 288 . Also take into account that to deal with this scale of input, the network will also need to be a lot larger than one used to classify colour-normalised MNIST digits, then you will understand the drawbacks of using such models.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=5, page_no=2, cluster=Cluster(id=5, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.107177734375, t=342.1416320800781, r=207.76548767089844, b=352.77398681640625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9495469331741333, cells=[Cell(id=38, text='1.1', bbox=BoundingBox(l=134.7650802210849, t=342.7606518925292, r=147.2183202415147, b=352.4642319461069, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='Overfitting', bbox=BoundingBox(l=157.1809202578585, t=342.7606518925292, r=207.5418703404767, b=352.4642319461069, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='1.1 Overfitting'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=2, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87643432617188, t=372.7414855957031, r=480.5868507884127, b=431.04357237997726, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9863153100013733, cells=[Cell(id=40, text='But why does it matter? Surely we could just increase the number of hidden lay-', bbox=BoundingBox(l=134.7650802210849, t=373.28991206109447, r=480.58679078841254, b=383.22262211593716, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='ers in our network, and perhaps increase the number of neurons within them?', bbox=BoundingBox(l=134.7650802210849, t=385.2449021271031, r=480.58682078841264, b=395.1776121819458, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='The simple answer to this question is no. This is down to two reasons, one be-', bbox=BoundingBox(l=134.7650802210849, t=397.1998921931118, r=480.58682078841264, b=407.13259224795445, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='ing the simple problem of not having unlimited computational power and time', bbox=BoundingBox(l=134.7650802210849, t=409.15487225912034, r=480.5868507884127, b=419.0875823139631, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='to train these huge ANNs.', bbox=BoundingBox(l=134.7650802210849, t=421.11087232513455, r=250.23160041051008, b=431.04357237997726, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='But why does it matter? Surely we could just increase the number of hidden layers in our network, and perhaps increase the number of neurons within them? The simple answer to this question is no. This is down to two reasons, one being the simple problem of not having unlimited computational power and time to train these huge ANNs.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=2, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.83572387695312, t=440.1903076171875, r=480.64300537109375, b=522.4312133789062, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9872767329216003, cells=[Cell(id=45, text='The second reason is stopping or reducing the effects of overfitting.', bbox=BoundingBox(l=134.7650802210849, t=440.6088524327913, r=427.9843107021171, b=450.54156248763405, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='Overfitting', bbox=BoundingBox(l=430.231080705803, t=440.6785824331763, r=480.5920107884211, b=450.382172486754, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='is basically when a network is unable to learn effectively due to a number of', bbox=BoundingBox(l=134.7650802210849, t=452.5638424988, r=480.5868507884127, b=462.49655255364274, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='reasons. It is an important concept of most, if not all machine learning algo-', bbox=BoundingBox(l=134.7650802210849, t=464.51882256480854, r=480.58682078841264, b=474.4515326196513, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='rithms and it is important that every precaution is taken as to reduce its effects.', bbox=BoundingBox(l=134.7650802210849, t=476.4738126308172, r=480.58691078841275, b=486.4065226856599, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='If our models were to exhibit signs of overfitting then we may see a reduced', bbox=BoundingBox(l=134.7650802210849, t=488.4288026968258, r=480.58691078841275, b=498.3615127516686, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='ability to pinpoint generalised features for not only our training dataset, but', bbox=BoundingBox(l=134.7650802210849, t=500.38479276284, r=480.58691078841275, b=510.3175028176828, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='also our test and prediction sets.', bbox=BoundingBox(l=134.7650802210849, t=512.3397828288487, r=276.7719704540501, b=522.2724928836915, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The second reason is stopping or reducing the effects of overfitting. Overfitting is basically when a network is unable to learn effectively due to a number of reasons. It is an important concept of most, if not all machine learning algorithms and it is important that every precaution is taken as to reduce its effects. If our models were to exhibit signs of overfitting then we may see a reduced ability to pinpoint generalised features for not only our training dataset, but also our test and prediction sets.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=2, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8554229736328, t=531.4302368164062, r=480.58688078841266, b=565.6804531233654, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9846621751785278, cells=[Cell(id=53, text='This is the main reason behind reducing the complexity of our ANNs. The less', bbox=BoundingBox(l=134.7650802210849, t=531.8377629365054, r=480.58688078841266, b=541.7704729913482, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='parameters required to train, the less likely the network will overfit - and of', bbox=BoundingBox(l=134.7650802210849, t=543.792753002514, r=480.58679078841254, b=553.7254630573568, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='course, improve the predictive performance of the model.', bbox=BoundingBox(l=134.7650802210849, t=555.7477430685227, r=388.2833906369869, b=565.6804531233654, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='This is the main reason behind reducing the complexity of our ANNs. The less parameters required to train, the less likely the network will overfit - and of course, improve the predictive performance of the model.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=9, page_no=2, cluster=Cluster(id=9, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.10931396484375, t=596.1439819335938, r=248.64112854003906, b=608.6050833603708, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9415203332901001, cells=[Cell(id=56, text='2', bbox=BoundingBox(l=134.7650802210849, t=596.9607232960773, r=140.74268023089127, b=608.6050833603708, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='CNN architecture', bbox=BoundingBox(l=152.697880250504, t=596.9607232960773, r=248.63835040789635, b=608.6050833603708, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2 CNN architecture'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=10, page_no=2, cluster=Cluster(id=10, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.93942260742188, t=631.9437255859375, r=480.8302001953125, b=666.7557373046875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9827980399131775, cells=[Cell(id=58, text='As noted earlier, CNNs primarily focus on the basis that the input will be com-', bbox=BoundingBox(l=134.7650802210849, t=632.8857434944348, r=480.58682078841264, b=642.8184535492776, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='prised of images. This focuses the architecture to be set up in way to best suit', bbox=BoundingBox(l=134.7650802210849, t=644.8407435604436, r=480.58679078841254, b=654.7734536152864, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='the need for dealing with the specific type of data.', bbox=BoundingBox(l=134.7650802210849, t=656.7957436264522, r=355.0182505824148, b=666.728453681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='As noted earlier, CNNs primarily focus on the basis that the input will be comprised of images. This focuses the architecture to be set up in way to best suit the need for dealing with the specific type of data.'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=3, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=133.68788146972656, t=93.82508850097656, r=139.39495849609375, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.6813156604766846, cells=[Cell(id=0, text='4', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='4'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=1, page_no=3, cluster=Cluster(id=1, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=-1.0, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea et al.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=3, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.76507568359375, t=118.65596771240234, r=480.6532897949219, b=189.66046142578125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.986056923866272, cells=[Cell(id=2, text='One of the key differences is that the neurons that the layers within the CNN', bbox=BoundingBox(l=134.76500022108476, t=119.67181066075966, r=480.58676078841245, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='are comprised of neurons organised into three dimensions, the spatial dimen-', bbox=BoundingBox(l=134.76500022108476, t=131.6268307267684, r=480.58676078841245, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='sionality of the input (', bbox=BoundingBox(l=134.76500022108476, t=143.58184079277714, r=231.18303037926052, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='height', bbox=BoundingBox(l=231.18500037926376, t=143.65155079316196, r=260.51489042738007, b=153.35510084673967, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='and the', bbox=BoundingBox(l=262.6799904309319, t=143.58184079277714, r=295.5266704848176, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='width', bbox=BoundingBox(l=297.6950104883748, t=143.65155079316196, r=324.80325053284645, b=153.35510084673967, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text=') and the', bbox=BoundingBox(l=324.803010532846, t=143.58184079277714, r=363.13907059573717, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='depth', bbox=BoundingBox(l=365.30103059928393, t=143.65155079316196, r=391.8613306428566, b=153.35510084673967, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='. The depth does not', bbox=BoundingBox(l=391.86203064285775, t=143.58184079277714, r=480.5889907884162, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='refer to the total number of layers within the ANN, but the third dimension of a', bbox=BoundingBox(l=134.76505022108483, t=155.53686085878599, r=480.58688078841266, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='activation volume. Unlike standard ANNS, the neurons within any given layer', bbox=BoundingBox(l=134.76505022108483, t=167.49188092479483, r=480.58679078841254, b=177.42456097963736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='will only connect to a small region of the layer preceding it.', bbox=BoundingBox(l=134.76505022108483, t=179.44787099080895, r=395.79517064931014, b=189.3805510456516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='One of the key differences is that the neurons that the layers within the CNN are comprised of neurons organised into three dimensions, the spatial dimensionality of the input ( height and the width ) and the depth . The depth does not refer to the total number of layers within the ANN, but the third dimension of a activation volume. Unlike standard ANNS, the neurons within any given layer will only connect to a small region of the layer preceding it.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=3, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.84022521972656, t=196.7172088623047, r=480.62689208984375, b=267.8779602050781, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9875405430793762, cells=[Cell(id=14, text='In practice this would mean that for the example given earlier, the input ’vol-', bbox=BoundingBox(l=134.76505022108483, t=197.79687109212182, r=480.58679078841254, b=207.72955114696435, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='ume’ will have a dimensionality of', bbox=BoundingBox(l=134.76505022108483, t=209.75189115813055, r=287.34225047139086, b=219.6845712129732, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='64', bbox=BoundingBox(l=289.74603047533435, t=209.96105115928538, r=299.7086204916782, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='×', bbox=BoundingBox(l=301.60602049479087, t=209.40319115620525, r=309.35492050750315, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='64', bbox=BoundingBox(l=311.2510105106137, t=209.96105115928538, r=321.21359052695755, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='×', bbox=BoundingBox(l=323.1109905300703, t=209.40319115620525, r=330.85989054278247, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='3', bbox=BoundingBox(l=332.75699054589467, t=209.96105115928538, r=337.7382805540666, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='(height, width and depth), lead-', bbox=BoundingBox(l=340.14297055801154, t=209.75189115813055, r=480.5957307884272, b=219.6845712129732, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='ing to a final output layer comprised of a dimensionality of', bbox=BoundingBox(l=134.7649702210847, t=221.7069012241392, r=401.304350658348, b=231.63958127898184, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='1', bbox=BoundingBox(l=404.45795066352156, t=221.91607122529422, r=409.4392406716935, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='×', bbox=BoundingBox(l=412.142940676129, t=221.3582112222141, r=419.8918506888412, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='1', bbox=BoundingBox(l=422.59595069327736, t=221.91607122529422, r=427.5772407014493, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='×', bbox=BoundingBox(l=430.2819507058864, t=221.3582112222141, r=438.03085071859863, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='n', bbox=BoundingBox(l=440.7339507230331, t=221.91607122529422, r=446.7135007328427, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='(where', bbox=BoundingBox(l=449.86694073801596, t=221.7069012241392, r=480.59161078842044, b=231.63958127898184, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='n', bbox=BoundingBox(l=134.7649502210847, t=233.87207129130843, r=140.74451023089426, b=242.71887134015537, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='represents the possible number of classes) as we would have condensed the', bbox=BoundingBox(l=143.63596023563775, t=233.6629012901535, r=480.59103078841946, b=243.59558134499605, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='full input dimensionality into a smaller volume of class scores filed across the', bbox=BoundingBox(l=134.7649502210847, t=245.61792135616224, r=480.58670078841243, b=255.55059141100492, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='depth dimension.', bbox=BoundingBox(l=134.7649502210847, t=257.5729314221711, r=212.35367034837057, b=267.50561147701364, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='In practice this would mean that for the example given earlier, the input ’volume’ will have a dimensionality of 64 × 64 × 3 (height, width and depth), leading to a final output layer comprised of a dimensionality of 1 × 1 × n (where n represents the possible number of classes) as we would have condensed the full input dimensionality into a smaller volume of class scores filed across the depth dimension.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=4, page_no=3, cluster=Cluster(id=4, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.0306854248047, t=294.2342224121094, r=247.65118040627686, b=304.46118168106136, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9402409791946411, cells=[Cell(id=33, text='2.1', bbox=BoundingBox(l=134.7649502210847, t=294.7576216274838, r=147.2182002415145, b=304.46118168106136, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='Overall architecture', bbox=BoundingBox(l=157.18080025785835, t=294.7576216274838, r=247.65118040627686, b=304.46118168106136, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2.1 Overall architecture'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=3, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.74267578125, t=321.2259826660156, r=480.75616455078125, b=367.63958202989653, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9827728271484375, cells=[Cell(id=35, text='CNNs are comprised of three types of layers. These are convolutional layers,', bbox=BoundingBox(l=134.7649502210847, t=321.8409417770224, r=480.5866407884123, b=331.77362183186506, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='pooling layers and', bbox=BoundingBox(l=134.7649502210847, t=333.7969018430365, r=219.5566303601872, b=343.7296118978793, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='fully-connected layers', bbox=BoundingBox(l=223.34396036640038, t=333.86663184342154, r=325.6299705342027, b=343.57022189699916, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='. When these layers are stacked, a', bbox=BoundingBox(l=325.6299705342027, t=333.7969018430365, r=480.5883207884151, b=343.7296118978793, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='CNN architecture has been formed. A simplified CNN architecture for MNIST', bbox=BoundingBox(l=134.7649702210847, t=345.7518919090451, r=480.58682078841264, b=355.684601963888, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='classification is illustrated in Figure 2.', bbox=BoundingBox(l=134.7649702210847, t=357.70687197505373, r=300.69211049329164, b=367.63958202989653, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='CNNs are comprised of three types of layers. These are convolutional layers, pooling layers and fully-connected layers . When these layers are stacked, a CNN architecture has been formed. A simplified CNN architecture for MNIST classification is illustrated in Figure 2.'), FigureElement(label=<DocItemLabel.PICTURE: 'picture'>, id=6, page_no=3, cluster=Cluster(id=6, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=194.3790283203125, t=389.60748215119077, r=421.985600692276, b=512.5117797851562, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9499910473823547, cells=[Cell(id=41, text='input', bbox=BoundingBox(l=208.48486034202372, t=477.9427126389276, r=223.0549303659262, b=485.8841526827757, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='0', bbox=BoundingBox(l=409.24475067137445, t=438.2520724197785, r=412.83917067727117, b=446.21008246371804, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='9', bbox=BoundingBox(l=409.24475067137445, t=470.3982525972714, r=412.83167067725884, b=478.3396626411194, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='convolution', bbox=BoundingBox(l=246.66763040466336, t=389.60748215119077, r=280.7496304605756, b=397.56549219513033, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text=' w/ReLu', bbox=BoundingBox(l=251.36052041236212, t=398.2413321988619, r=276.50482045361184, b=406.18280224271024, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='pooling', bbox=BoundingBox(l=289.15469047436426, t=397.5405221949925, r=311.1308005104165, b=405.49853223893206, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='output ', bbox=BoundingBox(l=401.81390065918396, t=483.1483726676703, r=421.985600692276, b=491.1063227116096, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='fully-connected', bbox=BoundingBox(l=313.20502051381925, t=495.8235127376552, r=358.2925405877863, b=503.7649527815033, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='w/ ReLu', bbox=BoundingBox(l=323.2782605303446, t=504.44409278525313, r=348.39026057154143, b=512.385492829101, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='fully-connected', bbox=BoundingBox(l=340.9916705594039, t=398.3609021995222, r=386.19403063355924, b=406.3023622433704, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='...', bbox=BoundingBox(l=408.3062106698348, t=454.31549250847155, r=413.68658067866136, b=462.2734925524111, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='', annotations=[], provenance=None, predicted_class=None, confidence=None), TextElement(label=<DocItemLabel.CAPTION: 'caption'>, id=7, page_no=3, cluster=Cluster(id=7, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=165.54881286621094, t=524.3402099609375, r=449.30869073710016, b=534.6312866210938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9542887210845947, cells=[Cell(id=52, text='Fig. 2: An simple CNN architecture, comprised of just five layers', bbox=BoundingBox(l=166.0520002724117, t=524.4947528959615, r=449.30869073710016, b=534.4274629508043, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Fig. 2: An simple CNN architecture, comprised of just five layers'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=3, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.9955596923828, t=559.4691772460938, r=480.5867307884124, b=582.2084332146236, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.969181478023529, cells=[Cell(id=53, text='The basic functionality of the example CNN above can be broken down into', bbox=BoundingBox(l=134.76500022108476, t=560.3207430937722, r=480.5867307884124, b=570.2534431486149, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='four key areas.', bbox=BoundingBox(l=134.76500022108476, t=572.2757231597809, r=199.50195032728706, b=582.2084332146236, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The basic functionality of the example CNN above can be broken down into four key areas.'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=9, page_no=3, cluster=Cluster(id=9, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.5282745361328, t=590.1478271484375, r=480.5958907884275, b=612.7933349609375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9722254276275635, cells=[Cell(id=55, text='1.', bbox=BoundingBox(l=139.2480002284392, t=590.6257332610992, r=146.818050240858, b=600.5584433159419, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='As found in other forms of ANN, the', bbox=BoundingBox(l=149.34140024499763, t=590.6257332610992, r=314.51996051597655, b=600.5584433159419, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='input layer', bbox=BoundingBox(l=316.9819905200155, t=590.6954632614842, r=367.034090602127, b=600.3990433150618, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='will hold the pixel values', bbox=BoundingBox(l=369.49298060616087, t=590.6257332610992, r=480.5958907884275, b=600.5584433159419, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='of the image.', bbox=BoundingBox(l=151.7009702488686, t=602.5807333271079, r=208.7368503424371, b=612.5134433819507, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='1. As found in other forms of ANN, the input layer will hold the pixel values of the image.'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=10, page_no=3, cluster=Cluster(id=10, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.4064483642578, t=619.9978637695312, r=480.59625078842805, b=666.728433681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9786887168884277, cells=[Cell(id=60, text='2.', bbox=BoundingBox(l=139.24797022843916, t=620.9297334284206, r=147.41849024184307, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='The', bbox=BoundingBox(l=150.14200024631103, t=620.9297334284206, r=168.37860027622855, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='convolutional layer', bbox=BoundingBox(l=170.7909702801861, t=620.9994634288056, r=258.9898704248782, b=630.7030434823832, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='will determine the output of neurons of which are', bbox=BoundingBox(l=261.4029504288369, t=620.9297334284206, r=480.59009078841797, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='connected to local regions of the input through the calculation of the scalar', bbox=BoundingBox(l=151.70096024886857, t=632.8847334944293, r=480.5961907884279, b=642.817443549272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=65, text='product between their weights and the region connected to the input vol-', bbox=BoundingBox(l=151.70096024886857, t=644.8407235604434, r=480.59625078842805, b=654.7734336152862, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='ume. The', bbox=BoundingBox(l=151.70096024886857, t=656.7957336264523, r=192.50775031581293, b=666.728433681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='rectified linear unit', bbox=BoundingBox(l=194.56696031919108, t=656.8654636268373, r=281.1319904612028, b=666.5690436804149, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=68, text='(commonly shortened to ReLu) aims to apply', bbox=BoundingBox(l=283.19095046458057, t=656.7957336264523, r=480.58984078841746, b=666.728433681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2. The convolutional layer will determine the output of neurons of which are connected to local regions of the input through the calculation of the scalar product between their weights and the region connected to the input volume. The rectified linear unit (commonly shortened to ReLu) aims to apply'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=4, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=255.9207763671875, t=93.53521728515625, r=447.56707763671875, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9285128712654114, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=4, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=475.2783203125, t=93.44747924804688, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.873298704624176, cells=[Cell(id=1, text='5', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='5'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=2, page_no=4, cluster=Cluster(id=2, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=150.73934936523438, t=118.60086059570312, r=480.5961907884279, b=141.57867431640625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7262013554573059, cells=[Cell(id=2, text='an ’elementwise’ activation function such as sigmoid to the output of the', bbox=BoundingBox(l=151.70102024886864, t=119.67181066075966, r=480.5961907884279, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='activation produced by the previous layer.', bbox=BoundingBox(l=151.70102024886864, t=131.6268307267684, r=337.7227205540411, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='an ’elementwise’ activation function such as sigmoid to the output of the activation produced by the previous layer.'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=3, page_no=4, cluster=Cluster(id=3, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.22279357910156, t=150.14556884765625, r=480.59641078842833, b=184.86651102072767, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9725128412246704, cells=[Cell(id=4, text='3.', bbox=BoundingBox(l=139.24802022843926, t=151.02282083386194, r=147.41855024184318, b=160.9555008887047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='The', bbox=BoundingBox(l=150.14204024631113, t=151.02282083386194, r=168.37866027622866, b=160.9555008887047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='pooling layer', bbox=BoundingBox(l=171.0980202806898, t=151.0925208342469, r=231.9396103805017, b=160.79608088782436, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='will then simply perform downsampling along the spa-', bbox=BoundingBox(l=234.6580203849613, t=151.02282083386194, r=480.59476078842556, b=160.9555008887047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='tial dimensionality of the given input, further reducing the number of pa-', bbox=BoundingBox(l=151.70102024886864, t=162.9778408998709, r=480.59641078842833, b=172.91052095471343, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='rameters within that activation.', bbox=BoundingBox(l=151.70102024886864, t=174.93383096588502, r=290.1014404759174, b=184.86651102072767, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='3. The pooling layer will then simply perform downsampling along the spatial dimensionality of the given input, further reducing the number of parameters within that activation.'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=4, page_no=4, cluster=Cluster(id=4, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.19285583496094, t=193.37098693847656, r=480.59628078842803, b=240.20932006835938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.978056788444519, cells=[Cell(id=10, text='4.', bbox=BoundingBox(l=139.24802022843926, t=194.32983107297866, r=147.41855024184318, b=204.2625111278213, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='The', bbox=BoundingBox(l=150.14204024631113, t=194.32983107297866, r=168.37866027622866, b=204.2625111278213, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='fully-connected layers', bbox=BoundingBox(l=172.55103028307352, t=194.3995310733635, r=275.22559045151326, b=204.103081126941, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='will then perform the same duties found in', bbox=BoundingBox(l=279.3970304583566, t=194.32983107297866, r=480.5917107884205, b=204.2625111278213, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='standard ANNs and attempt to produce class scores from the activations,', bbox=BoundingBox(l=151.70103024886865, t=206.28485113898762, r=480.59628078842803, b=216.21752119383007, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='to be used for classification. It is also suggested that ReLu may be used', bbox=BoundingBox(l=151.70103024886865, t=218.23986120499615, r=480.5863007884118, b=228.1725412598389, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='between these layers, as to improve performance.', bbox=BoundingBox(l=151.70103024886865, t=230.1948812710051, r=369.3041406058511, b=240.12756132584775, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='4. The fully-connected layers will then perform the same duties found in standard ANNs and attempt to produce class scores from the activations, to be used for classification. It is also suggested that ReLu may be used between these layers, as to improve performance.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=4, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.94287109375, t=248.67041015625, r=480.5868507884127, b=283.60284423828125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9642502665519714, cells=[Cell(id=17, text='Through this simple method of transformation, CNNs are able to transform', bbox=BoundingBox(l=134.7650302210848, t=249.59185137810402, r=480.5868507884127, b=259.5245314329467, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='the original input layer by layer using convolutional and downsampling tech-', bbox=BoundingBox(l=134.7650302210848, t=261.546871444113, r=480.58679078841254, b=271.47955149895563, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='niques to produce class scores for classification and regression purposes.', bbox=BoundingBox(l=134.7650302210848, t=273.5018915101218, r=453.4885907439574, b=283.43457156496436, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Through this simple method of transformation, CNNs are able to transform the original input layer by layer using convolutional and downsampling techniques to produce class scores for classification and regression purposes.'), TextElement(label=<DocItemLabel.CAPTION: 'caption'>, id=6, page_no=4, cluster=Cluster(id=6, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=133.96603393554688, t=452.9920349121094, r=480.58679078841254, b=499.6073303222656, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7702291011810303, cells=[Cell(id=20, text='Fig. 3: Activations taken from the first convolutional layer of a simplistic deep', bbox=BoundingBox(l=134.76500022108476, t=453.7317525052485, r=480.5866407884123, b=463.6644525600912, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='CNN, after training on the MNIST database of handwritten digits. If you look', bbox=BoundingBox(l=134.76500022108476, t=465.68673257125704, r=480.58679078841254, b=475.6194426260998, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='carefully, you can see that the network has successfully picked up on character-', bbox=BoundingBox(l=134.76500022108476, t=477.6417226372657, r=480.5867307884124, b=487.5744326921085, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='istics unique to specific numeric digits.', bbox=BoundingBox(l=134.76500022108476, t=489.59771270327985, r=305.6634505014472, b=499.53042275812265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Fig. 3: Activations taken from the first convolutional layer of a simplistic deep CNN, after training on the MNIST database of handwritten digits. If you look carefully, you can see that the network has successfully picked up on characteristics unique to specific numeric digits.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=4, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87347412109375, t=525.5286865234375, r=480.58679078841254, b=584.223383225749, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9733061194419861, cells=[Cell(id=24, text='However, it is important to note that simply understanding the overall archi-', bbox=BoundingBox(l=134.76500022108476, t=526.4707329068717, r=480.5867307884124, b=536.4034429617145, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='tecture of a CNN architecture will not suffice. The creation and optimisation', bbox=BoundingBox(l=134.76500022108476, t=538.4257229728803, r=480.58679078841254, b=548.3584230277231, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='of these models can take quite some time, and can be quite confusing. We will', bbox=BoundingBox(l=134.76500022108476, t=550.3807030388889, r=480.5867307884124, b=560.3134130937317, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='now explore in detail the individual layers, detailing their hyperparameters', bbox=BoundingBox(l=134.76500022108476, t=562.3356931048977, r=480.58679078841254, b=572.2684031597404, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='and connectivities.', bbox=BoundingBox(l=134.76500022108476, t=574.2906831709063, r=217.085970356134, b=584.223383225749, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='However, it is important to note that simply understanding the overall architecture of a CNN architecture will not suffice. The creation and optimisation of these models can take quite some time, and can be quite confusing. We will now explore in detail the individual layers, detailing their hyperparameters and connectivities.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=8, page_no=4, cluster=Cluster(id=8, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.96798706054688, t=613.8170776367188, r=248.6064910888672, b=624.6824951171875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9571674466133118, cells=[Cell(id=29, text='2.2', bbox=BoundingBox(l=134.76500022108476, t=614.616403393562, r=147.21825024151457, b=624.3199934471397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='Convolutional layer', bbox=BoundingBox(l=157.18085025785842, t=614.616403393562, r=248.22903040722483, b=624.3199934471397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2.2 Convolutional layer'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=9, page_no=4, cluster=Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87005615234375, t=643.7536010742188, r=480.58688078841266, b=666.9384155273438, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9720144271850586, cells=[Cell(id=31, text='As the name implies, the convolutional layer plays a vital role in how CNNs', bbox=BoundingBox(l=134.76500022108476, t=644.8406835604433, r=480.58688078841266, b=654.7733936152861, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='operate. The layers parameters focus around the use of learnable', bbox=BoundingBox(l=134.76500022108476, t=656.7956836264519, r=419.07770068750557, b=666.7283936812947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='kernels', bbox=BoundingBox(l=421.5659806915877, t=656.8654136268369, r=455.3192707469606, b=666.5690036804147, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='.', bbox=BoundingBox(l=455.3189707469602, t=656.7956836264519, r=457.80963075104614, b=666.7283936812947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='As the name implies, the convolutional layer plays a vital role in how CNNs operate. The layers parameters focus around the use of learnable kernels .'), FigureElement(label=<DocItemLabel.PICTURE: 'picture'>, id=10, page_no=4, cluster=Cluster(id=10, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=135.41848754882812, t=307.9389953613281, r=479.3868713378906, b=442.5080871582031, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9779683947563171, cells=[]), text='', annotations=[], provenance=None, predicted_class=None, confidence=None), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=5, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=133.78485107421875, t=94.09490966796875, r=139.71607971191406, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7533239722251892, cells=[Cell(id=0, text='6', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='6'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=5, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=167.21620178222656, t=93.54159545898438, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7271904349327087, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea et al.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=5, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.7237091064453, t=118.72857666015625, r=480.7948913574219, b=177.42456097963736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9832537770271301, cells=[Cell(id=2, text='These kernels are usually small in spatial dimensionality, but spreads along the', bbox=BoundingBox(l=134.76500022108476, t=119.67181066075966, r=480.58676078841245, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='entirety of the depth of the input. When the data hits a convolutional layer,', bbox=BoundingBox(l=134.76500022108476, t=131.6268307267684, r=480.58679078841254, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='the layer convolves each filter across the spatial dimensionality of the input to', bbox=BoundingBox(l=134.76500022108476, t=143.58184079277714, r=480.58676078841245, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='produce a 2D activation map. These activation maps can be visualised, as seen', bbox=BoundingBox(l=134.76500022108476, t=155.53686085878599, r=480.58676078841245, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='in Figure 3.', bbox=BoundingBox(l=134.76500022108476, t=167.49188092479483, r=184.42854030255882, b=177.42456097963736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='These kernels are usually small in spatial dimensionality, but spreads along the entirety of the depth of the input. When the data hits a convolutional layer, the layer convolves each filter across the spatial dimensionality of the input to produce a 2D activation map. These activation maps can be visualised, as seen in Figure 3.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=5, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.92149353027344, t=186.75308227539062, r=480.5972595214844, b=233.50762128929614, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9789468050003052, cells=[Cell(id=7, text='As we glide through the input, the scalar product is calculated for each value in', bbox=BoundingBox(l=134.76500022108476, t=187.70990103642725, r=480.58679078841254, b=197.6425710912697, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='that kernel. (Figure 4) From this the network will learn kernels that ’fire’ when', bbox=BoundingBox(l=134.76500022108476, t=199.6649111024359, r=480.5867307884124, b=209.59759115727866, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='they see a specific feature at a given spatial position of the input. These are', bbox=BoundingBox(l=134.76500022108476, t=211.61993116844474, r=480.58670078841243, b=221.55261122328739, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='commonly known as', bbox=BoundingBox(l=134.76500022108476, t=223.57495123445358, r=226.9688603723471, b=233.50762128929614, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='activations', bbox=BoundingBox(l=229.45900037643224, t=223.64465123483842, r=278.7041304572199, b=233.3482012884159, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='.', bbox=BoundingBox(l=278.7040104572196, t=223.57495123445358, r=281.19467046130563, b=233.50762128929614, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='As we glide through the input, the scalar product is calculated for each value in that kernel. (Figure 4) From this the network will learn kernels that ’fire’ when they see a specific feature at a given spatial position of the input. These are commonly known as activations .'), FigureElement(label=<DocItemLabel.PICTURE: 'picture'>, id=4, page_no=5, cluster=Cluster(id=4, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=168.14492797851562, t=257.09973141955834, r=446.12943073188455, b=344.0976257324219, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.922330915927887, cells=[Cell(id=13, text='0', bbox=BoundingBox(l=276.3548604533658, t=293.0746416181913, r=279.2306204580836, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='0', bbox=BoundingBox(l=287.97589047243036, t=293.0746416181913, r=290.85165047714816, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='0', bbox=BoundingBox(l=276.3548604533658, t=304.6956716823561, r=279.2306204580836, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='1', bbox=BoundingBox(l=287.97589047243036, t=304.6956716823561, r=290.85165047714816, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='0', bbox=BoundingBox(l=299.70312049166915, t=293.0746416181913, r=302.5788604963869, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='2', bbox=BoundingBox(l=299.70312049166915, t=304.6956716823561, r=302.5788604963869, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='0', bbox=BoundingBox(l=276.3548604533658, t=316.51141174759584, r=279.2306204580836, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='1', bbox=BoundingBox(l=287.97589047243036, t=316.51141174759584, r=290.85165047714816, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='1', bbox=BoundingBox(l=299.70312049166915, t=316.51141174759584, r=302.5788604963869, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='4', bbox=BoundingBox(l=342.4826705618499, t=293.0746416181913, r=345.3584005665676, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='0', bbox=BoundingBox(l=354.1214005809435, t=293.0746416181913, r=356.9971305856612, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='0', bbox=BoundingBox(l=342.4826705618499, t=304.6956716823561, r=345.3584005665676, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='0', bbox=BoundingBox(l=354.1214005809435, t=304.6956716823561, r=356.9971305856612, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='0', bbox=BoundingBox(l=365.8308706001531, t=293.0746416181913, r=368.70667060487085, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='0', bbox=BoundingBox(l=365.8308706001531, t=304.6956716823561, r=368.70667060487085, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='0', bbox=BoundingBox(l=342.4826705618499, t=316.51141174759584, r=345.3584005665676, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='0', bbox=BoundingBox(l=354.1214005809435, t=316.51141174759584, r=356.9971305856612, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='-4', bbox=BoundingBox(l=364.8516805985467, t=316.51141174759584, r=369.69177060648695, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='-8', bbox=BoundingBox(l=419.2581506878016, t=304.6956716823561, r=424.1100806957613, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='Pooled Vector', bbox=BoundingBox(l=269.128600441511, t=275.49743152114, r=309.9874305085408, b=283.3499715644973, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='Kernel', bbox=BoundingBox(l=345.9925505676079, t=276.30102152557697, r=365.1973305991138, b=284.1372015688439, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='Destination Pixel', bbox=BoundingBox(l=397.25488065170487, t=277.072381529836, r=446.12943073188455, b=284.9249815731936, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='0', bbox=BoundingBox(l=174.49387028626077, t=275.435301520797, r=177.37701029099065, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='0', bbox=BoundingBox(l=186.11195030532048, t=275.435301520797, r=188.99509031005033, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='0', bbox=BoundingBox(l=174.49387028626077, t=287.05633158496175, r=177.37701029099065, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='1', bbox=BoundingBox(l=186.11195030532048, t=287.05633158496175, r=188.99509031005033, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='0', bbox=BoundingBox(l=197.83917032455923, t=275.435301520797, r=200.7223103292891, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='2', bbox=BoundingBox(l=197.83917032455923, t=287.05633158496175, r=200.7223103292891, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='0', bbox=BoundingBox(l=174.49387028626077, t=298.8851916502739, r=177.36963029097853, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='1', bbox=BoundingBox(l=186.11195030532048, t=298.8851916502739, r=188.98772031003824, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='1', bbox=BoundingBox(l=197.83917032455923, t=298.8851916502739, r=200.714940329277, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='0', bbox=BoundingBox(l=209.65488034394315, t=275.435301520797, r=212.53799034867296, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='0', bbox=BoundingBox(l=221.269990362998, t=275.435301520797, r=224.15314036772787, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='1', bbox=BoundingBox(l=209.65488034394315, t=287.05633158496175, r=212.53799034867296, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='1', bbox=BoundingBox(l=221.269990362998, t=287.05633158496175, r=224.15314036772787, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='0', bbox=BoundingBox(l=232.99721038223674, t=275.435301520797, r=235.88034038696654, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='2', bbox=BoundingBox(l=232.99721038223674, t=287.05633158496175, r=235.88034038696654, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='1', bbox=BoundingBox(l=209.65488034394315, t=298.8851916502739, r=212.5306403486609, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='1', bbox=BoundingBox(l=221.269990362998, t=298.8851916502739, r=224.14575036771572, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='1', bbox=BoundingBox(l=232.99721038223674, t=298.8851916502739, r=235.87297038695448, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='1', bbox=BoundingBox(l=174.49387028626077, t=310.6949417154807, r=177.36963029097853, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='0', bbox=BoundingBox(l=186.11195030532048, t=310.6949417154807, r=188.98772031003824, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='0', bbox=BoundingBox(l=174.49387028626077, t=322.31604177964573, r=177.36963029097853, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='0', bbox=BoundingBox(l=186.11195030532048, t=322.31604177964573, r=188.98772031003824, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='0', bbox=BoundingBox(l=197.83917032455923, t=310.6949417154807, r=200.714940329277, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='1', bbox=BoundingBox(l=197.83917032455923, t=322.31604177964573, r=200.714940329277, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='0', bbox=BoundingBox(l=174.49387028626077, t=334.1317118448851, r=177.36963029097853, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='1', bbox=BoundingBox(l=186.11195030532048, t=334.1317118448851, r=188.98772031003824, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='1', bbox=BoundingBox(l=197.83917032455923, t=334.1317118448851, r=200.714940329277, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='0', bbox=BoundingBox(l=209.65488034394315, t=310.6949417154807, r=212.5306403486609, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='0', bbox=BoundingBox(l=221.269990362998, t=310.6949417154807, r=224.14575036771572, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='1', bbox=BoundingBox(l=209.65488034394315, t=322.31604177964573, r=212.5306403486609, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=65, text='1', bbox=BoundingBox(l=221.269990362998, t=322.31604177964573, r=224.14575036771572, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='0', bbox=BoundingBox(l=232.99721038223674, t=310.6949417154807, r=235.87297038695448, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='0', bbox=BoundingBox(l=232.99721038223674, t=322.31604177964573, r=235.87297038695448, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=68, text='1', bbox=BoundingBox(l=209.65488034394315, t=334.1317118448851, r=212.5306403486609, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=69, text='1', bbox=BoundingBox(l=221.269990362998, t=334.1317118448851, r=224.14575036771572, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=70, text='1', bbox=BoundingBox(l=232.99721038223674, t=334.1317118448851, r=235.87297038695448, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=71, text='Input Vector', bbox=BoundingBox(l=187.20328030711084, t=257.09973141955834, r=223.31931036635996, b=264.9359714628257, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='', annotations=[], provenance=None, predicted_class=None, confidence=None), TextElement(label=<DocItemLabel.CAPTION: 'caption'>, id=5, page_no=5, cluster=Cluster(id=5, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=134.05441284179688, t=354.8834533691406, r=480.58676078841245, b=389.0554521481427, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9449623823165894, cells=[Cell(id=72, text='Fig. 4: A visual representation of a convolutional layer. The centre element of the', bbox=BoundingBox(l=134.76500022108476, t=355.21276196128264, r=480.58676078841245, b=365.1454720161255, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=73, text='kernel is placed over the input vector, of which is then calculated and replaced', bbox=BoundingBox(l=134.76500022108476, t=367.16775202729133, r=480.5867307884124, b=377.1004620821341, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=74, text='with a weighted sum of itself and any nearby pixels.', bbox=BoundingBox(l=134.76500022108476, t=379.12274209329996, r=365.1202105989872, b=389.0554521481427, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Fig. 4: A visual representation of a convolutional layer. The centre element of the kernel is placed over the input vector, of which is then calculated and replaced with a weighted sum of itself and any nearby pixels.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=5, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.9464569091797, t=415.9989318847656, r=480.7759707887229, b=450.8507385253906, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9816268682479858, cells=[Cell(id=75, text='Every kernel will have a corresponding activation map, of which will be stacked', bbox=BoundingBox(l=134.76500022108476, t=416.8167423014248, r=480.7759707887229, b=426.74945235626757, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=76, text='along the depth dimension to form the full output volume from the convolu-', bbox=BoundingBox(l=134.76500022108476, t=428.77172236743337, r=480.58682078841264, b=438.7044324222761, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=77, text='tional layer.', bbox=BoundingBox(l=134.76500022108476, t=440.72671243344206, r=186.40115030579494, b=450.6594224882848, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Every kernel will have a corresponding activation map, of which will be stacked along the depth dimension to form the full output volume from the convolutional layer.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=5, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.86756896972656, t=460.1748962402344, r=480.5912807884199, b=542.937255859375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9872538447380066, cells=[Cell(id=78, text='As we alluded to earlier, training ANNs on inputs such as images results in', bbox=BoundingBox(l=134.76500022108476, t=460.9447025450743, r=480.58691078841275, b=470.8774125999171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=79, text='models of which are too big to train effectively. This comes down to the fully-', bbox=BoundingBox(l=134.76500022108476, t=472.89968261108294, r=480.58676078841245, b=482.8323926659257, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=80, text='connected manner of standard ANN neurons, so to mitigate against this every', bbox=BoundingBox(l=134.76500022108476, t=484.8546726770915, r=480.58679078841254, b=494.7873827319343, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=81, text='neuron in a convolutional layer is only connected to small region of the input', bbox=BoundingBox(l=134.76500022108476, t=496.80966274310015, r=480.5867307884124, b=506.74237279794295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=82, text='volume. The dimensionality of this region is commonly referred to as the', bbox=BoundingBox(l=134.76500022108476, t=508.7646428091088, r=465.0551807629326, b=518.6973528639516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=83, text='re-', bbox=BoundingBox(l=468.4169907684477, t=508.83438280949383, r=480.5912807884199, b=518.5379628630715, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=84, text='ceptive field size', bbox=BoundingBox(l=134.76498022108473, t=520.7893628755024, r=210.161930344775, b=530.49295292908, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=85, text='of the neuron. The magnitude of the connectivity through the', bbox=BoundingBox(l=212.4549903485368, t=520.7196328751174, r=480.5884407884152, b=530.6523429299602, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=86, text='depth is nearly always equal to the depth of the input.', bbox=BoundingBox(l=134.76498022108473, t=532.6756229411317, r=373.18994061222577, b=542.6083329959744, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='As we alluded to earlier, training ANNs on inputs such as images results in models of which are too big to train effectively. This comes down to the fullyconnected manner of standard ANN neurons, so to mitigate against this every neuron in a convolutional layer is only connected to small region of the input volume. The dimensionality of this region is commonly referred to as the receptive field size of the neuron. The magnitude of the connectivity through the depth is nearly always equal to the depth of the input.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=5, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.9088134765625, t=552.2114868164062, r=480.59470078842554, b=622.6449584960938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9872121214866638, cells=[Cell(id=87, text='For example, if the input to the network is an image of size', bbox=BoundingBox(l=134.76498022108473, t=552.8926330527584, r=392.8262006444395, b=562.8253431076012, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=88, text='64', bbox=BoundingBox(l=395.26599064844197, t=553.1018630539137, r=405.22858066478585, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=89, text='×', bbox=BoundingBox(l=407.24899066810036, t=552.5439430508331, r=414.9978906808126, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=90, text='64', bbox=BoundingBox(l=417.0189806841282, t=553.1018630539137, r=426.98157070047205, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=91, text='×', bbox=BoundingBox(l=429.00299070378827, t=552.5439430508331, r=436.7518907165005, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=92, text='3', bbox=BoundingBox(l=438.77298071981613, t=553.1018630539137, r=443.75427072798806, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=93, text='(a RGB-', bbox=BoundingBox(l=446.1929907319888, t=552.8926330527584, r=480.5938407884241, b=562.8253431076012, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=94, text='coloured image with a dimensionality of', bbox=BoundingBox(l=134.76498022108473, t=564.847623118767, r=312.8862905132964, b=574.7803331736098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=95, text='64', bbox=BoundingBox(l=315.2929705172446, t=565.0568531199224, r=325.25555053358846, b=573.9036231687692, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=96, text='×', bbox=BoundingBox(l=327.1609805367143, t=564.4989331168418, r=334.90988054942653, b=573.9036231687692, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=97, text='64', bbox=BoundingBox(l=336.8149705525519, t=565.0568531199224, r=346.7775605688957, b=573.9036231687692, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=98, text=') and we set the receptive field', bbox=BoundingBox(l=346.77698056889477, t=564.847623118767, r=480.59470078842554, b=574.7803331736098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=99, text='size as', bbox=BoundingBox(l=134.76498022108473, t=576.8026131847757, r=163.6963802685473, b=586.7353232396185, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=100, text='6', bbox=BoundingBox(l=166.5449802732205, t=577.011843185931, r=171.52628028139242, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=101, text='×', bbox=BoundingBox(l=174.00598028546037, t=576.4539131828503, r=181.75490029817266, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=102, text='6', bbox=BoundingBox(l=184.2349903022413, t=577.011843185931, r=189.21628031041323, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=103, text=', we would have a total of', bbox=BoundingBox(l=189.21599031041274, t=576.8026131847757, r=305.3599505009493, b=586.7353232396185, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=104, text='108', bbox=BoundingBox(l=308.2089805056232, t=577.011843185931, r=323.15289053013896, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=105, text='weights on each neuron within the', bbox=BoundingBox(l=326.00299053481467, t=576.8026131847757, r=480.5926807884222, b=586.7353232396185, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=106, text='convolutional layer. (', bbox=BoundingBox(l=134.76498022108473, t=588.7576132507844, r=228.17432037432468, b=598.6903233056272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=107, text='6', bbox=BoundingBox(l=228.1749903743258, t=588.9668232519396, r=233.1562803824977, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=108, text='×', bbox=BoundingBox(l=235.49298038633108, t=588.4089232488592, r=243.24190039904332, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=109, text='6', bbox=BoundingBox(l=245.57799040287574, t=588.9668232519396, r=250.55928041104767, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=110, text='×', bbox=BoundingBox(l=252.8959804148811, t=588.4089232488592, r=260.6449004275933, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=111, text='3', bbox=BoundingBox(l=262.98099043142577, t=588.9668232519396, r=267.9622804395977, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=112, text='where', bbox=BoundingBox(l=270.618990443956, t=588.7576132507844, r=298.02612048891797, b=598.6903233056272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=113, text='3', bbox=BoundingBox(l=300.68198049327503, t=588.9668232519396, r=305.6632705014469, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=114, text='is the magnitude of connectivity across', bbox=BoundingBox(l=308.3189705058036, t=588.7576132507844, r=480.5922207884214, b=598.6903233056272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=115, text='the depth of the volume) To put this into perspective, a standard neuron seen', bbox=BoundingBox(l=134.7649702210847, t=600.7126133167931, r=480.5867307884124, b=610.6453233716359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=116, text='in other forms of ANN would contain', bbox=BoundingBox(l=134.7649702210847, t=612.6686033828073, r=301.70825049495863, b=622.6013134376501, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=117, text='12', bbox=BoundingBox(l=304.1969604990414, t=612.8778233839625, r=314.1595505153852, b=621.7246034328093, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=118, text=',', bbox=BoundingBox(l=314.1599705153859, t=612.8778233839625, r=316.9275805199262, b=621.7246034328093, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=119, text='288', bbox=BoundingBox(l=318.5879805226502, t=612.8778233839625, r=333.5318905471659, b=621.7246034328093, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=120, text='weights each.', bbox=BoundingBox(l=336.0229805512526, t=612.6686033828073, r=395.7687106492667, b=622.6013134376501, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='For example, if the input to the network is an image of size 64 × 64 × 3 (a RGBcoloured image with a dimensionality of 64 × 64 ) and we set the receptive field size as 6 × 6 , we would have a total of 108 weights on each neuron within the convolutional layer. ( 6 × 6 × 3 where 3 is the magnitude of connectivity across the depth of the volume) To put this into perspective, a standard neuron seen in other forms of ANN would contain 12 , 288 weights each.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=9, page_no=5, cluster=Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.94898986816406, t=632.0844116210938, r=480.5867307884124, b=666.8765258789062, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9835361242294312, cells=[Cell(id=121, text='Convolutional layers are also able to significantly reduce the complexity of the', bbox=BoundingBox(l=134.76498022108473, t=632.885603494434, r=480.5867307884124, b=642.8183135492768, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=122, text='model through the optimisation of its output. These are optimised through', bbox=BoundingBox(l=134.76498022108473, t=644.8406035604428, r=480.5867307884124, b=654.7733136152856, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=123, text='three hyperparameters, the', bbox=BoundingBox(l=134.76498022108473, t=656.7956036264516, r=254.06711041680234, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=124, text='depth', bbox=BoundingBox(l=256.5559704208854, t=656.8653436268365, r=283.11627046445807, b=666.5689236804142, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=125, text=', the', bbox=BoundingBox(l=283.1169704644592, t=656.7956036264516, r=301.9164104953001, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=126, text='stride', bbox=BoundingBox(l=304.4069804993859, t=656.8653436268365, r=330.4093605420433, b=666.5689236804142, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=127, text='and setting', bbox=BoundingBox(l=332.8999905461293, t=656.7956036264516, r=381.98572062665545, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=128, text='zero-padding', bbox=BoundingBox(l=384.47598063074076, t=656.8653436268365, r=445.35742073061806, b=666.5689236804142, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=129, text='.', bbox=BoundingBox(l=445.3569907306173, t=656.7956036264516, r=447.84766073470337, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Convolutional layers are also able to significantly reduce the complexity of the model through the optimisation of its output. These are optimised through three hyperparameters, the depth , the stride and setting zero-padding .'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=6, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=255.95968627929688, t=93.53328704833984, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9278361201286316, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=6, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=475.1293029785156, t=93.63327026367188, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8769420981407166, cells=[Cell(id=1, text='7', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='7'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=6, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.85671997070312, t=118.58126831054688, r=480.73211669921875, b=201.79269409179688, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9878745675086975, cells=[Cell(id=2, text='The', bbox=BoundingBox(l=134.76501022108476, t=119.67181066075966, r=151.4424102484444, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='depth', bbox=BoundingBox(l=154.47302025341617, t=119.74151066114439, r=181.03331029698887, b=129.445060714722, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='of the output volume produced by the convolutional layers can be', bbox=BoundingBox(l=184.06403030196083, t=119.67181066075966, r=480.59082078841914, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='manually set through the number of neurons within the layer to a the same', bbox=BoundingBox(l=134.7650302210848, t=131.6268307267684, r=480.58676078841245, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='region of the input. This can be seen with other forms of ANNs, where the', bbox=BoundingBox(l=134.7650302210848, t=143.58184079277714, r=480.5867307884124, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='all of the neurons in the hidden layer are directly connected to every single', bbox=BoundingBox(l=134.7650302210848, t=155.53686085878599, r=480.58682078841264, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='neuron beforehand. Reducing this hyperparameter can significantly minimise', bbox=BoundingBox(l=134.7650302210848, t=167.49188092479483, r=480.5868507884127, b=177.42456097963736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='the total number of neurons of the network, but it can also significantly reduce', bbox=BoundingBox(l=134.7650302210848, t=179.44787099080895, r=480.58676078841245, b=189.3805510456516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='the pattern recognition capabilities of the model.', bbox=BoundingBox(l=134.7650302210848, t=191.4028910568178, r=348.2934605713826, b=201.33557111166033, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The depth of the output volume produced by the convolutional layers can be manually set through the number of neurons within the layer to a the same region of the input. This can be seen with other forms of ANNs, where the all of the neurons in the hidden layer are directly connected to every single neuron beforehand. Reducing this hyperparameter can significantly minimise the total number of neurons of the network, but it can also significantly reduce the pattern recognition capabilities of the model.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=6, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.76483154296875, t=211.41566467285156, r=480.6409912109375, b=282.1136415576709, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9880473613739014, cells=[Cell(id=11, text='We are also able to define the', bbox=BoundingBox(l=134.7650302210848, t=212.40490117277898, r=259.2974904253829, b=222.33758122762163, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='stride', bbox=BoundingBox(l=261.2180204285336, t=212.47460117316382, r=287.220400471191, b=222.17816122674128, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='in which we set the depth around the spatial', bbox=BoundingBox(l=289.1430104743451, t=212.40490117277898, r=480.5943607884249, b=222.33758122762163, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='dimensionality of the input in order to place the receptive field. For example if', bbox=BoundingBox(l=134.76500022108476, t=224.3609012387932, r=480.58682078841264, b=234.29357129363575, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='we were to set a stride as 1, then we would have a heavily overlapped receptive', bbox=BoundingBox(l=134.76500022108476, t=236.31591130480183, r=480.58676078841245, b=246.24859135964448, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='field producing extremely large activations. Alternatively, setting the stride to a', bbox=BoundingBox(l=134.76500022108476, t=248.27093137081079, r=480.5867307884124, b=258.2036114256533, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='greater number will reduce the amount of overlapping and produce an output', bbox=BoundingBox(l=134.76500022108476, t=260.2259514368196, r=480.5867307884124, b=270.1586314916623, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='of lower spatial dimensions.', bbox=BoundingBox(l=134.76500022108476, t=272.18096150282827, r=258.9986004248926, b=282.1136415576709, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='We are also able to define the stride in which we set the depth around the spatial dimensionality of the input in order to place the receptive field. For example if we were to set a stride as 1, then we would have a heavily overlapped receptive field producing extremely large activations. Alternatively, setting the stride to a greater number will reduce the amount of overlapping and produce an output of lower spatial dimensions.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=4, page_no=6, cluster=Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.788818359375, t=292.6251220703125, r=480.5876207884139, b=327.02667180565516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9830900430679321, cells=[Cell(id=19, text='Zero-padding', bbox=BoundingBox(l=134.76500022108476, t=293.25366161917987, r=197.31020032369145, b=302.95721167275724, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='is the simple process of padding the border of the input, and', bbox=BoundingBox(l=201.02701032978894, t=293.1839616187949, r=480.5876207884139, b=303.1166316736376, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='is an effective method to give further control as to the dimensionality of the', bbox=BoundingBox(l=134.76501022108476, t=305.13897168480366, r=480.58676078841245, b=315.0716517396463, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='output volumes.', bbox=BoundingBox(l=134.76501022108476, t=317.0939917508125, r=207.4521303403295, b=327.02667180565516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Zero-padding is the simple process of padding the border of the input, and is an effective method to give further control as to the dimensionality of the output volumes.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=6, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.85841369628906, t=337.35748291015625, r=480.78466796875, b=372.1994323730469, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9820359349250793, cells=[Cell(id=23, text='It is important to understand that through using these techniques, we will alter', bbox=BoundingBox(l=134.76501022108476, t=338.0969518667789, r=480.58682078841264, b=348.0296619216217, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='the spatial dimensionality of the convolutional layers output. To calculate this,', bbox=BoundingBox(l=134.76501022108476, t=350.0519419327876, r=480.5867307884124, b=359.98465198763034, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='you can make use of the following formula:', bbox=BoundingBox(l=134.76501022108476, t=362.0069219987962, r=326.46536053557315, b=371.93963205363895, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='It is important to understand that through using these techniques, we will alter the spatial dimensionality of the convolutional layers output. To calculate this, you can make use of the following formula:'), TextElement(label=<DocItemLabel.FORMULA: 'formula'>, id=6, page_no=6, cluster=Cluster(id=6, label=<DocItemLabel.FORMULA: 'formula'>, bbox=BoundingBox(l=276.7643127441406, t=395.47357177734375, r=338.3543395996094, b=418.7127623118935, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9492904543876648, cells=[Cell(id=26, text='(', bbox=BoundingBox(l=277.54602045532, t=396.29214218809966, r=281.42047046167613, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='V', bbox=BoundingBox(l=281.42102046167696, t=396.29214218809966, r=287.23221047121035, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='−', bbox=BoundingBox(l=291.66003047847425, t=395.73422218501906, r=299.4089404911865, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='R', bbox=BoundingBox(l=301.6230504948188, t=396.29214218809966, r=309.18765050722874, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text=') + 2', bbox=BoundingBox(l=309.2640405073541, t=396.29214218809966, r=330.29208054185096, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='Z', bbox=BoundingBox(l=330.2960505418574, t=396.29214218809966, r=337.09653055301374, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='S', bbox=BoundingBox(l=295.7569904851954, t=409.8659922630467, r=301.8660604952175, b=418.7127623118935, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='+ 1', bbox=BoundingBox(l=304.6550004997928, t=409.8659922630467, r=319.59689052430525, b=418.7127623118935, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='( V − R ) + 2 Z S + 1'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=6, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=134.1217041015625, t=437.6549987792969, r=480.6238708496094, b=496.4444885253906, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.986332356929779, cells=[Cell(id=34, text='Where', bbox=BoundingBox(l=134.76500022108476, t=438.3707524204338, r=163.82590026875977, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='V', bbox=BoundingBox(l=165.8860002721394, t=438.579982421589, r=171.6971902816728, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='represents the input volume size (', bbox=BoundingBox(l=175.97101028868408, t=438.3707524204338, r=323.0787705300174, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='height', bbox=BoundingBox(l=323.0780005300161, t=438.579982421589, r=349.92023057405135, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='×', bbox=BoundingBox(l=350.5410205750698, t=438.02206241850854, r=358.289920587782, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='width', bbox=BoundingBox(l=358.9100005887993, t=438.579982421589, r=383.8175006296605, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='×', bbox=BoundingBox(l=384.43600063067515, t=438.02206241850854, r=392.18491064338747, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='depth', bbox=BoundingBox(l=392.80499064440465, t=438.579982421589, r=417.71249068526595, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='),', bbox=BoundingBox(l=417.71201068526517, t=438.3707524204338, r=423.52020069479363, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='R', bbox=BoundingBox(l=425.5800206981728, t=438.579982421589, r=433.1446207105826, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='represents', bbox=BoundingBox(l=435.28201071408915, t=438.3707524204338, r=480.59192078842096, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='the receptive field size,', bbox=BoundingBox(l=134.76501022108476, t=450.3257424864424, r=236.31381038767768, b=460.25845254128524, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='Z', bbox=BoundingBox(l=239.082020392219, t=450.5349724875977, r=245.8824904033753, b=459.3817425364445, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='is the amount of zero padding set and', bbox=BoundingBox(l=249.36801040909333, t=450.3257424864424, r=418.18417068603975, b=460.25845254128524, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='S', bbox=BoundingBox(l=420.95203069058044, t=450.5349724875977, r=427.06110070060254, b=459.3817425364445, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='referring to', bbox=BoundingBox(l=430.4080207060932, t=450.3257424864424, r=480.58963078841714, b=460.25845254128524, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='the stride. If the calculated result from this equation is not equal to a whole', bbox=BoundingBox(l=134.76501022108476, t=462.28073255245107, r=480.58676078841245, b=472.2134426072938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='integer then the stride has been incorrectly set, as the neurons will be unable to', bbox=BoundingBox(l=134.76501022108476, t=474.2357126184597, r=480.58679078841254, b=484.1684226733024, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='fit neatly across the given input.', bbox=BoundingBox(l=134.76501022108476, t=486.19070268446836, r=275.6660504522358, b=496.1234127393111, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Where V represents the input volume size ( height × width × depth ), R represents the receptive field size, Z is the amount of zero padding set and S referring to the stride. If the calculated result from this equation is not equal to a whole integer then the stride has been incorrectly set, as the neurons will be unable to fit neatly across the given input.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=6, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8834686279297, t=506.8936462402344, r=480.5930507884228, b=552.9923730533092, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9862993359565735, cells=[Cell(id=53, text='Despite our best efforts so far we will still find that our models are still enor-', bbox=BoundingBox(l=134.76501022108476, t=507.19369280043486, r=480.58691078841275, b=517.1264028552775, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='mous if we use an image input of any', bbox=BoundingBox(l=134.76501022108476, t=519.1486828664436, r=304.7069704998781, b=529.0813929212864, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='real', bbox=BoundingBox(l=307.7520105048735, t=519.0789428660585, r=322.5166005290951, b=528.8721929201313, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='dimensionality. However, methods', bbox=BoundingBox(l=325.5650005340961, t=519.1486828664436, r=480.5930507884228, b=529.0813929212864, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='have been developed as to greatly curtail the overall number of parameters', bbox=BoundingBox(l=134.76500022108476, t=531.1036629324522, r=480.5866107884122, b=541.036372987295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='within the convolutional layer.', bbox=BoundingBox(l=134.76500022108476, t=543.0596629984664, r=269.78809044259293, b=552.9923730533092, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Despite our best efforts so far we will still find that our models are still enormous if we use an image input of any real dimensionality. However, methods have been developed as to greatly curtail the overall number of parameters within the convolutional layer.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=9, page_no=6, cluster=Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8805694580078, t=563.507568359375, r=480.5939607884243, b=622.790283203125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9858874678611755, cells=[Cell(id=59, text='Parameter sharing', bbox=BoundingBox(l=134.76500022108476, t=564.1314031148125, r=217.39481035664068, b=573.8349931683902, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='works on the assumption that if one region feature is useful', bbox=BoundingBox(l=219.77301036054214, t=564.0616731144275, r=480.5939607884243, b=573.9943831692703, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='to compute at a set spatial region, then it is likely to be useful in another region.', bbox=BoundingBox(l=134.76501022108476, t=576.0166631804361, r=480.58679078841254, b=585.9493732352789, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='If we constrain each individual activation map within the output volume to the', bbox=BoundingBox(l=134.76501022108476, t=587.9726532464504, r=480.5867307884124, b=597.9053633012932, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='same weights and bias, then we will see a massive reduction in the number of', bbox=BoundingBox(l=134.76501022108476, t=599.927653312459, r=480.58670078841243, b=609.8603633673018, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='parameters being produced by the convolutional layer.', bbox=BoundingBox(l=134.76501022108476, t=611.8826633784678, r=376.0691506169492, b=621.8153634333105, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Parameter sharing works on the assumption that if one region feature is useful to compute at a set spatial region, then it is likely to be useful in another region. If we constrain each individual activation map within the output volume to the same weights and bias, then we will see a massive reduction in the number of parameters being produced by the convolutional layer.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=10, page_no=6, cluster=Cluster(id=10, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.85092163085938, t=632.0819702148438, r=480.5868507884127, b=666.8784790039062, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9838565587997437, cells=[Cell(id=65, text='As a result of this as the backpropagation stage occurs, each neuron in the out-', bbox=BoundingBox(l=134.76501022108476, t=632.8856634944344, r=480.58676078841245, b=642.8183735492772, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='put will represent the overall gradient of which can be totalled across the depth', bbox=BoundingBox(l=134.76501022108476, t=644.8406635604431, r=480.5868507884127, b=654.7733736152859, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='- thus only updating a single set of weights, as opposed to every single one.', bbox=BoundingBox(l=134.76501022108476, t=656.7956636264519, r=466.95804076605424, b=666.7283736812946, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='As a result of this as the backpropagation stage occurs, each neuron in the output will represent the overall gradient of which can be totalled across the depth - thus only updating a single set of weights, as opposed to every single one.'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=7, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=133.78009033203125, t=93.9936294555664, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7759202718734741, cells=[Cell(id=0, text='8', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='8'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=7, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=167.4324493408203, t=93.5379638671875, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7988561391830444, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea et al.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=2, page_no=7, cluster=Cluster(id=2, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.94790649414062, t=118.79036712646484, r=218.04713439941406, b=129.75680541992188, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9537138938903809, cells=[Cell(id=2, text='2.3', bbox=BoundingBox(l=134.76500022108476, t=119.74151066114439, r=147.21825024151457, b=129.445060714722, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='Pooling layer', bbox=BoundingBox(l=157.18085025785842, t=119.74151066114439, r=217.79330035729438, b=129.445060714722, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2.3 Pooling layer'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=7, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=134.07662963867188, t=147.324462890625, r=480.58676078841245, b=182.26348100635516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9842262864112854, cells=[Cell(id=4, text='Pooling layers aim to gradually reduce the dimensionality of the representa-', bbox=BoundingBox(l=134.76500022108476, t=148.41980081948964, r=480.58676078841245, b=158.3524708743322, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='tion, and thus further reduce the number of parameters and the computational', bbox=BoundingBox(l=134.76500022108476, t=160.37579088550376, r=480.5867307884124, b=170.3084709403464, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='complexity of the model.', bbox=BoundingBox(l=134.76500022108476, t=172.3308109515126, r=244.44325040101418, b=182.26348100635516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Pooling layers aim to gradually reduce the dimensionality of the representation, and thus further reduce the number of parameters and the computational complexity of the model.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=4, page_no=7, cluster=Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.83888244628906, t=190.89772033691406, r=480.61639404296875, b=260.9205314406546, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9867627024650574, cells=[Cell(id=7, text='The pooling layer operates over each activation map in the input, and scales', bbox=BoundingBox(l=134.76500022108476, t=191.21179105576266, r=480.58679078841254, b=201.1444711106052, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='its dimensionality using the “MAX” function. In most CNNs, these come in the', bbox=BoundingBox(l=134.76500022108476, t=203.1668011217714, r=480.58667078841233, b=213.09948117661406, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='form of', bbox=BoundingBox(l=134.76500022108476, t=215.12182118778026, r=168.0400802756732, b=225.0545012426228, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='max-pooling layers', bbox=BoundingBox(l=171.07899028065862, t=215.1915211881651, r=258.78973042454993, b=224.89508124174267, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='with kernels of a dimensionality of', bbox=BoundingBox(l=261.8280004295342, t=215.12182118778026, r=417.961820685675, b=225.0545012426228, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='2', bbox=BoundingBox(l=421.0000006906592, t=215.33099118893517, r=425.9812906988311, b=224.17779123778212, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='×', bbox=BoundingBox(l=428.5979907031238, t=214.77313118585494, r=436.346890715836, b=224.17779123778212, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='2', bbox=BoundingBox(l=438.9629807201278, t=215.33099118893517, r=443.94427072829967, b=224.17779123778212, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='applied', bbox=BoundingBox(l=446.97800073327664, t=215.12182118778026, r=480.59180078842076, b=225.0545012426228, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='with a stride of', bbox=BoundingBox(l=134.76498022108473, t=227.07782125379447, r=205.8581103377145, b=237.01049130863703, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='2', bbox=BoundingBox(l=209.82999034423042, t=227.2869812549493, r=214.81128035240235, b=236.13378130379635, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='along the spatial dimensions of the input. This scales the', bbox=BoundingBox(l=218.78899035892786, t=227.07782125379447, r=480.5960707884277, b=237.01049130863703, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='activation map down to 25% of the original size - whilst maintaining the depth', bbox=BoundingBox(l=134.76498022108473, t=239.03283131980334, r=480.58670078841243, b=248.96551137464587, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='volume to its standard size.', bbox=BoundingBox(l=134.76498022108473, t=250.98785138581206, r=255.8603704197442, b=260.9205314406546, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The pooling layer operates over each activation map in the input, and scales its dimensionality using the “MAX” function. In most CNNs, these come in the form of max-pooling layers with kernels of a dimensionality of 2 × 2 applied with a stride of 2 along the spatial dimensions of the input. This scales the activation map down to 25% of the original size - whilst maintaining the depth volume to its standard size.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=7, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8048095703125, t=269.3749694824219, r=480.6969909667969, b=351.6319580078125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9878619909286499, cells=[Cell(id=21, text='Due to the destructive nature of the pooling layer, there are only two generally', bbox=BoundingBox(l=134.76498022108473, t=269.8688314900621, r=480.58679078841254, b=279.80151154490466, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='observed methods of max-pooling. Usually, the stride and filters of the pooling', bbox=BoundingBox(l=134.76498022108473, t=281.82385155607096, r=480.58682078841264, b=291.7565316109135, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='layers are both set to', bbox=BoundingBox(l=134.76498022108473, t=293.779841622085, r=228.8518103754361, b=303.7125216769276, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='2', bbox=BoundingBox(l=232.17899038089442, t=293.98901162324, r=237.16028038906634, b=302.83581167208695, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='×', bbox=BoundingBox(l=239.99298039371342, t=293.43115162015977, r=247.74190040642566, b=302.83581167208695, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='2', bbox=BoundingBox(l=250.57397041107177, t=293.98901162324, r=255.55527041924367, b=302.83581167208695, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text=', which will allow the layer to extend through the', bbox=BoundingBox(l=255.55498041924324, t=293.779841622085, r=480.590150788418, b=303.7125216769276, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='entirety of the spatial dimensionality of the input. Furthermore', bbox=BoundingBox(l=134.76498022108473, t=305.7348616880938, r=421.52844069152604, b=315.66754174293646, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='overlapping', bbox=BoundingBox(l=425.23898069761333, t=305.80456168847866, r=480.5911907884197, b=315.50811174205614, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='pooling', bbox=BoundingBox(l=134.76498022108473, t=317.7595817544875, r=170.19199027920348, b=327.4631318080651, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='may be utilised, where the stride is set to', bbox=BoundingBox(l=173.6349802848518, t=317.68988175410277, r=360.75253059182194, b=327.6225518089452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='2', bbox=BoundingBox(l=364.1929905974661, t=317.8990417552576, r=369.174290605638, b=326.74584180410454, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='with a kernel size set to', bbox=BoundingBox(l=372.6170006112859, t=317.68988175410277, r=480.5916407884205, b=327.6225518089452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='3', bbox=BoundingBox(l=134.76500022108476, t=329.8540618212663, r=139.74629022925666, b=338.7008318701132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='. Due to the destructive nature of pooling, having a kernel size above', bbox=BoundingBox(l=139.7460002292562, t=329.6448318201111, r=452.0536207416033, b=339.57754187495385, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='3', bbox=BoundingBox(l=455.33002074697833, t=329.8540618212663, r=460.3113107551502, b=338.7008318701132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='will', bbox=BoundingBox(l=463.5860307605225, t=329.6448318201111, r=480.5921907884214, b=339.57754187495385, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='usually greatly decrease the performance of the model.', bbox=BoundingBox(l=134.76501022108476, t=341.59982188611974, r=376.31824061735784, b=351.5325319409625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Due to the destructive nature of the pooling layer, there are only two generally observed methods of max-pooling. Usually, the stride and filters of the pooling layers are both set to 2 × 2 , which will allow the layer to extend through the entirety of the spatial dimensionality of the input. Furthermore overlapping pooling may be utilised, where the stride is set to 2 with a kernel size set to 3 . Due to the destructive nature of pooling, having a kernel size above 3 will usually greatly decrease the performance of the model.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=7, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.9580078125, t=359.5020751953125, r=480.59521078842636, b=418.52410888671875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9869884252548218, cells=[Cell(id=39, text='It is also important to understand that beyond max-pooling, CNN architectures', bbox=BoundingBox(l=134.76501022108476, t=360.4818119903754, r=480.58679078841254, b=370.4145220452181, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='may contain general-pooling.', bbox=BoundingBox(l=134.76501022108476, t=372.436792056384, r=263.6810304325742, b=382.3695021112268, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='General pooling', bbox=BoundingBox(l=265.9370104362751, t=372.506532056769, r=340.13846055800417, b=382.21011211034676, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='layers are comprised of pooling', bbox=BoundingBox(l=342.3940105617044, t=372.436792056384, r=480.59521078842636, b=382.3695021112268, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='neurons that are able to perform a multitude of common operations including', bbox=BoundingBox(l=134.76501022108476, t=384.39178212239267, r=480.58679078841254, b=394.3244921772354, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='L1/L2-normalisation, and average pooling. However, this tutorial will primar-', bbox=BoundingBox(l=134.76501022108476, t=396.34677218840125, r=480.5868507884127, b=406.2794822432441, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='ily focus on the use of max-pooling.', bbox=BoundingBox(l=134.76501022108476, t=408.30175225440985, r=292.00470047903974, b=418.23446230925265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='It is also important to understand that beyond max-pooling, CNN architectures may contain general-pooling. General pooling layers are comprised of pooling neurons that are able to perform a multitude of common operations including L1/L2-normalisation, and average pooling. However, this tutorial will primarily focus on the use of max-pooling.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=7, page_no=7, cluster=Cluster(id=7, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.02223205566406, t=446.0529479980469, r=255.7871551513672, b=456.9926452636719, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9582642912864685, cells=[Cell(id=46, text='2.4', bbox=BoundingBox(l=134.76501022108476, t=447.08248246853503, r=147.21826024151457, b=456.78607252211265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='Fully-connected layer', bbox=BoundingBox(l=157.18086025785843, t=447.08248246853503, r=255.41211041900885, b=456.78607252211265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2.4 Fully-connected layer'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=7, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.71995544433594, t=474.6971435546875, r=480.58682078841264, b=521.6184692382812, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9859198331832886, cells=[Cell(id=48, text='The fully-connected layer contains neurons of which are directly connected to', bbox=BoundingBox(l=134.76501022108476, t=475.76174262688556, r=480.58682078841264, b=485.6944526817283, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='the neurons in the two adjacent layers, without being connected to any layers', bbox=BoundingBox(l=134.76501022108476, t=487.7167326928942, r=480.58676078841245, b=497.649442747737, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='within them. This is analogous to way that neurons are arranged in traditional', bbox=BoundingBox(l=134.76501022108476, t=499.6717227589028, r=480.5867307884124, b=509.60443281374563, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='forms of ANN. (Figure 1)', bbox=BoundingBox(l=134.76501022108476, t=511.62771282491707, r=246.1269404037763, b=521.5604228797598, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The fully-connected layer contains neurons of which are directly connected to the neurons in the two adjacent layers, without being connected to any layers within them. This is analogous to way that neurons are arranged in traditional forms of ANN. (Figure 1)'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=9, page_no=7, cluster=Cluster(id=9, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.84336853027344, t=550.3342895507812, r=195.18660032020762, b=562.74267578125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9564018249511719, cells=[Cell(id=52, text='3', bbox=BoundingBox(l=134.76501022108476, t=550.9896830422515, r=140.74261023089116, b=562.6340631065451, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='Recipes', bbox=BoundingBox(l=152.6978102505039, t=550.9896830422515, r=195.18660032020762, b=562.6340631065451, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='3 Recipes'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=10, page_no=7, cluster=Cluster(id=10, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87806701660156, t=583.9160766601562, r=480.6416015625, b=666.999755859375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9855859875679016, cells=[Cell(id=54, text='Despite the relatively small number of layers required to form a CNN, there', bbox=BoundingBox(l=134.76501022108476, t=585.0647232303944, r=480.58682078841264, b=594.9974332852372, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='is no set way of formulating a CNN architecture. That being said, it would be', bbox=BoundingBox(l=134.76501022108476, t=597.0197232964031, r=480.58679078841254, b=606.9524333512459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='idiotic to simply throw a few of layers together and expect it to work. Through', bbox=BoundingBox(l=134.76501022108476, t=608.9747333624118, r=480.58682078841264, b=618.9074434172546, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='reading of related literature it is obvious that much like other forms of ANNs,', bbox=BoundingBox(l=134.76501022108476, t=620.9297334284206, r=480.5868507884127, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='CNNs tend to follow a common architecture. This common architecture is illus-', bbox=BoundingBox(l=134.76501022108476, t=632.8857234944347, r=480.58676078841245, b=642.8184335492775, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='trated in Figure 2, where convolutional layers are stacked, followed by pooling', bbox=BoundingBox(l=134.76501022108476, t=644.8407235604434, r=480.58688078841266, b=654.7734336152862, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='layers in a repeated manner before feeding forward to fully-connected layers.', bbox=BoundingBox(l=134.76501022108476, t=656.7957336264523, r=475.0078107792602, b=666.728433681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Despite the relatively small number of layers required to form a CNN, there is no set way of formulating a CNN architecture. That being said, it would be idiotic to simply throw a few of layers together and expect it to work. Through reading of related literature it is obvious that much like other forms of ANNs, CNNs tend to follow a common architecture. This common architecture is illustrated in Figure 2, where convolutional layers are stacked, followed by pooling layers in a repeated manner before feeding forward to fully-connected layers.'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=8, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=255.96002197265625, t=93.55046844482422, r=447.56597900390625, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9279627799987793, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=8, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=474.8158874511719, t=93.79475402832031, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8714832067489624, cells=[Cell(id=1, text='9', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='9'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=8, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87208557128906, t=118.63634490966797, r=480.66082763671875, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9839420914649963, cells=[Cell(id=2, text='Another common CNN architecture is to stack two convolutional layers before', bbox=BoundingBox(l=134.76501022108476, t=119.67181066075966, r=480.58676078841245, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='each pooling layer, as illustrated in Figure 5. This is strongly recommended as', bbox=BoundingBox(l=134.76501022108476, t=131.6268307267684, r=480.58682078841264, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='stacking multiple convolutional layers allows for more complex features of the', bbox=BoundingBox(l=134.76501022108476, t=143.58184079277714, r=480.5868507884127, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='input vector to be selected.', bbox=BoundingBox(l=134.76501022108476, t=155.53686085878599, r=252.77200041467768, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Another common CNN architecture is to stack two convolutional layers before each pooling layer, as illustrated in Figure 5. This is strongly recommended as stacking multiple convolutional layers allows for more complex features of the input vector to be selected.'), FigureElement(label=<DocItemLabel.PICTURE: 'picture'>, id=3, page_no=8, cluster=Cluster(id=3, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=133.3032989501953, t=188.10928344726562, r=481.8245507904431, b=277.20819153058585, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9117528200149536, cells=[Cell(id=6, text='input', bbox=BoundingBox(l=141.0171202313415, t=254.84454140710648, r=152.0187102493898, b=260.8498514402644, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='convolution w/ ReLu', bbox=BoundingBox(l=175.69994028823936, t=190.0086610491196, r=221.95847036412746, b=196.01397108227752, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='pooling', bbox=BoundingBox(l=232.13184038081707, t=190.21557105026216, r=248.78818040814215, b=196.2333910834891, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='convolution', bbox=BoundingBox(l=350.42499057487936, t=264.6906114614709, r=376.18756061714345, b=270.6959214946288, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='w/ ReLu', bbox=BoundingBox(l=353.9647205806864, t=271.1903614973588, r=372.95895061184683, b=277.20819153058585, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='pooling', bbox=BoundingBox(l=380.33847062395307, t=189.10449104412737, r=396.9493106512036, b=195.10980107728528, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='fully-connected', bbox=BoundingBox(l=399.56500065549466, t=262.8054814510623, r=433.6465807114061, b=268.8107914842202, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='w/ ReLu', bbox=BoundingBox(l=407.1688206679688, t=269.30072148692534, r=426.1585706991219, b=275.3185415201523, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='fully-connected', bbox=BoundingBox(l=420.5636606899434, t=189.10449104412737, r=454.6497807458623, b=195.10980107728528, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='convolution w/ ReLu', bbox=BoundingBox(l=268.3968804403106, t=188.21289103920446, r=314.67807051623583, b=194.2307110724313, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='pooling', bbox=BoundingBox(l=324.8378305329031, t=188.4489710405079, r=341.4487005601536, b=194.45434107366623, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='0', bbox=BoundingBox(l=472.1900607746376, t=219.15814121006656, r=474.9025007790873, b=225.16345124322447, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='9', bbox=BoundingBox(l=472.1900607746376, t=243.45690134423035, r=474.9025007790873, b=249.46221137738826, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='output ', bbox=BoundingBox(l=466.57080076541905, t=253.0985113974658, r=481.8245507904431, b=259.11633143069287, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='...', bbox=BoundingBox(l=471.47577077346574, t=230.8848212748146, r=475.54437078014035, b=236.89013130797252, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='', annotations=[], provenance=None, predicted_class=None, confidence=None), TextElement(label=<DocItemLabel.CAPTION: 'caption'>, id=4, page_no=8, cluster=Cluster(id=4, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=134.1597137451172, t=287.998046875, r=480.640380859375, b=322.92626953125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9685500264167786, cells=[Cell(id=21, text='Fig. 5: A common form of CNN architecture in which convolutional layers are', bbox=BoundingBox(l=134.76500022108476, t=288.8927615951013, r=480.58682078841264, b=298.825431649944, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='stacked between ReLus continuously before being passed through the pooling', bbox=BoundingBox(l=134.76500022108476, t=300.8477716611102, r=480.5867307884124, b=310.7804517159527, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='layer, before going between one or many fully connected ReLus.', bbox=BoundingBox(l=134.76500022108476, t=312.8027917271189, r=417.1848806844004, b=322.73547178196156, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Fig. 5: A common form of CNN architecture in which convolutional layers are stacked between ReLus continuously before being passed through the pooling layer, before going between one or many fully connected ReLus.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=8, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.85641479492188, t=348.0907897949219, r=480.6982727050781, b=478.8450622558594, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9880061745643616, cells=[Cell(id=24, text='It is also advised to split large convolutional layers up into many smaller sized', bbox=BoundingBox(l=134.76500022108476, t=349.1437619277732, r=480.58682078841264, b=359.0764719826159, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='convolutional layers. This is to reduce the amount of computational complexity', bbox=BoundingBox(l=134.76500022108476, t=361.0987519937818, r=480.5867307884124, b=371.0314620486245, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='within a given convolutional layer. For example, if you were to stack three con-', bbox=BoundingBox(l=134.76500022108476, t=373.0537420597904, r=480.58679078841254, b=382.9864521146332, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='volutional layers on top of each other with a receptive field of', bbox=BoundingBox(l=134.76500022108476, t=385.00973212580453, r=400.48746065700794, b=394.9424421806474, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='3', bbox=BoundingBox(l=402.51202066032926, t=385.2189621269598, r=407.4933206685012, b=394.0657321758067, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='×', bbox=BoundingBox(l=407.9780306692964, t=384.6610421238793, r=415.7269306820086, b=394.0657321758067, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='3', bbox=BoundingBox(l=416.2110306828028, t=385.2189621269598, r=421.19232069097467, b=394.0657321758067, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='. Each neuron', bbox=BoundingBox(l=421.19202069097423, t=385.00973212580453, r=480.5890507884163, b=394.9424421806474, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='of the first convolutional layer will have a', bbox=BoundingBox(l=134.76501022108476, t=396.9647221918132, r=316.8913005198667, b=406.897432246656, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='3', bbox=BoundingBox(l=319.1640005235951, t=397.1739521929685, r=324.145290531767, b=406.02072224181535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='×', bbox=BoundingBox(l=325.55801053408464, t=396.6160221898879, r=333.30692054679685, b=406.02072224181535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='3', bbox=BoundingBox(l=334.72000054911507, t=397.1739521929685, r=339.70129055728694, b=406.02072224181535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='view of the input vector. A neu-', bbox=BoundingBox(l=341.97601056101865, t=396.9647221918132, r=480.59567078842707, b=406.897432246656, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='ron on the second convolutional layer will then have a', bbox=BoundingBox(l=134.76501022108476, t=408.9197022578219, r=376.1587806170962, b=418.8524123126646, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='5', bbox=BoundingBox(l=378.9180306216228, t=409.1289322589771, r=383.8993206297947, b=417.9757023078239, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='×', bbox=BoundingBox(l=386.3090206337479, t=408.5710122558965, r=394.05792064646016, b=417.9757023078239, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='5', bbox=BoundingBox(l=396.4670106504123, t=409.1289322589771, r=401.44830065858423, b=417.9757023078239, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='view of the input', bbox=BoundingBox(l=404.2030006631033, t=408.9197022578219, r=480.5961907884279, b=418.8524123126646, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='vector. A neuron on the third convolutional layer will then have a', bbox=BoundingBox(l=134.76501022108476, t=420.8746923238305, r=421.9369506921963, b=430.8074023786733, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='7', bbox=BoundingBox(l=424.30200069607616, t=421.0839223249858, r=429.2832907042481, b=429.9306923738326, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='×', bbox=BoundingBox(l=431.026000707107, t=420.5260023219052, r=438.7749007198193, b=429.9306923738326, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='7', bbox=BoundingBox(l=440.51700072267727, t=421.0839223249858, r=445.49829073084913, b=429.9306923738326, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='view of', bbox=BoundingBox(l=447.86200073472685, t=420.8746923238305, r=480.5891407884164, b=430.8074023786733, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='the input vector. As these stacks feature non-linearities which in turn allows us', bbox=BoundingBox(l=134.76501022108476, t=432.82968238983915, r=480.58679078841254, b=442.76239244468195, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='to express stronger features of the input with fewer parameters. However, it is', bbox=BoundingBox(l=134.76501022108476, t=444.78466245584775, r=480.58682078841264, b=454.7173725106905, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='important to understand that this does come with a distinct memory allocation', bbox=BoundingBox(l=134.76501022108476, t=456.74066252186196, r=480.58679078841254, b=466.67337257670476, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='problem - especially when making use of the backpropagation algorithm.', bbox=BoundingBox(l=134.76501022108476, t=468.69564258787057, r=457.4038107503804, b=478.62835264271337, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='It is also advised to split large convolutional layers up into many smaller sized convolutional layers. This is to reduce the amount of computational complexity within a given convolutional layer. For example, if you were to stack three convolutional layers on top of each other with a receptive field of 3 × 3 . Each neuron of the first convolutional layer will have a 3 × 3 view of the input vector. A neuron on the second convolutional layer will then have a 5 × 5 view of the input vector. A neuron on the third convolutional layer will then have a 7 × 7 view of the input vector. As these stacks feature non-linearities which in turn allows us to express stronger features of the input with fewer parameters. However, it is important to understand that this does come with a distinct memory allocation problem - especially when making use of the backpropagation algorithm.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=8, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.77923583984375, t=486.6978454589844, r=480.58676078841245, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.966922402381897, cells=[Cell(id=51, text='The input layer should be recursively divisible by two. Common numbers in-', bbox=BoundingBox(l=134.76501022108476, t=487.5596626920269, r=480.58676078841245, b=497.4923727468697, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='clude', bbox=BoundingBox(l=134.76501022108476, t=499.51464275803556, r=158.95421026076767, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='32', bbox=BoundingBox(l=161.44402026485224, t=499.72387275919084, r=171.4066202811961, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='×', bbox=BoundingBox(l=173.62102028482886, t=499.16595275611024, r=181.36993029754112, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='32', bbox=BoundingBox(l=183.58302030117173, t=499.72387275919084, r=193.54562031751558, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text=',', bbox=BoundingBox(l=193.54602031751622, t=499.51464275803556, r=196.0366703216022, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='64', bbox=BoundingBox(l=198.52702032568766, t=499.72387275919084, r=208.48962034203151, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='×', bbox=BoundingBox(l=210.7040303456643, t=499.16595275611024, r=218.45294035837657, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='64', bbox=BoundingBox(l=220.66603036200718, t=499.72387275919084, r=230.628630378351, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text=',', bbox=BoundingBox(l=230.6290303783517, t=499.51464275803556, r=233.11967038243762, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='96', bbox=BoundingBox(l=235.6100303865231, t=499.72387275919084, r=245.572630402867, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='×', bbox=BoundingBox(l=247.7870304064997, t=499.16595275611024, r=255.535950419212, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='96', bbox=BoundingBox(l=257.7500304228442, t=499.72387275919084, r=267.7126204391881, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text=',', bbox=BoundingBox(l=267.7120404391871, t=499.51464275803556, r=270.20270044327305, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=65, text='128', bbox=BoundingBox(l=272.69403044736015, t=499.72387275919084, r=287.637940471876, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='×', bbox=BoundingBox(l=289.85104047550664, t=499.16595275611024, r=297.5999504882188, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='128', bbox=BoundingBox(l=299.81406049185114, t=499.72387275919084, r=314.7579705163669, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=68, text='and', bbox=BoundingBox(l=317.2490505204536, t=499.51464275803556, r=334.1157205481237, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=69, text='224', bbox=BoundingBox(l=336.6060505522091, t=499.72387275919084, r=351.54996057672497, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=70, text='×', bbox=BoundingBox(l=353.76404058035723, t=499.16595275611024, r=361.51294059306946, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=71, text='224', bbox=BoundingBox(l=363.72705059670176, t=499.72387275919084, r=378.6709606212175, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=72, text='.', bbox=BoundingBox(l=378.67105062121766, t=499.51464275803556, r=381.16171062530367, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The input layer should be recursively divisible by two. Common numbers include 32 × 32 , 64 × 64 , 96 × 96 , 128 × 128 and 224 × 224 .'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=8, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.83213806152344, t=517.3685302734375, r=480.58688078841266, b=564.4635009765625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.982406735420227, cells=[Cell(id=73, text='Whilst using small filters, set stride to one and make use of zero-padding as to', bbox=BoundingBox(l=134.76505022108483, t=518.3796328621972, r=480.5867307884124, b=528.31234291704, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=74, text='ensure that the convolutional layers do not reconfigure any of the dimension-', bbox=BoundingBox(l=134.76505022108483, t=530.3346229282059, r=480.58682078841264, b=540.2673329830487, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=75, text='ality of the input. The amount of zero-padding to be used should be calculated', bbox=BoundingBox(l=134.76505022108483, t=542.2896129942146, r=480.5868507884127, b=552.2223230490574, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=76, text='by taking one away from the receptive field size and dividing by two.activation', bbox=BoundingBox(l=134.76505022108483, t=554.2445930602232, r=480.58688078841266, b=564.177303115066, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Whilst using small filters, set stride to one and make use of zero-padding as to ensure that the convolutional layers do not reconfigure any of the dimensionality of the input. The amount of zero-padding to be used should be calculated by taking one away from the receptive field size and dividing by two.activation'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=8, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.7228240966797, t=571.9942626953125, r=480.7103271484375, b=667.2128295898438, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9852842688560486, cells=[Cell(id=77, text='CNNs are extremely powerful machine learning algorithms, however they can', bbox=BoundingBox(l=134.76505022108483, t=573.109583164385, r=480.5868507884127, b=583.0422932192278, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=78, text='be horrendously resource-heavy. An example of this problem could be in filter-', bbox=BoundingBox(l=134.76505022108483, t=585.0646032303937, r=480.58698078841286, b=594.9972932852364, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=79, text='ing a large image (anything over', bbox=BoundingBox(l=134.76505022108483, t=597.0195932964024, r=277.987370456044, b=606.9523033512452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=80, text='128', bbox=BoundingBox(l=280.46405046010705, t=597.2288032975575, r=295.40796048462283, b=606.0755933464045, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=81, text='×', bbox=BoundingBox(l=297.56506048816163, t=596.6708932944771, r=305.31396050087386, b=606.0755933464045, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=82, text='128', bbox=BoundingBox(l=307.470060504411, t=597.2288032975575, r=322.41397052892677, b=606.0755933464045, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=83, text='could be considered large), so if the', bbox=BoundingBox(l=324.8900805329889, t=597.0195932964024, r=480.59555078842686, b=606.9523033512452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=84, text='input is', bbox=BoundingBox(l=134.7650802210849, t=608.974593362411, r=168.67776027671934, b=618.9073034172538, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=85, text='227', bbox=BoundingBox(l=171.53407028140518, t=609.1838033635662, r=186.47797030592096, b=618.0305934124132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=86, text='×', bbox=BoundingBox(l=188.9600703099929, t=608.6259033604858, r=196.70898032270514, b=618.0305934124132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=87, text='227', bbox=BoundingBox(l=199.19107032677704, t=609.1838033635662, r=214.13496035129282, b=618.0305934124132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=88, text='(as seen with ImageNet) and we’re filtering with 64 kernels', bbox=BoundingBox(l=216.98807035597338, t=608.974593362411, r=480.5884707884153, b=618.9073034172538, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=89, text='each with a zero padding of then the result will be three activation vectors of', bbox=BoundingBox(l=134.7650802210849, t=620.9295934284198, r=480.5868507884127, b=630.8623034832626, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=90, text='size', bbox=BoundingBox(l=134.7650802210849, t=632.885583494434, r=151.64172024877135, b=642.8182935492767, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=91, text='227', bbox=BoundingBox(l=154.46307025339985, t=633.0948034955892, r=169.4069702779156, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=92, text='×', bbox=BoundingBox(l=171.86607028194985, t=632.5368934925087, r=179.6149902946621, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=93, text='227', bbox=BoundingBox(l=182.07407029869626, t=633.0948034955892, r=197.017960323212, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=94, text='×', bbox=BoundingBox(l=199.47707032724625, t=632.5368934925087, r=207.22598033995848, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=95, text='64', bbox=BoundingBox(l=209.68506034399266, t=633.0948034955892, r=219.64766036033652, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=96, text='- which calculates to roughly 10 million activations - or an', bbox=BoundingBox(l=222.4690603649651, t=632.885583494434, r=480.590120788418, b=642.8182935492767, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=97, text='enormous 70 megabytes of memory per image. In this case you have two op-', bbox=BoundingBox(l=134.76505022108483, t=644.8405935604427, r=480.58679078841254, b=654.7733036152855, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=98, text='tions. Firstly, you can reduce the spatial dimensionality of the input images by', bbox=BoundingBox(l=134.76505022108483, t=656.7955936264514, r=480.58676078841245, b=666.7283036812942, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='CNNs are extremely powerful machine learning algorithms, however they can be horrendously resource-heavy. An example of this problem could be in filtering a large image (anything over 128 × 128 could be considered large), so if the input is 227 × 227 (as seen with ImageNet) and we’re filtering with 64 kernels each with a zero padding of then the result will be three activation vectors of size 227 × 227 × 64 - which calculates to roughly 10 million activations - or an enormous 70 megabytes of memory per image. In this case you have two options. Firstly, you can reduce the spatial dimensionality of the input images by'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=9, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=134.48631286621094, t=93.9991683959961, r=143.73849487304688, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8442774415016174, cells=[Cell(id=0, text='10', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=143.73140023579433, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='10'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=9, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=167.15977478027344, t=93.5068588256836, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8268627524375916, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea et al.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=9, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8414764404297, t=118.74977111816406, r=480.58688078841266, b=153.6955108642578, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9780817627906799, cells=[Cell(id=2, text='resizing the raw images to something a little less heavy. Alternatively, you can', bbox=BoundingBox(l=134.76500022108476, t=119.67181066075966, r=480.58682078841264, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='go against everything we stated earlier in this document and opt for larger filter', bbox=BoundingBox(l=134.76500022108476, t=131.6268307267684, r=480.58688078841266, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='sizes with a larger stride (2, as opposed to 1).', bbox=BoundingBox(l=134.76500022108476, t=143.58184079277714, r=331.9746705446113, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='resizing the raw images to something a little less heavy. Alternatively, you can go against everything we stated earlier in this document and opt for larger filter sizes with a larger stride (2, as opposed to 1).'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=9, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=134.21324157714844, t=160.7611083984375, r=480.58679078841254, b=207.31256114466203, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9785709977149963, cells=[Cell(id=5, text='In addition to the few rules-of-thumb outlined above, it is also important to ac-', bbox=BoundingBox(l=134.76500022108476, t=161.51483089179294, r=480.58679078841254, b=171.4475109466356, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='knowledge a few ’tricks’ about generalised ANN training techniques. The au-', bbox=BoundingBox(l=134.76500022108476, t=173.4698409578017, r=480.5867307884124, b=183.40252101264423, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='thors suggest a read of Geoffrey Hinton’s excellent “Practical Guide to Training', bbox=BoundingBox(l=134.76500022108476, t=185.42486102381054, r=480.58670078841243, b=195.35754107865318, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='Restricted Boltzmann Machines”.', bbox=BoundingBox(l=134.76500022108476, t=197.37988108981926, r=281.8328904623527, b=207.31256114466203, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='In addition to the few rules-of-thumb outlined above, it is also important to acknowledge a few ’tricks’ about generalised ANN training techniques. The authors suggest a read of Geoffrey Hinton’s excellent “Practical Guide to Training Restricted Boltzmann Machines”.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=4, page_no=9, cluster=Cluster(id=4, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.84384155273438, t=231.3874969482422, r=215.1159103529021, b=243.4951713444417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9349105358123779, cells=[Cell(id=9, text='4', bbox=BoundingBox(l=134.76500022108476, t=231.85083128014833, r=140.74260023089116, b=243.4951713444417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='Conclusion', bbox=BoundingBox(l=152.6978002505039, t=231.85083128014833, r=215.1159103529021, b=243.4951713444417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='4 Conclusion'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=9, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.68148803710938, t=259.8370056152344, r=480.58682078841264, b=306.9059143066406, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.982923686504364, cells=[Cell(id=11, text='Convolutional Neural Networks differ to other forms of Artifical Neural Net-', bbox=BoundingBox(l=134.76500022108476, t=261.03387144128044, r=480.58676078841245, b=270.9665514961231, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='work in that instead of focusing on the entirety of the problem domain, knowl-', bbox=BoundingBox(l=134.76500022108476, t=272.98986150729456, r=480.58682078841264, b=282.9225415621372, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='edge about the specific type of input is exploited. This in turn allows for a much', bbox=BoundingBox(l=134.76500022108476, t=284.9448815733035, r=480.5867307884124, b=294.87756162814617, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='simpler network architecture to be set up.', bbox=BoundingBox(l=134.76500022108476, t=296.89990163931225, r=318.04703052176274, b=306.8325816941549, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Convolutional Neural Networks differ to other forms of Artifical Neural Network in that instead of focusing on the entirety of the problem domain, knowledge about the specific type of input is exploited. This in turn allows for a much simpler network architecture to be set up.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=9, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.798095703125, t=313.9832458496094, r=480.58679078841254, b=348.8280944824219, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9790396094322205, cells=[Cell(id=15, text='This paper has outlined the basic concepts of Convolutional Neural Networks,', bbox=BoundingBox(l=134.76500022108476, t=314.83288173832796, r=480.58679078841254, b=324.7655617931705, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='explaining the layers required to build one and detailing how best to structure', bbox=BoundingBox(l=134.76500022108476, t=326.7878418043364, r=480.5867307884124, b=336.7205518591792, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='the network in most image analysis tasks.', bbox=BoundingBox(l=134.76500022108476, t=338.74282187034504, r=318.21628052204034, b=348.67553192518784, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='This paper has outlined the basic concepts of Convolutional Neural Networks, explaining the layers required to build one and detailing how best to structure the network in most image analysis tasks.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=9, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8413543701172, t=356.0538024902344, r=480.8612365722656, b=426.3844923542525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9865900278091431, cells=[Cell(id=18, text='Research in the field of image analysis using neural networks has somewhat', bbox=BoundingBox(l=134.76500022108476, t=356.675841969361, r=480.58667078841233, b=366.60855202420373, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='slowed in recent times. This is partly due to the incorrect belief surrounding the', bbox=BoundingBox(l=134.76500022108476, t=368.6308220353696, r=480.58676078841245, b=378.5635320902124, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='level of complexity and knowledge required to begin modelling these superbly', bbox=BoundingBox(l=134.76500022108476, t=380.5858121013783, r=480.58676078841245, b=390.518522156221, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='powerful machine learning algorithms. The authors hope that this paper has', bbox=BoundingBox(l=134.76500022108476, t=392.54080216738686, r=480.58679078841254, b=402.47351222222966, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='in some way reduced this confusion, and made the field more accessible to', bbox=BoundingBox(l=134.76500022108476, t=404.49679223340104, r=480.58667078841233, b=414.4295022882438, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='beginners.', bbox=BoundingBox(l=134.76500022108476, t=416.4517822994097, r=180.5033002961194, b=426.3844923542525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Research in the field of image analysis using neural networks has somewhat slowed in recent times. This is partly due to the incorrect belief surrounding the level of complexity and knowledge required to begin modelling these superbly powerful machine learning algorithms. The authors hope that this paper has in some way reduced this confusion, and made the field more accessible to beginners.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=8, page_no=9, cluster=Cluster(id=8, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.9271240234375, t=450.19580078125, r=243.67688039975695, b=462.5867004394531, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9575116038322449, cells=[Cell(id=24, text='Acknowledgements', bbox=BoundingBox(l=134.76500022108476, t=450.9227624897389, r=243.67688039975695, b=462.5671325540324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Acknowledgements'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=9, page_no=9, cluster=Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.86122131347656, t=479.3764343261719, r=480.82696533203125, b=502.0371398925781, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9725787043571472, cells=[Cell(id=25, text='The authors would like to thank Dr. Chuan Lu and Nicholas Dimonaco for', bbox=BoundingBox(l=134.76500022108476, t=480.105802650871, r=480.58667078841233, b=490.03851270571374, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='useful discussion and suggestions.', bbox=BoundingBox(l=134.76500022108476, t=492.0607927168796, r=286.96362047076974, b=501.99350277172243, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The authors would like to thank Dr. Chuan Lu and Nicholas Dimonaco for useful discussion and suggestions.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=10, page_no=9, cluster=Cluster(id=10, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.29351806640625, t=525.9068603515625, r=194.5290503191289, b=538.1761429715024, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9404545426368713, cells=[Cell(id=27, text='References', bbox=BoundingBox(l=134.76500022108476, t=526.5317629072088, r=194.5290503191289, b=538.1761429715024, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='References'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=11, page_no=9, cluster=Cluster(id=11, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.7662811279297, t=548.0494995117188, r=480.59470078842554, b=579.7995932013234, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.973379373550415, cells=[Cell(id=28, text='1.', bbox=BoundingBox(l=139.2480002284392, t=548.9420730309457, r=146.00546023952495, b=557.8815930803046, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for im-', bbox=BoundingBox(l=148.25793024322016, t=548.9420730309457, r=480.5899007884176, b=557.8815930803046, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='age classification. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE', bbox=BoundingBox(l=150.95399024764313, t=559.9010930914551, r=480.59470078842554, b=568.840603140814, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='Conference on. pp. 3642-3649. IEEE (2012)', bbox=BoundingBox(l=150.95399024764313, t=570.8600731519643, r=318.41046052235896, b=579.7995932013234, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='1. Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for image classification. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. pp. 3642-3649. IEEE (2012)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=12, page_no=9, cluster=Cluster(id=12, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.2350311279297, t=580.685791015625, r=480.658203125, b=623.1235834405338, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9770330190658569, cells=[Cell(id=32, text='2.', bbox=BoundingBox(l=139.2480002284392, t=581.3070932096468, r=146.07829023964445, b=590.2465832590057, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='Cires¸an, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in', bbox=BoundingBox(l=148.35506024337954, t=581.3070932096468, r=480.5899007884176, b=590.2465832590057, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='breast cancer histology images with deep neural networks. In: Medical Image Com-', bbox=BoundingBox(l=150.95399024764313, t=592.2660832701563, r=480.59467078842545, b=601.2055833195151, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='puting and Computer-Assisted Intervention-MICCAI 2013, pp. 411-418. Springer', bbox=BoundingBox(l=150.95399024764313, t=603.2250833306656, r=480.59479078842566, b=612.1645833800244, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='(2013)', bbox=BoundingBox(l=150.95399024764313, t=614.1840833911749, r=174.85841028685883, b=623.1235834405338, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2. Cires¸an, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in breast cancer histology images with deep neural networks. In: Medical Image Computing and Computer-Assisted Intervention-MICCAI 2013, pp. 411-418. Springer (2013)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=13, page_no=9, cluster=Cluster(id=13, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.2299041748047, t=624.0062255859375, r=480.5948507884258, b=666.5886840820312, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9793604612350464, cells=[Cell(id=37, text='3.', bbox=BoundingBox(l=139.2480002284392, t=624.6310834488573, r=146.0001202395162, b=633.5705834982161, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible,', bbox=BoundingBox(l=148.25081024320852, t=624.6310834488573, r=480.5899007884176, b=633.5705834982161, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='high performance convolutional neural networks for image classification. In: IJCAI', bbox=BoundingBox(l=150.95399024764313, t=635.5900835093667, r=480.5948507884258, b=644.5295835587256, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='Proceedings-International Joint Conference on Artificial Intelligence. vol. 22, p. 1237', bbox=BoundingBox(l=150.95399024764313, t=646.5490835698761, r=480.59467078842545, b=655.4885836192349, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='(2011)', bbox=BoundingBox(l=150.95399024764313, t=657.5080836303854, r=174.85841028685883, b=666.4475836797442, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='3. Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible, high performance convolutional neural networks for image classification. In: IJCAI Proceedings-International Joint Conference on Artificial Intelligence. vol. 22, p. 1237 (2011)'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=10, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=256.0866394042969, t=93.61783599853516, r=447.5473937988281, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9234234094619751, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=10, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=471.0235290527344, t=93.66058349609375, r=480.5921307884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.889333963394165, cells=[Cell(id=1, text='11', bbox=BoundingBox(l=471.6257307737117, t=94.48107052167063, r=480.5921307884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='11'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=2, page_no=10, cluster=Cluster(id=2, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.15142822265625, t=119.36505126953125, r=480.60302734375, b=162.19952089557341, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.970146894454956, cells=[Cell(id=2, text='4.', bbox=BoundingBox(l=139.24802022843926, t=120.3840906646924, r=145.95924023944914, b=129.32354071405098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='Cires¸an, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural', bbox=BoundingBox(l=148.19632024311912, t=120.3840906646924, r=480.58994078841766, b=129.32354071405098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='network committees for handwritten character classification. In: Document Analysis', bbox=BoundingBox(l=150.95401024764317, t=131.34307072520176, r=480.5947307884256, b=140.2825307745602, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='and Recognition (ICDAR), 2011 International Conference on. pp. 1135-1139. IEEE', bbox=BoundingBox(l=150.95401024764317, t=142.30206078571098, r=480.5947307884256, b=151.24151083506956, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='(2011)', bbox=BoundingBox(l=150.95401024764317, t=153.26007084621483, r=174.85843028685886, b=162.19952089557341, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='4. Cires¸an, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural network committees for handwritten character classification. In: Document Analysis and Recognition (ICDAR), 2011 International Conference on. pp. 1135-1139. IEEE (2011)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=3, page_no=10, cluster=Cluster(id=3, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.33560180664062, t=163.44866943359375, r=480.71539306640625, b=184.11749101659188, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9617682695388794, cells=[Cell(id=7, text='5.', bbox=BoundingBox(l=139.24802022843926, t=164.21905090672408, r=145.96364023945637, b=173.15850095608266, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural net-', bbox=BoundingBox(l=148.20218024312874, t=164.21905090672408, r=480.58978078841744, b=173.15850095608266, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='worksa review. Pattern recognition 35(10), 2279-2301 (2002)', bbox=BoundingBox(l=150.95401024764317, t=175.1780309672332, r=386.3400006337987, b=184.11749101659188, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='5. Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural networksa review. Pattern recognition 35(10), 2279-2301 (2002)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=4, page_no=10, cluster=Cluster(id=4, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.4720458984375, t=185.40130615234375, r=480.6998291015625, b=227.95343125862905, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9742252230644226, cells=[Cell(id=10, text='6.', bbox=BoundingBox(l=139.24802022843926, t=186.13702102774266, r=146.00636023952646, b=195.07647107710125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware', bbox=BoundingBox(l=148.25916024322223, t=186.13702102774266, r=480.58987078841767, b=195.07647107710125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='accelerated convolutional neural networks for synthetic vision systems. In: Circuits', bbox=BoundingBox(l=150.95401024764317, t=197.0960010882519, r=480.59467078842545, b=206.03546113761047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp.', bbox=BoundingBox(l=150.95401024764317, t=208.05499114876113, r=480.5948507884258, b=216.99444119811972, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='257-260. IEEE (2010)', bbox=BoundingBox(l=150.95401024764317, t=219.0139712092705, r=232.4227303812943, b=227.95343125862905, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='6. Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware accelerated convolutional neural networks for synthetic vision systems. In: Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp. 257-260. IEEE (2010)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=5, page_no=10, cluster=Cluster(id=5, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.4396514892578, t=229.38987731933594, r=480.5898107884175, b=249.87139137964755, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9618682861328125, cells=[Cell(id=15, text='7.', bbox=BoundingBox(l=139.24802022843926, t=229.97296126977972, r=145.9334702394069, b=238.9124113191383, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum', bbox=BoundingBox(l=148.16196024306274, t=229.97296126977972, r=480.5898107884175, b=238.9124113191383, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='9(1), 926 (2010)', bbox=BoundingBox(l=150.95401024764317, t=240.93194133028896, r=209.97086034446153, b=249.87139137964755, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='7. Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum 9(1), 926 (2010)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=6, page_no=10, cluster=Cluster(id=6, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.49429321289062, t=251.51890563964844, r=480.5947307884256, b=282.74737156117, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.974453330039978, cells=[Cell(id=18, text='8.', bbox=BoundingBox(l=139.24802022843926, t=251.8909313907983, r=146.05434023960515, b=260.8303814401569, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Im-', bbox=BoundingBox(l=148.3231202433271, t=251.8909313907983, r=480.5898107884175, b=260.8303814401569, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='proving neural networks by preventing co-adaptation of feature detectors. arXiv', bbox=BoundingBox(l=150.95401024764317, t=262.84991145130755, r=480.5947307884256, b=271.78936150066613, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='preprint arXiv:1207.0580 (2012)', bbox=BoundingBox(l=150.95401024764317, t=273.8079215118115, r=274.11649044969374, b=282.74737156117, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='8. Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 (2012)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=7, page_no=10, cluster=Cluster(id=7, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.10630798339844, t=284.2738342285156, r=480.59470078842554, b=315.62432174269793, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9755162000656128, cells=[Cell(id=22, text='9.', bbox=BoundingBox(l=139.24802022843926, t=284.76690157232076, r=146.0757602396403, b=293.70636162167943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action', bbox=BoundingBox(l=148.35168024337398, t=284.76690157232076, r=480.5898107884175, b=293.70636162167943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1),', bbox=BoundingBox(l=150.95401024764317, t=295.72589163283, r=480.59470078842554, b=304.66534168218857, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='221-231 (2013)', bbox=BoundingBox(l=150.95401024764317, t=306.68487169333923, r=208.4824203420197, b=315.62432174269793, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='9. Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1), 221-231 (2013)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=8, page_no=10, cluster=Cluster(id=8, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.7255096435547, t=316.7909851074219, r=480.77935791015625, b=359.46029198473514, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9749436378479004, cells=[Cell(id=26, text='10.', bbox=BoundingBox(l=134.76501022108476, t=317.6438617538487, r=146.3162402400348, b=326.58331180320715, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-', bbox=BoundingBox(l=148.62648024382477, t=317.6438617538487, r=480.590120788418, b=326.58331180320715, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='scale video classification with convolutional neural networks. In: Computer Vision', bbox=BoundingBox(l=150.95401024764317, t=328.6028418143578, r=480.59476078842556, b=337.54232186371655, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725-1732. IEEE', bbox=BoundingBox(l=150.95401024764317, t=339.56179187486697, r=480.5947307884256, b=348.5013119242259, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='(2014)', bbox=BoundingBox(l=150.95401024764317, t=350.5207819353762, r=174.85843028685886, b=359.46029198473514, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='10. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Largescale video classification with convolutional neural networks. In: Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725-1732. IEEE (2014)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=9, page_no=10, cluster=Cluster(id=9, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.47573852539062, t=360.74285888671875, r=480.59479078842566, b=392.3362721662576, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9674268960952759, cells=[Cell(id=31, text='11.', bbox=BoundingBox(l=134.76501022108476, t=361.4797619958855, r=145.9576402394465, b=370.4192820452444, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convo-', bbox=BoundingBox(l=148.19617024311887, t=361.4797619958855, r=480.5902407884182, b=370.4192820452444, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='lutional neural networks. In: Advances in neural information processing systems.', bbox=BoundingBox(l=150.95401024764317, t=372.4387520563948, r=480.59479078842566, b=381.37826210575366, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='pp. 1097-1105 (2012)', bbox=BoundingBox(l=150.95401024764317, t=383.39675211689865, r=232.70963038176492, b=392.3362721662576, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='11. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems. pp. 1097-1105 (2012)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=10, page_no=10, cluster=Cluster(id=10, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.62811279296875, t=393.55804443359375, r=480.59467078842545, b=425.21322234778535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9672126173973083, cells=[Cell(id=35, text='12.', bbox=BoundingBox(l=134.76501022108476, t=394.355742177408, r=145.86594023929607, b=403.2952522267668, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel,', bbox=BoundingBox(l=148.08614024293834, t=394.355742177408, r=480.5900307884179, b=403.2952522267668, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='L.D.: Backpropagation applied to handwritten zip code recognition. Neural compu-', bbox=BoundingBox(l=150.95401024764317, t=405.3147222379172, r=480.59467078842545, b=414.25424228727616, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='tation 1(4), 541-551 (1989)', bbox=BoundingBox(l=150.95401024764317, t=416.2737122984265, r=253.19788041537635, b=425.21322234778535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='12. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural computation 1(4), 541-551 (1989)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=11, page_no=10, cluster=Cluster(id=11, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.47447204589844, t=426.6608581542969, r=480.59009078841797, b=447.1311924688039, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9590675234794617, cells=[Cell(id=39, text='13.', bbox=BoundingBox(l=134.76501022108476, t=427.2326923589357, r=145.78894023916976, b=436.1722124082947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to doc-', bbox=BoundingBox(l=147.99373024278677, t=427.2326923589357, r=480.59009078841797, b=436.1722124082947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='ument recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998)', bbox=BoundingBox(l=150.95401024764317, t=438.1916824194451, r=420.4302706897245, b=447.1311924688039, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='13. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=12, page_no=10, cluster=Cluster(id=12, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.4318084716797, t=448.2698059082031, r=480.5900607884179, b=469.0491625898225, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9524788856506348, cells=[Cell(id=42, text='14.', bbox=BoundingBox(l=134.76501022108476, t=449.1506624799543, r=146.421830240208, b=458.0901725293132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='Nebauer, C.: Evaluation of convolutional neural networks for visual recognition.', bbox=BoundingBox(l=148.75319024403268, t=449.1506624799543, r=480.5900607884179, b=458.0901725293132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='Neural Networks, IEEE Transactions on 9(4), 685-696 (1998)', bbox=BoundingBox(l=150.95401024764317, t=460.1096525404636, r=387.7297706360787, b=469.0491625898225, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='14. Nebauer, C.: Evaluation of convolutional neural networks for visual recognition. Neural Networks, IEEE Transactions on 9(4), 685-696 (1998)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=13, page_no=10, cluster=Cluster(id=13, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.57110595703125, t=470.0508117675781, r=480.6988525390625, b=490.96713271084104, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9601881504058838, cells=[Cell(id=45, text='15.', bbox=BoundingBox(l=134.76501022108476, t=471.0686326009729, r=146.22498023988507, b=480.00814265033176, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural net-', bbox=BoundingBox(l=148.51697024364515, t=471.0686326009729, r=480.590120788418, b=480.00814265033176, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='works applied to visual document analysis. In: null. p. 958. IEEE (2003)', bbox=BoundingBox(l=150.95401024764317, t=482.0276126614821, r=431.96100070864094, b=490.96713271084104, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='15. Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural networks applied to visual document analysis. In: null. p. 958. IEEE (2003)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=14, page_no=10, cluster=Cluster(id=14, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.7103729248047, t=492.13946533203125, r=480.5900607884179, b=512.8841228318543, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9577903747558594, cells=[Cell(id=48, text='16.', bbox=BoundingBox(l=134.76501022108476, t=492.98562272198603, r=145.95445023944131, b=501.9251427713449, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of', bbox=BoundingBox(l=148.1923402431126, t=492.98562272198603, r=480.5900607884179, b=501.9251427713449, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='Toronto (2013)', bbox=BoundingBox(l=150.95401024764317, t=503.9446127824953, r=207.980320341196, b=512.8841228318543, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='16. Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of Toronto (2013)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=15, page_no=10, cluster=Cluster(id=15, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.64859008789062, t=514.1981811523438, r=480.5948207884257, b=545.761073013382, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9668182134628296, cells=[Cell(id=51, text='17.', bbox=BoundingBox(l=134.76501022108476, t=514.9035928430046, r=145.9241202393915, b=523.8431028923634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with con-', bbox=BoundingBox(l=148.15594024305284, t=514.9035928430046, r=480.590150788418, b=523.8431028923634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='volutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings.', bbox=BoundingBox(l=150.95401024764317, t=525.8625729035139, r=480.5948207884257, b=534.8020929528727, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='IEEE. pp. 224-229. IEEE (2005)', bbox=BoundingBox(l=150.95401024764317, t=536.8215629640231, r=271.6238104456045, b=545.761073013382, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='17. Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with convolutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings. IEEE. pp. 224-229. IEEE (2005)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=16, page_no=10, cluster=Cluster(id=16, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.76501022108476, t=546.84033203125, r=480.5900307884179, b=567.6790431344007, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9606278538703918, cells=[Cell(id=55, text='18.', bbox=BoundingBox(l=134.76501022108476, t=547.7805430245323, r=146.3605302401075, b=556.7200630738914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In:', bbox=BoundingBox(l=148.67964024391202, t=547.7805430245323, r=480.5900307884179, b=556.7200630738914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='Advances in Neural Information Processing Systems. pp. 2553-2561 (2013)', bbox=BoundingBox(l=150.95401024764317, t=558.7395330850417, r=445.8409707314113, b=567.6790431344007, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='18. Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In: Advances in Neural Information Processing Systems. pp. 2553-2561 (2013)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=17, page_no=10, cluster=Cluster(id=17, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.6785888671875, t=568.6400146484375, r=480.60009765625, b=600.720947265625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9559718370437622, cells=[Cell(id=58, text='19.', bbox=BoundingBox(l=134.76501022108476, t=569.6985131455509, r=146.7078102406772, b=578.6380331949099, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks', bbox=BoundingBox(l=149.09637024459568, t=569.6985131455509, r=480.590150788418, b=578.6380331949099, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='(siconnets) and their application of face detection. In: Neural Networks, 2003. Pro-', bbox=BoundingBox(l=150.95401024764317, t=580.6575332060604, r=480.59479078842566, b=589.5970132554191, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='ceedings of the International Joint Conference on. vol. 3, pp. 2157-2162. IEEE (2003)', bbox=BoundingBox(l=150.95401024764317, t=591.6165132665697, r=480.59479078842566, b=600.5560133159285, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='19. Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks (siconnets) and their application of face detection. In: Neural Networks, 2003. Proceedings of the International Joint Conference on. vol. 3, pp. 2157-2162. IEEE (2003)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=18, page_no=10, cluster=Cluster(id=18, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.3703155517578, t=601.6026611328125, r=480.59009078841797, b=622.4730234369417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.958541750907898, cells=[Cell(id=62, text='20.', bbox=BoundingBox(l=134.76501022108476, t=602.5745233270735, r=146.06206023961784, b=611.5140233764323, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional', bbox=BoundingBox(l=148.32147024332446, t=602.5745233270735, r=480.59009078841797, b=611.5140233764323, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='neural networks. arXiv preprint arXiv:1301.3557 (2013)', bbox=BoundingBox(l=150.95401024764317, t=613.5335233875829, r=367.45676060282034, b=622.4730234369417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='20. Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional neural networks. arXiv preprint arXiv:1301.3557 (2013)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=19, page_no=10, cluster=Cluster(id=19, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.26535034179688, t=623.5272216796875, r=480.590150788418, b=644.814208984375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9669718146324158, cells=[Cell(id=65, text='21.', bbox=BoundingBox(l=134.76501022108476, t=624.4925234480922, r=146.0039102395224, b=633.4320234974512, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In:', bbox=BoundingBox(l=148.25168024320993, t=624.4925234480922, r=480.590150788418, b=633.4320234974512, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='Computer Vision-ECCV 2014, pp. 818-833. Springer (2014)', bbox=BoundingBox(l=150.95401024764317, t=635.4515235086017, r=384.4032306306214, b=644.3910235579605, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='21. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In: Computer Vision-ECCV 2014, pp. 818-833. Springer (2014)')], body=[TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=0, page_no=0, cluster=Cluster(id=0, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=137.64463806152344, t=116.22373962402344, r=476.82874078224734, b=130.95611072306508, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8532029986381531, cells=[Cell(id=0, text='An Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=138.53101022726298, t=116.98291064591297, r=476.82874078224734, b=130.95611072306508, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='An Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=1, page_no=0, cluster=Cluster(id=1, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=235.75559997558594, t=160.30120088509193, r=378.7718811035156, b=171.6171875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7410858869552612, cells=[Cell(id=1, text='Keiron O’Shea', bbox=BoundingBox(l=236.21001038750742, t=161.63281089244435, r=299.9806504921245, b=171.565490947287, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=2, text='1', bbox=BoundingBox(l=299.981020492125, t=160.30120088509193, r=303.9526104986405, b=166.49395091928477, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='and Ryan Nash', bbox=BoundingBox(l=306.94101050354305, t=161.63281089244435, r=374.67673061466496, b=171.565490947287, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='2', bbox=BoundingBox(l=374.6760306146638, t=160.30120088509193, r=378.6476106211792, b=166.49395091928477, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea 1 and Ryan Nash 2'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=2, page_no=0, cluster=Cluster(id=2, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=140.86863708496094, t=181.55941772460938, r=474.033690777662, b=202.1148223876953, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.6404754519462585, cells=[Cell(id=5, text='1', bbox=BoundingBox(l=141.3270302318499, t=181.6796210031315, r=144.97993023784258, b=186.9877310324398, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB', bbox=BoundingBox(l=149.96103024601416, t=183.22613101167042, r=474.033690777662, b=192.1655810610289, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='keo7@aber.ac.uk', bbox=BoundingBox(l=267.3290404385588, t=194.99108107662983, r=348.0266405709449, b=201.9131411148494, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB keo7@aber.ac.uk'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=3, page_no=0, cluster=Cluster(id=3, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=139.36328125, t=203.46005249023438, r=475.54037078013374, b=225.04162124255174, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.6642215251922607, cells=[Cell(id=8, text='2', bbox=BoundingBox(l=139.82004022937767, t=203.5966811241451, r=143.47295023537032, b=208.9047811534533, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='School of Computing and Communications, Lancaster University, Lancashire, LA1', bbox=BoundingBox(l=148.45503024354352, t=205.14318113268382, r=475.54037078013374, b=214.0826411820425, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='4YW', bbox=BoundingBox(l=297.96301048881446, t=216.10217119319316, r=317.39319052069004, b=225.04162124255174, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2 School of Computing and Communications, Lancaster University, Lancashire, LA1 4YW'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=4, page_no=0, cluster=Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=245.39329528808594, t=226.80148315429688, r=369.5463306062484, b=234.79016129637762, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7631615996360779, cells=[Cell(id=11, text='nashrd@live.lancs.ac.uk', bbox=BoundingBox(l=245.8100104032564, t=227.86810125815794, r=369.5463306062484, b=234.79016129637762, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='nashrd@live.lancs.ac.uk'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=0, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=162.13299560546875, t=253.8007049560547, r=452.33795166015625, b=351.5365219409846, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9657419919967651, cells=[Cell(id=12, text='Abstract.', bbox=BoundingBox(l=163.11102026758698, t=254.98889140790345, r=199.71187032763143, b=263.72216145612356, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='The field of machine learning has taken a dramatic twist in re-', bbox=BoundingBox(l=204.6930203358031, t=254.926141407557, r=452.2463107419194, b=263.86560145691567, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='cent times, with the rise of the Artificial Neural Network (ANN). These', bbox=BoundingBox(l=163.11102026758698, t=265.88415146806096, r=452.2416707419117, b=274.82360151741955, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='biologically inspired computational models are able to far exceed the per-', bbox=BoundingBox(l=163.11102026758698, t=276.8431315285702, r=452.2416107419117, b=285.78259157792877, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='formance of previous forms of artificial intelligence in common machine', bbox=BoundingBox(l=163.11102026758698, t=287.80212158907943, r=452.2415207419116, b=296.7415716384379, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='learning tasks. One of the most impressive forms of ANN architecture is', bbox=BoundingBox(l=163.11102026758698, t=298.7611016495888, r=452.2415207419116, b=307.70056169894735, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='that of the Convolutional Neural Network (CNN). CNNs are primarily', bbox=BoundingBox(l=163.11102026758698, t=309.720091710098, r=452.2415807419116, b=318.6595417594566, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='used to solve difficult image-driven pattern recognition tasks and with', bbox=BoundingBox(l=163.11102026758698, t=320.67907177060727, r=452.2415207419116, b=329.6185318199658, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='their precise yet simple architecture, offers a simplified method of getting', bbox=BoundingBox(l=163.11102026758698, t=331.6380318311164, r=452.2415507419116, b=340.5775418804753, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='started with ANNs.', bbox=BoundingBox(l=163.11102026758698, t=342.5970118916257, r=241.19044039567788, b=351.5365219409846, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Abstract. The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=0, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=162.31292724609375, t=359.31500244140625, r=452.2415507419116, b=402.4580993652344, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9703543782234192, cells=[Cell(id=22, text='This document provides a brief introduction to CNNs, discussing recently', bbox=BoundingBox(l=163.11102026758698, t=360.5400019906967, r=452.2414607419114, b=369.47952204005566, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='published papers and newly formed techniques in developing these bril-', bbox=BoundingBox(l=163.11102026758698, t=371.49899205120596, r=452.2415507419116, b=380.43850210056485, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='liantly fantastic image recognition models. This introduction assumes you', bbox=BoundingBox(l=163.11102026758698, t=382.45700211170987, r=452.2415507419116, b=391.3965121610688, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='are familiar with the fundamentals of ANNs and machine learning.', bbox=BoundingBox(l=163.11102026758698, t=393.41598217221906, r=429.83463070515256, b=402.355492221578, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=0, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=162.44024658203125, t=421.299072265625, r=452.4108907421894, b=442.73138427734375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9514586925506592, cells=[Cell(id=26, text='Keywords:', bbox=BoundingBox(l=163.11102026758698, t=422.3807623321461, r=207.19881033991393, b=431.1140423803664, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='Pattern recognition, artificial neural networks, machine learn-', bbox=BoundingBox(l=211.6810303472671, t=422.31799233179953, r=452.4108907421894, b=431.2575023811584, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='ing, image analysis', bbox=BoundingBox(l=163.11102026758698, t=433.27700239230904, r=238.93089039197108, b=442.21652244166796, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keywords: Pattern recognition, artificial neural networks, machine learning, image analysis'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=8, page_no=0, cluster=Cluster(id=8, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.2177734375, t=471.4940490722656, r=221.76300036380678, b=483.745082670965, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9292116761207581, cells=[Cell(id=29, text='1', bbox=BoundingBox(l=134.76501022108476, t=472.10070260667135, r=140.74261023089116, b=483.745082670965, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='Introduction', bbox=BoundingBox(l=152.6978102505039, t=472.10070260667135, r=221.76300036380678, b=483.745082670965, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='1 Introduction'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=9, page_no=0, cluster=Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.661865234375, t=505.0313415527344, r=480.72076416015625, b=576.1156616210938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9834169149398804, cells=[Cell(id=31, text='Artificial Neural Networks', bbox=BoundingBox(l=134.76501022108476, t=506.41946279616, r=257.86292042302944, b=516.1230428497377, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='(ANNs) are computational processing systems of', bbox=BoundingBox(l=261.0410204282432, t=506.349732795775, r=480.58688078841266, b=516.2824428506178, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='which are heavily inspired by way biological nervous systems (such as the hu-', bbox=BoundingBox(l=134.76501022108476, t=518.3047128617836, r=480.58670078841243, b=528.2374229166264, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='man brain) operate. ANNs are mainly comprised of a high number of intercon-', bbox=BoundingBox(l=134.76501022108476, t=530.2597029277922, r=480.58676078841245, b=540.192412982635, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='nected computational nodes (referred to as neurons), of which work entwine in', bbox=BoundingBox(l=134.76501022108476, t=542.2146929938009, r=480.58682078841264, b=552.1474030486437, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='a distributed fashion to collectively learn from the input in order to optimise its', bbox=BoundingBox(l=134.76501022108476, t=554.1696730598095, r=480.58679078841254, b=564.1023831146523, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='final output.', bbox=BoundingBox(l=134.76501022108476, t=566.1256731258238, r=189.3899503106981, b=576.0583831806665, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Artificial Neural Networks (ANNs) are computational processing systems of which are heavily inspired by way biological nervous systems (such as the human brain) operate. ANNs are mainly comprised of a high number of interconnected computational nodes (referred to as neurons), of which work entwine in a distributed fashion to collectively learn from the input in order to optimise its final output.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=10, page_no=0, cluster=Cluster(id=10, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.71791076660156, t=583.977294921875, r=480.8759460449219, b=667.2293701171875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9865064024925232, cells=[Cell(id=38, text='The basic structure of a ANN can be modelled as shown in Figure 1. We would', bbox=BoundingBox(l=134.76501022108476, t=585.064663230394, r=480.5867307884124, b=594.9973732852368, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='load the input, usually in the form of a multidimensional vector to the input', bbox=BoundingBox(l=134.76501022108476, t=597.0196632964028, r=480.5867307884124, b=606.9523733512456, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='layer of which will distribute it to the hidden layers. The hidden layers will then', bbox=BoundingBox(l=134.76501022108476, t=608.9746733624115, r=480.5867307884124, b=618.9073734172542, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='make decisions from the previous layer and weigh up how a stochastic change', bbox=BoundingBox(l=134.76501022108476, t=620.9296734284203, r=480.58679078841254, b=630.8623834832631, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='within itself detriments or improves the final output, and this is referred to as', bbox=BoundingBox(l=134.76501022108476, t=632.884673494429, r=480.5867307884124, b=642.8173835492717, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='the process of learning. Having multiple hidden layers stacked upon each-other', bbox=BoundingBox(l=134.76501022108476, t=644.8406635604431, r=480.5867307884124, b=654.7733736152859, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='is commonly called deep learning.', bbox=BoundingBox(l=134.76501022108476, t=656.7956636264519, r=285.7780804688248, b=666.7283736812946, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The basic structure of a ANN can be modelled as shown in Figure 1. We would load the input, usually in the form of a multidimensional vector to the input layer of which will distribute it to the hidden layers. The hidden layers will then make decisions from the previous layer and weigh up how a stochastic change within itself detriments or improves the final output, and this is referred to as the process of learning. Having multiple hidden layers stacked upon each-other is commonly called deep learning.'), TextElement(label=<DocItemLabel.CAPTION: 'caption'>, id=2, page_no=1, cluster=Cluster(id=2, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=133.7849884033203, t=278.09375, r=480.58679078841254, b=336.2594918566335, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9529556035995483, cells=[Cell(id=2, text='Fig. 1: A simple three layered feedforward neural network (FNN), comprised', bbox=BoundingBox(l=134.76500022108476, t=278.50677153775587, r=480.58679078841254, b=288.4394515925984, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='of a input layer, a hidden layer and an output layer. This structure is the basis', bbox=BoundingBox(l=134.76500022108476, t=290.4617916037647, r=480.5866407884123, b=300.39447165860736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='of a number of common ANN architectures, included but not limited to Feed-', bbox=BoundingBox(l=134.76500022108476, t=302.41680166977346, r=480.58676078841245, b=312.349481724616, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='forward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and', bbox=BoundingBox(l=134.76500022108476, t=314.3718217357823, r=480.58676078841245, b=324.30450179062484, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='Recurrent Neural Networks (RNNs).', bbox=BoundingBox(l=134.76500022108476, t=326.32678180179073, r=296.7867704868848, b=336.2594918566335, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Fig. 1: A simple three layered feedforward neural network (FNN), comprised of a input layer, a hidden layer and an output layer. This structure is the basis of a number of common ANN architectures, included but not limited to Feedforward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and Recurrent Neural Networks (RNNs).'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=1, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.74380493164062, t=365.6249084472656, r=480.59271078842227, b=447.6732482910156, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9868213534355164, cells=[Cell(id=7, text='The two key learning paradigms in image processing tasks are supervised and', bbox=BoundingBox(l=134.76500022108476, t=365.8777720201687, r=480.58679078841254, b=375.8104820750116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='unsupervised learning.', bbox=BoundingBox(l=134.76500022108476, t=377.8327620861774, r=237.7583603900475, b=387.76547214102015, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='Supervised learning', bbox=BoundingBox(l=241.2870003958363, t=377.90249208656246, r=334.47714054871665, b=387.6060721401401, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='is learning through pre-labelled', bbox=BoundingBox(l=338.00800055450907, t=377.8327620861774, r=480.59271078842227, b=387.76547214102015, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='inputs, which act as targets. For each training example there will be a set of', bbox=BoundingBox(l=134.76500022108476, t=389.78775215218604, r=480.58676078841245, b=399.72045220702887, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='input values (vectors) and one or more associated designated output values.', bbox=BoundingBox(l=134.76500022108476, t=401.7427322181947, r=480.58679078841254, b=411.6754422730375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='The goal of this form of training is to reduce the models overall classification', bbox=BoundingBox(l=134.76500022108476, t=413.69772228420334, r=480.58679078841254, b=423.6304323390461, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='error, through correct calculation of the output value of training example by', bbox=BoundingBox(l=134.76500022108476, t=425.65271235021197, r=480.58676078841245, b=435.58541240505474, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='training.', bbox=BoundingBox(l=134.76500022108476, t=437.6087024162262, r=172.35388028275008, b=447.54141247106895, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The two key learning paradigms in image processing tasks are supervised and unsupervised learning. Supervised learning is learning through pre-labelled inputs, which act as targets. For each training example there will be a set of input values (vectors) and one or more associated designated output values. The goal of this form of training is to reduce the models overall classification error, through correct calculation of the output value of training example by training.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=4, page_no=1, cluster=Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.91444396972656, t=458.1381530761719, r=480.59070078841904, b=517.1390380859375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9838359951972961, cells=[Cell(id=16, text='Unsupervised learning', bbox=BoundingBox(l=134.76500022108476, t=458.9344425339748, r=239.35237039266252, b=468.63803258755246, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='differs in that the training set does not include any la-', bbox=BoundingBox(l=242.10600039717988, t=458.8647125335898, r=480.59070078841904, b=468.79742258843254, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='bels. Success is usually determined by whether the network is able to reduce or', bbox=BoundingBox(l=134.76500022108476, t=470.81970259959843, r=480.5867307884124, b=480.7524126544412, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='increase an associated cost function. However, it is important to note that most', bbox=BoundingBox(l=134.76500022108476, t=482.77569266561255, r=480.58679078841254, b=492.70840272045535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='image-focused pattern-recognition tasks usually depend on classification using', bbox=BoundingBox(l=134.76500022108476, t=494.7306827316212, r=480.5867307884124, b=504.663392786464, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='supervised learning.', bbox=BoundingBox(l=134.76500022108476, t=506.6856627976299, r=224.9065903689639, b=516.6183728524726, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Unsupervised learning differs in that the training set does not include any labels. Success is usually determined by whether the network is able to reduce or increase an associated cost function. However, it is important to note that most image-focused pattern-recognition tasks usually depend on classification using supervised learning.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=1, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.6765899658203, t=527.3146362304688, r=480.7476806640625, b=621.6336669921875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9860644340515137, cells=[Cell(id=22, text='Convolutional Neural Networks', bbox=BoundingBox(l=134.76500022108476, t=528.012392915384, r=283.65607046534365, b=537.7159729689615, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='(CNNs) are analogous to traditional ANNs', bbox=BoundingBox(l=286.989990470813, t=527.9426529149989, r=480.593200788423, b=537.8753629698416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='in that they are comprised of neurons that self-optimise through learning. Each', bbox=BoundingBox(l=134.76498022108473, t=539.8976429810075, r=480.5867307884124, b=549.8303530358503, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='neuron will still receive an input and perform a operation (such as a scalar', bbox=BoundingBox(l=134.76498022108473, t=551.8526330470162, r=480.58667078841233, b=561.7853331018589, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='product followed by a non-linear function) - the basis of countless ANNs. From', bbox=BoundingBox(l=134.76498022108473, t=563.8076131130247, r=480.5866407884123, b=573.7403231678675, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='the input raw image vectors to the final output of the class score, the entire of', bbox=BoundingBox(l=134.76498022108473, t=575.7626031790335, r=480.5866407884123, b=585.695313233876, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='the network will still express a single perceptive score function (the weight).', bbox=BoundingBox(l=134.76498022108473, t=587.7185932450475, r=480.58688078841266, b=597.6513032998903, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='The last layer will contain loss functions associated with the classes, and all of', bbox=BoundingBox(l=134.76498022108473, t=599.6735933110563, r=480.58682078841264, b=609.6063033658991, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='the regular tips and tricks developed for traditional ANNs still apply.', bbox=BoundingBox(l=134.76498022108473, t=611.628603377065, r=439.90955072168066, b=621.5613134319078, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Convolutional Neural Networks (CNNs) are analogous to traditional ANNs in that they are comprised of neurons that self-optimise through learning. Each neuron will still receive an input and perform a operation (such as a scalar product followed by a non-linear function) - the basis of countless ANNs. From the input raw image vectors to the final output of the class score, the entire of the network will still express a single perceptive score function (the weight). The last layer will contain loss functions associated with the classes, and all of the regular tips and tricks developed for traditional ANNs still apply.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=1, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.68446350097656, t=631.9942626953125, r=480.5868507884127, b=666.78271484375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9788795113563538, cells=[Cell(id=31, text='The only notable difference between CNNs and traditional ANNs is that CNNs', bbox=BoundingBox(l=134.76498022108473, t=632.885603494434, r=480.5868507884127, b=642.8183135492768, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='are primarily used in the field of pattern recognition within images. This allows', bbox=BoundingBox(l=134.76498022108473, t=644.8406035604428, r=480.58679078841254, b=654.7733136152856, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='us to encode image-specific features into the architecture, making the network', bbox=BoundingBox(l=134.76498022108473, t=656.7956036264516, r=480.58682078841264, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The only notable difference between CNNs and traditional ANNs is that CNNs are primarily used in the field of pattern recognition within images. This allows us to encode image-specific features into the architecture, making the network'), FigureElement(label=<DocItemLabel.PICTURE: 'picture'>, id=7, page_no=1, cluster=Cluster(id=7, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=193.8206024169922, t=115.9693374633789, r=420.3355407714844, b=267.38494873046875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9722045063972473, cells=[]), text='', annotations=[], provenance=None, predicted_class=None, confidence=None), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=2, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=134.2552032470703, t=118.86754608154297, r=480.58670078841243, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.974609911441803, cells=[Cell(id=2, text='more suited for image-focused tasks - whilst further reducing the parameters', bbox=BoundingBox(l=134.76501022108476, t=119.67181066075966, r=480.58670078841243, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='required to set up the model.', bbox=BoundingBox(l=134.76501022108476, t=131.6268307267684, r=262.07709042994287, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='more suited for image-focused tasks - whilst further reducing the parameters required to set up the model.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=2, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.69602966308594, t=150.32054138183594, r=481.443480789818, b=244.74359135133477, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9850017428398132, cells=[Cell(id=4, text='One of the largest limitations of traditional forms of ANN is that they tend to', bbox=BoundingBox(l=134.76501022108476, t=151.12481083442503, r=480.5867307884124, b=161.0574908892678, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='struggle with the computational complexity required to compute image data.', bbox=BoundingBox(l=134.76501022108476, t=163.079830900434, r=480.58679078841254, b=173.01251095527653, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='Common machine learning benchmarking datasets such as the MNIST database', bbox=BoundingBox(l=134.76501022108476, t=175.03485096644295, r=481.443480789818, b=184.9675210212854, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='of handwritten digits are suitable for most forms of ANN, due to its relatively', bbox=BoundingBox(l=134.76501022108476, t=186.98986103245147, r=480.58670078841243, b=196.92254108729412, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='small image dimensionality of just', bbox=BoundingBox(l=134.76501022108476, t=198.94586109846568, r=286.20648046952766, b=208.87854115330833, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='28', bbox=BoundingBox(l=288.66400047355927, t=199.15502109962063, r=298.6265904899031, b=208.00183114846766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='×', bbox=BoundingBox(l=300.6990104933029, t=198.5971610965405, r=308.44791050601515, b=208.00183114846766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='28', bbox=BoundingBox(l=310.5200205094145, t=199.15502109962063, r=320.4826005257583, b=208.00183114846766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='. With this dataset a single neuron in', bbox=BoundingBox(l=320.48203052575735, t=198.94586109846568, r=480.59103078841946, b=208.87854115330833, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='the first hidden layer will contain', bbox=BoundingBox(l=134.7650302210848, t=210.90087116447455, r=280.3784504599666, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='784', bbox=BoundingBox(l=282.62103046364564, t=211.11004116562947, r=297.56494048816137, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='weights (', bbox=BoundingBox(l=299.80502049183633, t=210.90087116447455, r=340.1535305580289, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='28', bbox=BoundingBox(l=340.15204055802644, t=211.11004116562947, r=350.11462057437024, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='×', bbox=BoundingBox(l=351.40204057648225, t=210.55218116254923, r=359.15094058919453, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='28', bbox=BoundingBox(l=360.438050591306, t=211.11004116562947, r=370.4006306076499, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='×', bbox=BoundingBox(l=371.68704060976023, t=210.55218116254923, r=379.4359406224725, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='1', bbox=BoundingBox(l=380.723050624584, t=211.11004116562947, r=385.7043506327559, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='where', bbox=BoundingBox(l=387.9440606364302, t=210.90087116447455, r=415.35120068139224, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='1', bbox=BoundingBox(l=417.5920706850684, t=211.11004116562947, r=422.5733606932403, b=219.9568412144764, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='bare in mind', bbox=BoundingBox(l=424.81409069691625, t=210.90087116447455, r=480.59470078842554, b=220.8335512193171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='that MNIST is normalised to just black and white values), which is manageable', bbox=BoundingBox(l=134.7650802210849, t=222.85589123048328, r=480.58676078841245, b=232.78857128532593, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='for most forms of ANN.', bbox=BoundingBox(l=134.7650802210849, t=234.81091129649212, r=240.3985103943787, b=244.74359135133477, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='One of the largest limitations of traditional forms of ANN is that they tend to struggle with the computational complexity required to compute image data. Common machine learning benchmarking datasets such as the MNIST database of handwritten digits are suitable for most forms of ANN, due to its relatively small image dimensionality of just 28 × 28 . With this dataset a single neuron in the first hidden layer will contain 784 weights ( 28 × 28 × 1 where 1 bare in mind that MNIST is normalised to just black and white values), which is manageable for most forms of ANN.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=4, page_no=2, cluster=Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.93231201171875, t=253.4578094482422, r=480.5915207884203, b=312.1922912597656, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9856253266334534, cells=[Cell(id=26, text='If you consider a more substantial coloured image input of', bbox=BoundingBox(l=134.7650802210849, t=254.30889140414888, r=391.39169064208613, b=264.2415714589914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='64', bbox=BoundingBox(l=393.7400806459387, t=254.5180614053039, r=403.70267066228257, b=263.36486145415074, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='×', bbox=BoundingBox(l=405.381070665036, t=253.96020140222367, r=413.12997067774825, b=263.36486145415074, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='64', bbox=BoundingBox(l=414.80908068050286, t=254.5180614053039, r=424.7716706968467, b=263.36486145415074, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text=', the number', bbox=BoundingBox(l=424.7710906968457, t=254.30889140414888, r=480.5915207884203, b=264.2415714589914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='of weights on just a single neuron of the first layer increases substantially to', bbox=BoundingBox(l=134.7650802210849, t=266.26391147015784, r=480.58682078841264, b=276.1965915250004, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='12', bbox=BoundingBox(l=134.7650802210849, t=278.42907153732676, r=144.72768023742873, b=287.2758715861738, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text=',', bbox=BoundingBox(l=144.72708023742774, t=278.42907153732676, r=147.49469024196807, b=287.2758715861738, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='288', bbox=BoundingBox(l=149.155080244692, t=278.42907153732676, r=164.09897026920777, b=287.2758715861738, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='. Also take into account that to deal with this scale of input, the network', bbox=BoundingBox(l=164.09908026920792, t=278.21991153617194, r=480.5909407884193, b=288.1525815910145, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='will also need to be a lot larger than one used to classify colour-normalised', bbox=BoundingBox(l=134.7650802210849, t=290.1749216021807, r=480.5868507884127, b=300.10760165702334, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='MNIST digits, then you will understand the drawbacks of using such models.', bbox=BoundingBox(l=134.7650802210849, t=302.12994166818953, r=476.6915007820223, b=312.0626217230322, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='If you consider a more substantial coloured image input of 64 × 64 , the number of weights on just a single neuron of the first layer increases substantially to 12 , 288 . Also take into account that to deal with this scale of input, the network will also need to be a lot larger than one used to classify colour-normalised MNIST digits, then you will understand the drawbacks of using such models.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=5, page_no=2, cluster=Cluster(id=5, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.107177734375, t=342.1416320800781, r=207.76548767089844, b=352.77398681640625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9495469331741333, cells=[Cell(id=38, text='1.1', bbox=BoundingBox(l=134.7650802210849, t=342.7606518925292, r=147.2183202415147, b=352.4642319461069, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='Overfitting', bbox=BoundingBox(l=157.1809202578585, t=342.7606518925292, r=207.5418703404767, b=352.4642319461069, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='1.1 Overfitting'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=2, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87643432617188, t=372.7414855957031, r=480.5868507884127, b=431.04357237997726, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9863153100013733, cells=[Cell(id=40, text='But why does it matter? Surely we could just increase the number of hidden lay-', bbox=BoundingBox(l=134.7650802210849, t=373.28991206109447, r=480.58679078841254, b=383.22262211593716, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='ers in our network, and perhaps increase the number of neurons within them?', bbox=BoundingBox(l=134.7650802210849, t=385.2449021271031, r=480.58682078841264, b=395.1776121819458, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='The simple answer to this question is no. This is down to two reasons, one be-', bbox=BoundingBox(l=134.7650802210849, t=397.1998921931118, r=480.58682078841264, b=407.13259224795445, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='ing the simple problem of not having unlimited computational power and time', bbox=BoundingBox(l=134.7650802210849, t=409.15487225912034, r=480.5868507884127, b=419.0875823139631, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='to train these huge ANNs.', bbox=BoundingBox(l=134.7650802210849, t=421.11087232513455, r=250.23160041051008, b=431.04357237997726, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='But why does it matter? Surely we could just increase the number of hidden layers in our network, and perhaps increase the number of neurons within them? The simple answer to this question is no. This is down to two reasons, one being the simple problem of not having unlimited computational power and time to train these huge ANNs.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=2, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.83572387695312, t=440.1903076171875, r=480.64300537109375, b=522.4312133789062, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9872767329216003, cells=[Cell(id=45, text='The second reason is stopping or reducing the effects of overfitting.', bbox=BoundingBox(l=134.7650802210849, t=440.6088524327913, r=427.9843107021171, b=450.54156248763405, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='Overfitting', bbox=BoundingBox(l=430.231080705803, t=440.6785824331763, r=480.5920107884211, b=450.382172486754, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='is basically when a network is unable to learn effectively due to a number of', bbox=BoundingBox(l=134.7650802210849, t=452.5638424988, r=480.5868507884127, b=462.49655255364274, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='reasons. It is an important concept of most, if not all machine learning algo-', bbox=BoundingBox(l=134.7650802210849, t=464.51882256480854, r=480.58682078841264, b=474.4515326196513, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='rithms and it is important that every precaution is taken as to reduce its effects.', bbox=BoundingBox(l=134.7650802210849, t=476.4738126308172, r=480.58691078841275, b=486.4065226856599, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='If our models were to exhibit signs of overfitting then we may see a reduced', bbox=BoundingBox(l=134.7650802210849, t=488.4288026968258, r=480.58691078841275, b=498.3615127516686, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='ability to pinpoint generalised features for not only our training dataset, but', bbox=BoundingBox(l=134.7650802210849, t=500.38479276284, r=480.58691078841275, b=510.3175028176828, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='also our test and prediction sets.', bbox=BoundingBox(l=134.7650802210849, t=512.3397828288487, r=276.7719704540501, b=522.2724928836915, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The second reason is stopping or reducing the effects of overfitting. Overfitting is basically when a network is unable to learn effectively due to a number of reasons. It is an important concept of most, if not all machine learning algorithms and it is important that every precaution is taken as to reduce its effects. If our models were to exhibit signs of overfitting then we may see a reduced ability to pinpoint generalised features for not only our training dataset, but also our test and prediction sets.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=2, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8554229736328, t=531.4302368164062, r=480.58688078841266, b=565.6804531233654, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9846621751785278, cells=[Cell(id=53, text='This is the main reason behind reducing the complexity of our ANNs. The less', bbox=BoundingBox(l=134.7650802210849, t=531.8377629365054, r=480.58688078841266, b=541.7704729913482, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='parameters required to train, the less likely the network will overfit - and of', bbox=BoundingBox(l=134.7650802210849, t=543.792753002514, r=480.58679078841254, b=553.7254630573568, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='course, improve the predictive performance of the model.', bbox=BoundingBox(l=134.7650802210849, t=555.7477430685227, r=388.2833906369869, b=565.6804531233654, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='This is the main reason behind reducing the complexity of our ANNs. The less parameters required to train, the less likely the network will overfit - and of course, improve the predictive performance of the model.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=9, page_no=2, cluster=Cluster(id=9, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.10931396484375, t=596.1439819335938, r=248.64112854003906, b=608.6050833603708, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9415203332901001, cells=[Cell(id=56, text='2', bbox=BoundingBox(l=134.7650802210849, t=596.9607232960773, r=140.74268023089127, b=608.6050833603708, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='CNN architecture', bbox=BoundingBox(l=152.697880250504, t=596.9607232960773, r=248.63835040789635, b=608.6050833603708, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2 CNN architecture'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=10, page_no=2, cluster=Cluster(id=10, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.93942260742188, t=631.9437255859375, r=480.8302001953125, b=666.7557373046875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9827980399131775, cells=[Cell(id=58, text='As noted earlier, CNNs primarily focus on the basis that the input will be com-', bbox=BoundingBox(l=134.7650802210849, t=632.8857434944348, r=480.58682078841264, b=642.8184535492776, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='prised of images. This focuses the architecture to be set up in way to best suit', bbox=BoundingBox(l=134.7650802210849, t=644.8407435604436, r=480.58679078841254, b=654.7734536152864, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='the need for dealing with the specific type of data.', bbox=BoundingBox(l=134.7650802210849, t=656.7957436264522, r=355.0182505824148, b=666.728453681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='As noted earlier, CNNs primarily focus on the basis that the input will be comprised of images. This focuses the architecture to be set up in way to best suit the need for dealing with the specific type of data.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=1, page_no=3, cluster=Cluster(id=1, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=-1.0, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea et al.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=3, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.76507568359375, t=118.65596771240234, r=480.6532897949219, b=189.66046142578125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.986056923866272, cells=[Cell(id=2, text='One of the key differences is that the neurons that the layers within the CNN', bbox=BoundingBox(l=134.76500022108476, t=119.67181066075966, r=480.58676078841245, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='are comprised of neurons organised into three dimensions, the spatial dimen-', bbox=BoundingBox(l=134.76500022108476, t=131.6268307267684, r=480.58676078841245, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='sionality of the input (', bbox=BoundingBox(l=134.76500022108476, t=143.58184079277714, r=231.18303037926052, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='height', bbox=BoundingBox(l=231.18500037926376, t=143.65155079316196, r=260.51489042738007, b=153.35510084673967, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='and the', bbox=BoundingBox(l=262.6799904309319, t=143.58184079277714, r=295.5266704848176, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='width', bbox=BoundingBox(l=297.6950104883748, t=143.65155079316196, r=324.80325053284645, b=153.35510084673967, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text=') and the', bbox=BoundingBox(l=324.803010532846, t=143.58184079277714, r=363.13907059573717, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='depth', bbox=BoundingBox(l=365.30103059928393, t=143.65155079316196, r=391.8613306428566, b=153.35510084673967, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='. The depth does not', bbox=BoundingBox(l=391.86203064285775, t=143.58184079277714, r=480.5889907884162, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='refer to the total number of layers within the ANN, but the third dimension of a', bbox=BoundingBox(l=134.76505022108483, t=155.53686085878599, r=480.58688078841266, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='activation volume. Unlike standard ANNS, the neurons within any given layer', bbox=BoundingBox(l=134.76505022108483, t=167.49188092479483, r=480.58679078841254, b=177.42456097963736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='will only connect to a small region of the layer preceding it.', bbox=BoundingBox(l=134.76505022108483, t=179.44787099080895, r=395.79517064931014, b=189.3805510456516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='One of the key differences is that the neurons that the layers within the CNN are comprised of neurons organised into three dimensions, the spatial dimensionality of the input ( height and the width ) and the depth . The depth does not refer to the total number of layers within the ANN, but the third dimension of a activation volume. Unlike standard ANNS, the neurons within any given layer will only connect to a small region of the layer preceding it.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=3, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.84022521972656, t=196.7172088623047, r=480.62689208984375, b=267.8779602050781, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9875405430793762, cells=[Cell(id=14, text='In practice this would mean that for the example given earlier, the input ’vol-', bbox=BoundingBox(l=134.76505022108483, t=197.79687109212182, r=480.58679078841254, b=207.72955114696435, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='ume’ will have a dimensionality of', bbox=BoundingBox(l=134.76505022108483, t=209.75189115813055, r=287.34225047139086, b=219.6845712129732, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='64', bbox=BoundingBox(l=289.74603047533435, t=209.96105115928538, r=299.7086204916782, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='×', bbox=BoundingBox(l=301.60602049479087, t=209.40319115620525, r=309.35492050750315, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='64', bbox=BoundingBox(l=311.2510105106137, t=209.96105115928538, r=321.21359052695755, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='×', bbox=BoundingBox(l=323.1109905300703, t=209.40319115620525, r=330.85989054278247, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='3', bbox=BoundingBox(l=332.75699054589467, t=209.96105115928538, r=337.7382805540666, b=218.8078612081324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='(height, width and depth), lead-', bbox=BoundingBox(l=340.14297055801154, t=209.75189115813055, r=480.5957307884272, b=219.6845712129732, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='ing to a final output layer comprised of a dimensionality of', bbox=BoundingBox(l=134.7649702210847, t=221.7069012241392, r=401.304350658348, b=231.63958127898184, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='1', bbox=BoundingBox(l=404.45795066352156, t=221.91607122529422, r=409.4392406716935, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='×', bbox=BoundingBox(l=412.142940676129, t=221.3582112222141, r=419.8918506888412, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='1', bbox=BoundingBox(l=422.59595069327736, t=221.91607122529422, r=427.5772407014493, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='×', bbox=BoundingBox(l=430.2819507058864, t=221.3582112222141, r=438.03085071859863, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='n', bbox=BoundingBox(l=440.7339507230331, t=221.91607122529422, r=446.7135007328427, b=230.76287127414116, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='(where', bbox=BoundingBox(l=449.86694073801596, t=221.7069012241392, r=480.59161078842044, b=231.63958127898184, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='n', bbox=BoundingBox(l=134.7649502210847, t=233.87207129130843, r=140.74451023089426, b=242.71887134015537, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='represents the possible number of classes) as we would have condensed the', bbox=BoundingBox(l=143.63596023563775, t=233.6629012901535, r=480.59103078841946, b=243.59558134499605, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='full input dimensionality into a smaller volume of class scores filed across the', bbox=BoundingBox(l=134.7649502210847, t=245.61792135616224, r=480.58670078841243, b=255.55059141100492, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='depth dimension.', bbox=BoundingBox(l=134.7649502210847, t=257.5729314221711, r=212.35367034837057, b=267.50561147701364, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='In practice this would mean that for the example given earlier, the input ’volume’ will have a dimensionality of 64 × 64 × 3 (height, width and depth), leading to a final output layer comprised of a dimensionality of 1 × 1 × n (where n represents the possible number of classes) as we would have condensed the full input dimensionality into a smaller volume of class scores filed across the depth dimension.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=4, page_no=3, cluster=Cluster(id=4, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.0306854248047, t=294.2342224121094, r=247.65118040627686, b=304.46118168106136, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9402409791946411, cells=[Cell(id=33, text='2.1', bbox=BoundingBox(l=134.7649502210847, t=294.7576216274838, r=147.2182002415145, b=304.46118168106136, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='Overall architecture', bbox=BoundingBox(l=157.18080025785835, t=294.7576216274838, r=247.65118040627686, b=304.46118168106136, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2.1 Overall architecture'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=3, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.74267578125, t=321.2259826660156, r=480.75616455078125, b=367.63958202989653, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9827728271484375, cells=[Cell(id=35, text='CNNs are comprised of three types of layers. These are convolutional layers,', bbox=BoundingBox(l=134.7649502210847, t=321.8409417770224, r=480.5866407884123, b=331.77362183186506, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='pooling layers and', bbox=BoundingBox(l=134.7649502210847, t=333.7969018430365, r=219.5566303601872, b=343.7296118978793, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='fully-connected layers', bbox=BoundingBox(l=223.34396036640038, t=333.86663184342154, r=325.6299705342027, b=343.57022189699916, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='. When these layers are stacked, a', bbox=BoundingBox(l=325.6299705342027, t=333.7969018430365, r=480.5883207884151, b=343.7296118978793, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='CNN architecture has been formed. A simplified CNN architecture for MNIST', bbox=BoundingBox(l=134.7649702210847, t=345.7518919090451, r=480.58682078841264, b=355.684601963888, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='classification is illustrated in Figure 2.', bbox=BoundingBox(l=134.7649702210847, t=357.70687197505373, r=300.69211049329164, b=367.63958202989653, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='CNNs are comprised of three types of layers. These are convolutional layers, pooling layers and fully-connected layers . When these layers are stacked, a CNN architecture has been formed. A simplified CNN architecture for MNIST classification is illustrated in Figure 2.'), FigureElement(label=<DocItemLabel.PICTURE: 'picture'>, id=6, page_no=3, cluster=Cluster(id=6, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=194.3790283203125, t=389.60748215119077, r=421.985600692276, b=512.5117797851562, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9499910473823547, cells=[Cell(id=41, text='input', bbox=BoundingBox(l=208.48486034202372, t=477.9427126389276, r=223.0549303659262, b=485.8841526827757, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='0', bbox=BoundingBox(l=409.24475067137445, t=438.2520724197785, r=412.83917067727117, b=446.21008246371804, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='9', bbox=BoundingBox(l=409.24475067137445, t=470.3982525972714, r=412.83167067725884, b=478.3396626411194, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='convolution', bbox=BoundingBox(l=246.66763040466336, t=389.60748215119077, r=280.7496304605756, b=397.56549219513033, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text=' w/ReLu', bbox=BoundingBox(l=251.36052041236212, t=398.2413321988619, r=276.50482045361184, b=406.18280224271024, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='pooling', bbox=BoundingBox(l=289.15469047436426, t=397.5405221949925, r=311.1308005104165, b=405.49853223893206, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='output ', bbox=BoundingBox(l=401.81390065918396, t=483.1483726676703, r=421.985600692276, b=491.1063227116096, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='fully-connected', bbox=BoundingBox(l=313.20502051381925, t=495.8235127376552, r=358.2925405877863, b=503.7649527815033, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='w/ ReLu', bbox=BoundingBox(l=323.2782605303446, t=504.44409278525313, r=348.39026057154143, b=512.385492829101, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='fully-connected', bbox=BoundingBox(l=340.9916705594039, t=398.3609021995222, r=386.19403063355924, b=406.3023622433704, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='...', bbox=BoundingBox(l=408.3062106698348, t=454.31549250847155, r=413.68658067866136, b=462.2734925524111, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='', annotations=[], provenance=None, predicted_class=None, confidence=None), TextElement(label=<DocItemLabel.CAPTION: 'caption'>, id=7, page_no=3, cluster=Cluster(id=7, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=165.54881286621094, t=524.3402099609375, r=449.30869073710016, b=534.6312866210938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9542887210845947, cells=[Cell(id=52, text='Fig. 2: An simple CNN architecture, comprised of just five layers', bbox=BoundingBox(l=166.0520002724117, t=524.4947528959615, r=449.30869073710016, b=534.4274629508043, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Fig. 2: An simple CNN architecture, comprised of just five layers'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=3, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.9955596923828, t=559.4691772460938, r=480.5867307884124, b=582.2084332146236, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.969181478023529, cells=[Cell(id=53, text='The basic functionality of the example CNN above can be broken down into', bbox=BoundingBox(l=134.76500022108476, t=560.3207430937722, r=480.5867307884124, b=570.2534431486149, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='four key areas.', bbox=BoundingBox(l=134.76500022108476, t=572.2757231597809, r=199.50195032728706, b=582.2084332146236, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The basic functionality of the example CNN above can be broken down into four key areas.'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=9, page_no=3, cluster=Cluster(id=9, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.5282745361328, t=590.1478271484375, r=480.5958907884275, b=612.7933349609375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9722254276275635, cells=[Cell(id=55, text='1.', bbox=BoundingBox(l=139.2480002284392, t=590.6257332610992, r=146.818050240858, b=600.5584433159419, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='As found in other forms of ANN, the', bbox=BoundingBox(l=149.34140024499763, t=590.6257332610992, r=314.51996051597655, b=600.5584433159419, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='input layer', bbox=BoundingBox(l=316.9819905200155, t=590.6954632614842, r=367.034090602127, b=600.3990433150618, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='will hold the pixel values', bbox=BoundingBox(l=369.49298060616087, t=590.6257332610992, r=480.5958907884275, b=600.5584433159419, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='of the image.', bbox=BoundingBox(l=151.7009702488686, t=602.5807333271079, r=208.7368503424371, b=612.5134433819507, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='1. As found in other forms of ANN, the input layer will hold the pixel values of the image.'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=10, page_no=3, cluster=Cluster(id=10, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.4064483642578, t=619.9978637695312, r=480.59625078842805, b=666.728433681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9786887168884277, cells=[Cell(id=60, text='2.', bbox=BoundingBox(l=139.24797022843916, t=620.9297334284206, r=147.41849024184307, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='The', bbox=BoundingBox(l=150.14200024631103, t=620.9297334284206, r=168.37860027622855, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='convolutional layer', bbox=BoundingBox(l=170.7909702801861, t=620.9994634288056, r=258.9898704248782, b=630.7030434823832, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='will determine the output of neurons of which are', bbox=BoundingBox(l=261.4029504288369, t=620.9297334284206, r=480.59009078841797, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='connected to local regions of the input through the calculation of the scalar', bbox=BoundingBox(l=151.70096024886857, t=632.8847334944293, r=480.5961907884279, b=642.817443549272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=65, text='product between their weights and the region connected to the input vol-', bbox=BoundingBox(l=151.70096024886857, t=644.8407235604434, r=480.59625078842805, b=654.7734336152862, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='ume. The', bbox=BoundingBox(l=151.70096024886857, t=656.7957336264523, r=192.50775031581293, b=666.728433681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='rectified linear unit', bbox=BoundingBox(l=194.56696031919108, t=656.8654636268373, r=281.1319904612028, b=666.5690436804149, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=68, text='(commonly shortened to ReLu) aims to apply', bbox=BoundingBox(l=283.19095046458057, t=656.7957336264523, r=480.58984078841746, b=666.728433681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2. The convolutional layer will determine the output of neurons of which are connected to local regions of the input through the calculation of the scalar product between their weights and the region connected to the input volume. The rectified linear unit (commonly shortened to ReLu) aims to apply'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=2, page_no=4, cluster=Cluster(id=2, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=150.73934936523438, t=118.60086059570312, r=480.5961907884279, b=141.57867431640625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7262013554573059, cells=[Cell(id=2, text='an ’elementwise’ activation function such as sigmoid to the output of the', bbox=BoundingBox(l=151.70102024886864, t=119.67181066075966, r=480.5961907884279, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='activation produced by the previous layer.', bbox=BoundingBox(l=151.70102024886864, t=131.6268307267684, r=337.7227205540411, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='an ’elementwise’ activation function such as sigmoid to the output of the activation produced by the previous layer.'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=3, page_no=4, cluster=Cluster(id=3, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.22279357910156, t=150.14556884765625, r=480.59641078842833, b=184.86651102072767, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9725128412246704, cells=[Cell(id=4, text='3.', bbox=BoundingBox(l=139.24802022843926, t=151.02282083386194, r=147.41855024184318, b=160.9555008887047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='The', bbox=BoundingBox(l=150.14204024631113, t=151.02282083386194, r=168.37866027622866, b=160.9555008887047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='pooling layer', bbox=BoundingBox(l=171.0980202806898, t=151.0925208342469, r=231.9396103805017, b=160.79608088782436, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='will then simply perform downsampling along the spa-', bbox=BoundingBox(l=234.6580203849613, t=151.02282083386194, r=480.59476078842556, b=160.9555008887047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='tial dimensionality of the given input, further reducing the number of pa-', bbox=BoundingBox(l=151.70102024886864, t=162.9778408998709, r=480.59641078842833, b=172.91052095471343, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='rameters within that activation.', bbox=BoundingBox(l=151.70102024886864, t=174.93383096588502, r=290.1014404759174, b=184.86651102072767, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='3. The pooling layer will then simply perform downsampling along the spatial dimensionality of the given input, further reducing the number of parameters within that activation.'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=4, page_no=4, cluster=Cluster(id=4, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.19285583496094, t=193.37098693847656, r=480.59628078842803, b=240.20932006835938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.978056788444519, cells=[Cell(id=10, text='4.', bbox=BoundingBox(l=139.24802022843926, t=194.32983107297866, r=147.41855024184318, b=204.2625111278213, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='The', bbox=BoundingBox(l=150.14204024631113, t=194.32983107297866, r=168.37866027622866, b=204.2625111278213, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='fully-connected layers', bbox=BoundingBox(l=172.55103028307352, t=194.3995310733635, r=275.22559045151326, b=204.103081126941, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='will then perform the same duties found in', bbox=BoundingBox(l=279.3970304583566, t=194.32983107297866, r=480.5917107884205, b=204.2625111278213, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='standard ANNs and attempt to produce class scores from the activations,', bbox=BoundingBox(l=151.70103024886865, t=206.28485113898762, r=480.59628078842803, b=216.21752119383007, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='to be used for classification. It is also suggested that ReLu may be used', bbox=BoundingBox(l=151.70103024886865, t=218.23986120499615, r=480.5863007884118, b=228.1725412598389, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='between these layers, as to improve performance.', bbox=BoundingBox(l=151.70103024886865, t=230.1948812710051, r=369.3041406058511, b=240.12756132584775, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='4. The fully-connected layers will then perform the same duties found in standard ANNs and attempt to produce class scores from the activations, to be used for classification. It is also suggested that ReLu may be used between these layers, as to improve performance.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=4, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.94287109375, t=248.67041015625, r=480.5868507884127, b=283.60284423828125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9642502665519714, cells=[Cell(id=17, text='Through this simple method of transformation, CNNs are able to transform', bbox=BoundingBox(l=134.7650302210848, t=249.59185137810402, r=480.5868507884127, b=259.5245314329467, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='the original input layer by layer using convolutional and downsampling tech-', bbox=BoundingBox(l=134.7650302210848, t=261.546871444113, r=480.58679078841254, b=271.47955149895563, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='niques to produce class scores for classification and regression purposes.', bbox=BoundingBox(l=134.7650302210848, t=273.5018915101218, r=453.4885907439574, b=283.43457156496436, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Through this simple method of transformation, CNNs are able to transform the original input layer by layer using convolutional and downsampling techniques to produce class scores for classification and regression purposes.'), TextElement(label=<DocItemLabel.CAPTION: 'caption'>, id=6, page_no=4, cluster=Cluster(id=6, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=133.96603393554688, t=452.9920349121094, r=480.58679078841254, b=499.6073303222656, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7702291011810303, cells=[Cell(id=20, text='Fig. 3: Activations taken from the first convolutional layer of a simplistic deep', bbox=BoundingBox(l=134.76500022108476, t=453.7317525052485, r=480.5866407884123, b=463.6644525600912, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='CNN, after training on the MNIST database of handwritten digits. If you look', bbox=BoundingBox(l=134.76500022108476, t=465.68673257125704, r=480.58679078841254, b=475.6194426260998, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='carefully, you can see that the network has successfully picked up on character-', bbox=BoundingBox(l=134.76500022108476, t=477.6417226372657, r=480.5867307884124, b=487.5744326921085, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='istics unique to specific numeric digits.', bbox=BoundingBox(l=134.76500022108476, t=489.59771270327985, r=305.6634505014472, b=499.53042275812265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Fig. 3: Activations taken from the first convolutional layer of a simplistic deep CNN, after training on the MNIST database of handwritten digits. If you look carefully, you can see that the network has successfully picked up on characteristics unique to specific numeric digits.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=4, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87347412109375, t=525.5286865234375, r=480.58679078841254, b=584.223383225749, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9733061194419861, cells=[Cell(id=24, text='However, it is important to note that simply understanding the overall archi-', bbox=BoundingBox(l=134.76500022108476, t=526.4707329068717, r=480.5867307884124, b=536.4034429617145, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='tecture of a CNN architecture will not suffice. The creation and optimisation', bbox=BoundingBox(l=134.76500022108476, t=538.4257229728803, r=480.58679078841254, b=548.3584230277231, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='of these models can take quite some time, and can be quite confusing. We will', bbox=BoundingBox(l=134.76500022108476, t=550.3807030388889, r=480.5867307884124, b=560.3134130937317, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='now explore in detail the individual layers, detailing their hyperparameters', bbox=BoundingBox(l=134.76500022108476, t=562.3356931048977, r=480.58679078841254, b=572.2684031597404, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='and connectivities.', bbox=BoundingBox(l=134.76500022108476, t=574.2906831709063, r=217.085970356134, b=584.223383225749, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='However, it is important to note that simply understanding the overall architecture of a CNN architecture will not suffice. The creation and optimisation of these models can take quite some time, and can be quite confusing. We will now explore in detail the individual layers, detailing their hyperparameters and connectivities.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=8, page_no=4, cluster=Cluster(id=8, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.96798706054688, t=613.8170776367188, r=248.6064910888672, b=624.6824951171875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9571674466133118, cells=[Cell(id=29, text='2.2', bbox=BoundingBox(l=134.76500022108476, t=614.616403393562, r=147.21825024151457, b=624.3199934471397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='Convolutional layer', bbox=BoundingBox(l=157.18085025785842, t=614.616403393562, r=248.22903040722483, b=624.3199934471397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2.2 Convolutional layer'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=9, page_no=4, cluster=Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87005615234375, t=643.7536010742188, r=480.58688078841266, b=666.9384155273438, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9720144271850586, cells=[Cell(id=31, text='As the name implies, the convolutional layer plays a vital role in how CNNs', bbox=BoundingBox(l=134.76500022108476, t=644.8406835604433, r=480.58688078841266, b=654.7733936152861, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='operate. The layers parameters focus around the use of learnable', bbox=BoundingBox(l=134.76500022108476, t=656.7956836264519, r=419.07770068750557, b=666.7283936812947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='kernels', bbox=BoundingBox(l=421.5659806915877, t=656.8654136268369, r=455.3192707469606, b=666.5690036804147, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='.', bbox=BoundingBox(l=455.3189707469602, t=656.7956836264519, r=457.80963075104614, b=666.7283936812947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='As the name implies, the convolutional layer plays a vital role in how CNNs operate. The layers parameters focus around the use of learnable kernels .'), FigureElement(label=<DocItemLabel.PICTURE: 'picture'>, id=10, page_no=4, cluster=Cluster(id=10, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=135.41848754882812, t=307.9389953613281, r=479.3868713378906, b=442.5080871582031, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9779683947563171, cells=[]), text='', annotations=[], provenance=None, predicted_class=None, confidence=None), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=5, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.7237091064453, t=118.72857666015625, r=480.7948913574219, b=177.42456097963736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9832537770271301, cells=[Cell(id=2, text='These kernels are usually small in spatial dimensionality, but spreads along the', bbox=BoundingBox(l=134.76500022108476, t=119.67181066075966, r=480.58676078841245, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='entirety of the depth of the input. When the data hits a convolutional layer,', bbox=BoundingBox(l=134.76500022108476, t=131.6268307267684, r=480.58679078841254, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='the layer convolves each filter across the spatial dimensionality of the input to', bbox=BoundingBox(l=134.76500022108476, t=143.58184079277714, r=480.58676078841245, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='produce a 2D activation map. These activation maps can be visualised, as seen', bbox=BoundingBox(l=134.76500022108476, t=155.53686085878599, r=480.58676078841245, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='in Figure 3.', bbox=BoundingBox(l=134.76500022108476, t=167.49188092479483, r=184.42854030255882, b=177.42456097963736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='These kernels are usually small in spatial dimensionality, but spreads along the entirety of the depth of the input. When the data hits a convolutional layer, the layer convolves each filter across the spatial dimensionality of the input to produce a 2D activation map. These activation maps can be visualised, as seen in Figure 3.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=5, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.92149353027344, t=186.75308227539062, r=480.5972595214844, b=233.50762128929614, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9789468050003052, cells=[Cell(id=7, text='As we glide through the input, the scalar product is calculated for each value in', bbox=BoundingBox(l=134.76500022108476, t=187.70990103642725, r=480.58679078841254, b=197.6425710912697, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='that kernel. (Figure 4) From this the network will learn kernels that ’fire’ when', bbox=BoundingBox(l=134.76500022108476, t=199.6649111024359, r=480.5867307884124, b=209.59759115727866, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='they see a specific feature at a given spatial position of the input. These are', bbox=BoundingBox(l=134.76500022108476, t=211.61993116844474, r=480.58670078841243, b=221.55261122328739, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='commonly known as', bbox=BoundingBox(l=134.76500022108476, t=223.57495123445358, r=226.9688603723471, b=233.50762128929614, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='activations', bbox=BoundingBox(l=229.45900037643224, t=223.64465123483842, r=278.7041304572199, b=233.3482012884159, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='.', bbox=BoundingBox(l=278.7040104572196, t=223.57495123445358, r=281.19467046130563, b=233.50762128929614, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='As we glide through the input, the scalar product is calculated for each value in that kernel. (Figure 4) From this the network will learn kernels that ’fire’ when they see a specific feature at a given spatial position of the input. These are commonly known as activations .'), FigureElement(label=<DocItemLabel.PICTURE: 'picture'>, id=4, page_no=5, cluster=Cluster(id=4, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=168.14492797851562, t=257.09973141955834, r=446.12943073188455, b=344.0976257324219, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.922330915927887, cells=[Cell(id=13, text='0', bbox=BoundingBox(l=276.3548604533658, t=293.0746416181913, r=279.2306204580836, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='0', bbox=BoundingBox(l=287.97589047243036, t=293.0746416181913, r=290.85165047714816, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='0', bbox=BoundingBox(l=276.3548604533658, t=304.6956716823561, r=279.2306204580836, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='1', bbox=BoundingBox(l=287.97589047243036, t=304.6956716823561, r=290.85165047714816, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='0', bbox=BoundingBox(l=299.70312049166915, t=293.0746416181913, r=302.5788604963869, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='2', bbox=BoundingBox(l=299.70312049166915, t=304.6956716823561, r=302.5788604963869, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='0', bbox=BoundingBox(l=276.3548604533658, t=316.51141174759584, r=279.2306204580836, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='1', bbox=BoundingBox(l=287.97589047243036, t=316.51141174759584, r=290.85165047714816, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='1', bbox=BoundingBox(l=299.70312049166915, t=316.51141174759584, r=302.5788604963869, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='4', bbox=BoundingBox(l=342.4826705618499, t=293.0746416181913, r=345.3584005665676, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='0', bbox=BoundingBox(l=354.1214005809435, t=293.0746416181913, r=356.9971305856612, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='0', bbox=BoundingBox(l=342.4826705618499, t=304.6956716823561, r=345.3584005665676, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='0', bbox=BoundingBox(l=354.1214005809435, t=304.6956716823561, r=356.9971305856612, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='0', bbox=BoundingBox(l=365.8308706001531, t=293.0746416181913, r=368.70667060487085, b=299.4415816533459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='0', bbox=BoundingBox(l=365.8308706001531, t=304.6956716823561, r=368.70667060487085, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='0', bbox=BoundingBox(l=342.4826705618499, t=316.51141174759584, r=345.3584005665676, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='0', bbox=BoundingBox(l=354.1214005809435, t=316.51141174759584, r=356.9971305856612, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='-4', bbox=BoundingBox(l=364.8516805985467, t=316.51141174759584, r=369.69177060648695, b=322.87835178275054, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='-8', bbox=BoundingBox(l=419.2581506878016, t=304.6956716823561, r=424.1100806957613, b=311.06262171751075, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='Pooled Vector', bbox=BoundingBox(l=269.128600441511, t=275.49743152114, r=309.9874305085408, b=283.3499715644973, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='Kernel', bbox=BoundingBox(l=345.9925505676079, t=276.30102152557697, r=365.1973305991138, b=284.1372015688439, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='Destination Pixel', bbox=BoundingBox(l=397.25488065170487, t=277.072381529836, r=446.12943073188455, b=284.9249815731936, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='0', bbox=BoundingBox(l=174.49387028626077, t=275.435301520797, r=177.37701029099065, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='0', bbox=BoundingBox(l=186.11195030532048, t=275.435301520797, r=188.99509031005033, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='0', bbox=BoundingBox(l=174.49387028626077, t=287.05633158496175, r=177.37701029099065, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='1', bbox=BoundingBox(l=186.11195030532048, t=287.05633158496175, r=188.99509031005033, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='0', bbox=BoundingBox(l=197.83917032455923, t=275.435301520797, r=200.7223103292891, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='2', bbox=BoundingBox(l=197.83917032455923, t=287.05633158496175, r=200.7223103292891, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='0', bbox=BoundingBox(l=174.49387028626077, t=298.8851916502739, r=177.36963029097853, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='1', bbox=BoundingBox(l=186.11195030532048, t=298.8851916502739, r=188.98772031003824, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='1', bbox=BoundingBox(l=197.83917032455923, t=298.8851916502739, r=200.714940329277, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='0', bbox=BoundingBox(l=209.65488034394315, t=275.435301520797, r=212.53799034867296, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='0', bbox=BoundingBox(l=221.269990362998, t=275.435301520797, r=224.15314036772787, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='1', bbox=BoundingBox(l=209.65488034394315, t=287.05633158496175, r=212.53799034867296, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='1', bbox=BoundingBox(l=221.269990362998, t=287.05633158496175, r=224.15314036772787, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='0', bbox=BoundingBox(l=232.99721038223674, t=275.435301520797, r=235.88034038696654, b=281.8185415560416, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='2', bbox=BoundingBox(l=232.99721038223674, t=287.05633158496175, r=235.88034038696654, b=293.43957162020627, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='1', bbox=BoundingBox(l=209.65488034394315, t=298.8851916502739, r=212.5306403486609, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='1', bbox=BoundingBox(l=221.269990362998, t=298.8851916502739, r=224.14575036771572, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='1', bbox=BoundingBox(l=232.99721038223674, t=298.8851916502739, r=235.87297038695448, b=305.2521316854286, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='1', bbox=BoundingBox(l=174.49387028626077, t=310.6949417154807, r=177.36963029097853, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='0', bbox=BoundingBox(l=186.11195030532048, t=310.6949417154807, r=188.98772031003824, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='0', bbox=BoundingBox(l=174.49387028626077, t=322.31604177964573, r=177.36963029097853, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='0', bbox=BoundingBox(l=186.11195030532048, t=322.31604177964573, r=188.98772031003824, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='0', bbox=BoundingBox(l=197.83917032455923, t=310.6949417154807, r=200.714940329277, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='1', bbox=BoundingBox(l=197.83917032455923, t=322.31604177964573, r=200.714940329277, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='0', bbox=BoundingBox(l=174.49387028626077, t=334.1317118448851, r=177.36963029097853, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='1', bbox=BoundingBox(l=186.11195030532048, t=334.1317118448851, r=188.98772031003824, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='1', bbox=BoundingBox(l=197.83917032455923, t=334.1317118448851, r=200.714940329277, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='0', bbox=BoundingBox(l=209.65488034394315, t=310.6949417154807, r=212.5306403486609, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='0', bbox=BoundingBox(l=221.269990362998, t=310.6949417154807, r=224.14575036771572, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='1', bbox=BoundingBox(l=209.65488034394315, t=322.31604177964573, r=212.5306403486609, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=65, text='1', bbox=BoundingBox(l=221.269990362998, t=322.31604177964573, r=224.14575036771572, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='0', bbox=BoundingBox(l=232.99721038223674, t=310.6949417154807, r=235.87297038695448, b=317.06189175063525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='0', bbox=BoundingBox(l=232.99721038223674, t=322.31604177964573, r=235.87297038695448, b=328.6829818148003, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=68, text='1', bbox=BoundingBox(l=209.65488034394315, t=334.1317118448851, r=212.5306403486609, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=69, text='1', bbox=BoundingBox(l=221.269990362998, t=334.1317118448851, r=224.14575036771572, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=70, text='1', bbox=BoundingBox(l=232.99721038223674, t=334.1317118448851, r=235.87297038695448, b=340.4986518800397, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=71, text='Input Vector', bbox=BoundingBox(l=187.20328030711084, t=257.09973141955834, r=223.31931036635996, b=264.9359714628257, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='', annotations=[], provenance=None, predicted_class=None, confidence=None), TextElement(label=<DocItemLabel.CAPTION: 'caption'>, id=5, page_no=5, cluster=Cluster(id=5, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=134.05441284179688, t=354.8834533691406, r=480.58676078841245, b=389.0554521481427, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9449623823165894, cells=[Cell(id=72, text='Fig. 4: A visual representation of a convolutional layer. The centre element of the', bbox=BoundingBox(l=134.76500022108476, t=355.21276196128264, r=480.58676078841245, b=365.1454720161255, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=73, text='kernel is placed over the input vector, of which is then calculated and replaced', bbox=BoundingBox(l=134.76500022108476, t=367.16775202729133, r=480.5867307884124, b=377.1004620821341, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=74, text='with a weighted sum of itself and any nearby pixels.', bbox=BoundingBox(l=134.76500022108476, t=379.12274209329996, r=365.1202105989872, b=389.0554521481427, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Fig. 4: A visual representation of a convolutional layer. The centre element of the kernel is placed over the input vector, of which is then calculated and replaced with a weighted sum of itself and any nearby pixels.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=5, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.9464569091797, t=415.9989318847656, r=480.7759707887229, b=450.8507385253906, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9816268682479858, cells=[Cell(id=75, text='Every kernel will have a corresponding activation map, of which will be stacked', bbox=BoundingBox(l=134.76500022108476, t=416.8167423014248, r=480.7759707887229, b=426.74945235626757, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=76, text='along the depth dimension to form the full output volume from the convolu-', bbox=BoundingBox(l=134.76500022108476, t=428.77172236743337, r=480.58682078841264, b=438.7044324222761, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=77, text='tional layer.', bbox=BoundingBox(l=134.76500022108476, t=440.72671243344206, r=186.40115030579494, b=450.6594224882848, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Every kernel will have a corresponding activation map, of which will be stacked along the depth dimension to form the full output volume from the convolutional layer.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=5, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.86756896972656, t=460.1748962402344, r=480.5912807884199, b=542.937255859375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9872538447380066, cells=[Cell(id=78, text='As we alluded to earlier, training ANNs on inputs such as images results in', bbox=BoundingBox(l=134.76500022108476, t=460.9447025450743, r=480.58691078841275, b=470.8774125999171, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=79, text='models of which are too big to train effectively. This comes down to the fully-', bbox=BoundingBox(l=134.76500022108476, t=472.89968261108294, r=480.58676078841245, b=482.8323926659257, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=80, text='connected manner of standard ANN neurons, so to mitigate against this every', bbox=BoundingBox(l=134.76500022108476, t=484.8546726770915, r=480.58679078841254, b=494.7873827319343, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=81, text='neuron in a convolutional layer is only connected to small region of the input', bbox=BoundingBox(l=134.76500022108476, t=496.80966274310015, r=480.5867307884124, b=506.74237279794295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=82, text='volume. The dimensionality of this region is commonly referred to as the', bbox=BoundingBox(l=134.76500022108476, t=508.7646428091088, r=465.0551807629326, b=518.6973528639516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=83, text='re-', bbox=BoundingBox(l=468.4169907684477, t=508.83438280949383, r=480.5912807884199, b=518.5379628630715, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=84, text='ceptive field size', bbox=BoundingBox(l=134.76498022108473, t=520.7893628755024, r=210.161930344775, b=530.49295292908, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=85, text='of the neuron. The magnitude of the connectivity through the', bbox=BoundingBox(l=212.4549903485368, t=520.7196328751174, r=480.5884407884152, b=530.6523429299602, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=86, text='depth is nearly always equal to the depth of the input.', bbox=BoundingBox(l=134.76498022108473, t=532.6756229411317, r=373.18994061222577, b=542.6083329959744, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='As we alluded to earlier, training ANNs on inputs such as images results in models of which are too big to train effectively. This comes down to the fullyconnected manner of standard ANN neurons, so to mitigate against this every neuron in a convolutional layer is only connected to small region of the input volume. The dimensionality of this region is commonly referred to as the receptive field size of the neuron. The magnitude of the connectivity through the depth is nearly always equal to the depth of the input.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=5, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.9088134765625, t=552.2114868164062, r=480.59470078842554, b=622.6449584960938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9872121214866638, cells=[Cell(id=87, text='For example, if the input to the network is an image of size', bbox=BoundingBox(l=134.76498022108473, t=552.8926330527584, r=392.8262006444395, b=562.8253431076012, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=88, text='64', bbox=BoundingBox(l=395.26599064844197, t=553.1018630539137, r=405.22858066478585, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=89, text='×', bbox=BoundingBox(l=407.24899066810036, t=552.5439430508331, r=414.9978906808126, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=90, text='64', bbox=BoundingBox(l=417.0189806841282, t=553.1018630539137, r=426.98157070047205, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=91, text='×', bbox=BoundingBox(l=429.00299070378827, t=552.5439430508331, r=436.7518907165005, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=92, text='3', bbox=BoundingBox(l=438.77298071981613, t=553.1018630539137, r=443.75427072798806, b=561.9486331027606, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=93, text='(a RGB-', bbox=BoundingBox(l=446.1929907319888, t=552.8926330527584, r=480.5938407884241, b=562.8253431076012, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=94, text='coloured image with a dimensionality of', bbox=BoundingBox(l=134.76498022108473, t=564.847623118767, r=312.8862905132964, b=574.7803331736098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=95, text='64', bbox=BoundingBox(l=315.2929705172446, t=565.0568531199224, r=325.25555053358846, b=573.9036231687692, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=96, text='×', bbox=BoundingBox(l=327.1609805367143, t=564.4989331168418, r=334.90988054942653, b=573.9036231687692, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=97, text='64', bbox=BoundingBox(l=336.8149705525519, t=565.0568531199224, r=346.7775605688957, b=573.9036231687692, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=98, text=') and we set the receptive field', bbox=BoundingBox(l=346.77698056889477, t=564.847623118767, r=480.59470078842554, b=574.7803331736098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=99, text='size as', bbox=BoundingBox(l=134.76498022108473, t=576.8026131847757, r=163.6963802685473, b=586.7353232396185, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=100, text='6', bbox=BoundingBox(l=166.5449802732205, t=577.011843185931, r=171.52628028139242, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=101, text='×', bbox=BoundingBox(l=174.00598028546037, t=576.4539131828503, r=181.75490029817266, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=102, text='6', bbox=BoundingBox(l=184.2349903022413, t=577.011843185931, r=189.21628031041323, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=103, text=', we would have a total of', bbox=BoundingBox(l=189.21599031041274, t=576.8026131847757, r=305.3599505009493, b=586.7353232396185, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=104, text='108', bbox=BoundingBox(l=308.2089805056232, t=577.011843185931, r=323.15289053013896, b=585.8586132347777, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=105, text='weights on each neuron within the', bbox=BoundingBox(l=326.00299053481467, t=576.8026131847757, r=480.5926807884222, b=586.7353232396185, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=106, text='convolutional layer. (', bbox=BoundingBox(l=134.76498022108473, t=588.7576132507844, r=228.17432037432468, b=598.6903233056272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=107, text='6', bbox=BoundingBox(l=228.1749903743258, t=588.9668232519396, r=233.1562803824977, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=108, text='×', bbox=BoundingBox(l=235.49298038633108, t=588.4089232488592, r=243.24190039904332, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=109, text='6', bbox=BoundingBox(l=245.57799040287574, t=588.9668232519396, r=250.55928041104767, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=110, text='×', bbox=BoundingBox(l=252.8959804148811, t=588.4089232488592, r=260.6449004275933, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=111, text='3', bbox=BoundingBox(l=262.98099043142577, t=588.9668232519396, r=267.9622804395977, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=112, text='where', bbox=BoundingBox(l=270.618990443956, t=588.7576132507844, r=298.02612048891797, b=598.6903233056272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=113, text='3', bbox=BoundingBox(l=300.68198049327503, t=588.9668232519396, r=305.6632705014469, b=597.8136133007865, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=114, text='is the magnitude of connectivity across', bbox=BoundingBox(l=308.3189705058036, t=588.7576132507844, r=480.5922207884214, b=598.6903233056272, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=115, text='the depth of the volume) To put this into perspective, a standard neuron seen', bbox=BoundingBox(l=134.7649702210847, t=600.7126133167931, r=480.5867307884124, b=610.6453233716359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=116, text='in other forms of ANN would contain', bbox=BoundingBox(l=134.7649702210847, t=612.6686033828073, r=301.70825049495863, b=622.6013134376501, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=117, text='12', bbox=BoundingBox(l=304.1969604990414, t=612.8778233839625, r=314.1595505153852, b=621.7246034328093, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=118, text=',', bbox=BoundingBox(l=314.1599705153859, t=612.8778233839625, r=316.9275805199262, b=621.7246034328093, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=119, text='288', bbox=BoundingBox(l=318.5879805226502, t=612.8778233839625, r=333.5318905471659, b=621.7246034328093, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=120, text='weights each.', bbox=BoundingBox(l=336.0229805512526, t=612.6686033828073, r=395.7687106492667, b=622.6013134376501, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='For example, if the input to the network is an image of size 64 × 64 × 3 (a RGBcoloured image with a dimensionality of 64 × 64 ) and we set the receptive field size as 6 × 6 , we would have a total of 108 weights on each neuron within the convolutional layer. ( 6 × 6 × 3 where 3 is the magnitude of connectivity across the depth of the volume) To put this into perspective, a standard neuron seen in other forms of ANN would contain 12 , 288 weights each.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=9, page_no=5, cluster=Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.94898986816406, t=632.0844116210938, r=480.5867307884124, b=666.8765258789062, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9835361242294312, cells=[Cell(id=121, text='Convolutional layers are also able to significantly reduce the complexity of the', bbox=BoundingBox(l=134.76498022108473, t=632.885603494434, r=480.5867307884124, b=642.8183135492768, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=122, text='model through the optimisation of its output. These are optimised through', bbox=BoundingBox(l=134.76498022108473, t=644.8406035604428, r=480.5867307884124, b=654.7733136152856, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=123, text='three hyperparameters, the', bbox=BoundingBox(l=134.76498022108473, t=656.7956036264516, r=254.06711041680234, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=124, text='depth', bbox=BoundingBox(l=256.5559704208854, t=656.8653436268365, r=283.11627046445807, b=666.5689236804142, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=125, text=', the', bbox=BoundingBox(l=283.1169704644592, t=656.7956036264516, r=301.9164104953001, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=126, text='stride', bbox=BoundingBox(l=304.4069804993859, t=656.8653436268365, r=330.4093605420433, b=666.5689236804142, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=127, text='and setting', bbox=BoundingBox(l=332.8999905461293, t=656.7956036264516, r=381.98572062665545, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=128, text='zero-padding', bbox=BoundingBox(l=384.47598063074076, t=656.8653436268365, r=445.35742073061806, b=666.5689236804142, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=129, text='.', bbox=BoundingBox(l=445.3569907306173, t=656.7956036264516, r=447.84766073470337, b=666.7283136812943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Convolutional layers are also able to significantly reduce the complexity of the model through the optimisation of its output. These are optimised through three hyperparameters, the depth , the stride and setting zero-padding .'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=6, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.85671997070312, t=118.58126831054688, r=480.73211669921875, b=201.79269409179688, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9878745675086975, cells=[Cell(id=2, text='The', bbox=BoundingBox(l=134.76501022108476, t=119.67181066075966, r=151.4424102484444, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='depth', bbox=BoundingBox(l=154.47302025341617, t=119.74151066114439, r=181.03331029698887, b=129.445060714722, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='of the output volume produced by the convolutional layers can be', bbox=BoundingBox(l=184.06403030196083, t=119.67181066075966, r=480.59082078841914, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='manually set through the number of neurons within the layer to a the same', bbox=BoundingBox(l=134.7650302210848, t=131.6268307267684, r=480.58676078841245, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='region of the input. This can be seen with other forms of ANNs, where the', bbox=BoundingBox(l=134.7650302210848, t=143.58184079277714, r=480.5867307884124, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='all of the neurons in the hidden layer are directly connected to every single', bbox=BoundingBox(l=134.7650302210848, t=155.53686085878599, r=480.58682078841264, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='neuron beforehand. Reducing this hyperparameter can significantly minimise', bbox=BoundingBox(l=134.7650302210848, t=167.49188092479483, r=480.5868507884127, b=177.42456097963736, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='the total number of neurons of the network, but it can also significantly reduce', bbox=BoundingBox(l=134.7650302210848, t=179.44787099080895, r=480.58676078841245, b=189.3805510456516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='the pattern recognition capabilities of the model.', bbox=BoundingBox(l=134.7650302210848, t=191.4028910568178, r=348.2934605713826, b=201.33557111166033, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The depth of the output volume produced by the convolutional layers can be manually set through the number of neurons within the layer to a the same region of the input. This can be seen with other forms of ANNs, where the all of the neurons in the hidden layer are directly connected to every single neuron beforehand. Reducing this hyperparameter can significantly minimise the total number of neurons of the network, but it can also significantly reduce the pattern recognition capabilities of the model.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=6, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.76483154296875, t=211.41566467285156, r=480.6409912109375, b=282.1136415576709, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9880473613739014, cells=[Cell(id=11, text='We are also able to define the', bbox=BoundingBox(l=134.7650302210848, t=212.40490117277898, r=259.2974904253829, b=222.33758122762163, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='stride', bbox=BoundingBox(l=261.2180204285336, t=212.47460117316382, r=287.220400471191, b=222.17816122674128, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='in which we set the depth around the spatial', bbox=BoundingBox(l=289.1430104743451, t=212.40490117277898, r=480.5943607884249, b=222.33758122762163, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='dimensionality of the input in order to place the receptive field. For example if', bbox=BoundingBox(l=134.76500022108476, t=224.3609012387932, r=480.58682078841264, b=234.29357129363575, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='we were to set a stride as 1, then we would have a heavily overlapped receptive', bbox=BoundingBox(l=134.76500022108476, t=236.31591130480183, r=480.58676078841245, b=246.24859135964448, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='field producing extremely large activations. Alternatively, setting the stride to a', bbox=BoundingBox(l=134.76500022108476, t=248.27093137081079, r=480.5867307884124, b=258.2036114256533, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='greater number will reduce the amount of overlapping and produce an output', bbox=BoundingBox(l=134.76500022108476, t=260.2259514368196, r=480.5867307884124, b=270.1586314916623, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='of lower spatial dimensions.', bbox=BoundingBox(l=134.76500022108476, t=272.18096150282827, r=258.9986004248926, b=282.1136415576709, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='We are also able to define the stride in which we set the depth around the spatial dimensionality of the input in order to place the receptive field. For example if we were to set a stride as 1, then we would have a heavily overlapped receptive field producing extremely large activations. Alternatively, setting the stride to a greater number will reduce the amount of overlapping and produce an output of lower spatial dimensions.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=4, page_no=6, cluster=Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.788818359375, t=292.6251220703125, r=480.5876207884139, b=327.02667180565516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9830900430679321, cells=[Cell(id=19, text='Zero-padding', bbox=BoundingBox(l=134.76500022108476, t=293.25366161917987, r=197.31020032369145, b=302.95721167275724, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='is the simple process of padding the border of the input, and', bbox=BoundingBox(l=201.02701032978894, t=293.1839616187949, r=480.5876207884139, b=303.1166316736376, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='is an effective method to give further control as to the dimensionality of the', bbox=BoundingBox(l=134.76501022108476, t=305.13897168480366, r=480.58676078841245, b=315.0716517396463, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='output volumes.', bbox=BoundingBox(l=134.76501022108476, t=317.0939917508125, r=207.4521303403295, b=327.02667180565516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Zero-padding is the simple process of padding the border of the input, and is an effective method to give further control as to the dimensionality of the output volumes.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=6, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.85841369628906, t=337.35748291015625, r=480.78466796875, b=372.1994323730469, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9820359349250793, cells=[Cell(id=23, text='It is important to understand that through using these techniques, we will alter', bbox=BoundingBox(l=134.76501022108476, t=338.0969518667789, r=480.58682078841264, b=348.0296619216217, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='the spatial dimensionality of the convolutional layers output. To calculate this,', bbox=BoundingBox(l=134.76501022108476, t=350.0519419327876, r=480.5867307884124, b=359.98465198763034, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='you can make use of the following formula:', bbox=BoundingBox(l=134.76501022108476, t=362.0069219987962, r=326.46536053557315, b=371.93963205363895, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='It is important to understand that through using these techniques, we will alter the spatial dimensionality of the convolutional layers output. To calculate this, you can make use of the following formula:'), TextElement(label=<DocItemLabel.FORMULA: 'formula'>, id=6, page_no=6, cluster=Cluster(id=6, label=<DocItemLabel.FORMULA: 'formula'>, bbox=BoundingBox(l=276.7643127441406, t=395.47357177734375, r=338.3543395996094, b=418.7127623118935, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9492904543876648, cells=[Cell(id=26, text='(', bbox=BoundingBox(l=277.54602045532, t=396.29214218809966, r=281.42047046167613, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='V', bbox=BoundingBox(l=281.42102046167696, t=396.29214218809966, r=287.23221047121035, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='−', bbox=BoundingBox(l=291.66003047847425, t=395.73422218501906, r=299.4089404911865, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='R', bbox=BoundingBox(l=301.6230504948188, t=396.29214218809966, r=309.18765050722874, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text=') + 2', bbox=BoundingBox(l=309.2640405073541, t=396.29214218809966, r=330.29208054185096, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='Z', bbox=BoundingBox(l=330.2960505418574, t=396.29214218809966, r=337.09653055301374, b=405.13891223694645, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='S', bbox=BoundingBox(l=295.7569904851954, t=409.8659922630467, r=301.8660604952175, b=418.7127623118935, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='+ 1', bbox=BoundingBox(l=304.6550004997928, t=409.8659922630467, r=319.59689052430525, b=418.7127623118935, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='( V − R ) + 2 Z S + 1'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=6, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=134.1217041015625, t=437.6549987792969, r=480.6238708496094, b=496.4444885253906, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.986332356929779, cells=[Cell(id=34, text='Where', bbox=BoundingBox(l=134.76500022108476, t=438.3707524204338, r=163.82590026875977, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='V', bbox=BoundingBox(l=165.8860002721394, t=438.579982421589, r=171.6971902816728, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='represents the input volume size (', bbox=BoundingBox(l=175.97101028868408, t=438.3707524204338, r=323.0787705300174, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='height', bbox=BoundingBox(l=323.0780005300161, t=438.579982421589, r=349.92023057405135, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='×', bbox=BoundingBox(l=350.5410205750698, t=438.02206241850854, r=358.289920587782, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='width', bbox=BoundingBox(l=358.9100005887993, t=438.579982421589, r=383.8175006296605, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='×', bbox=BoundingBox(l=384.43600063067515, t=438.02206241850854, r=392.18491064338747, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='depth', bbox=BoundingBox(l=392.80499064440465, t=438.579982421589, r=417.71249068526595, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='),', bbox=BoundingBox(l=417.71201068526517, t=438.3707524204338, r=423.52020069479363, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='R', bbox=BoundingBox(l=425.5800206981728, t=438.579982421589, r=433.1446207105826, b=447.4267524704359, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='represents', bbox=BoundingBox(l=435.28201071408915, t=438.3707524204338, r=480.59192078842096, b=448.3034624752766, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='the receptive field size,', bbox=BoundingBox(l=134.76501022108476, t=450.3257424864424, r=236.31381038767768, b=460.25845254128524, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='Z', bbox=BoundingBox(l=239.082020392219, t=450.5349724875977, r=245.8824904033753, b=459.3817425364445, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='is the amount of zero padding set and', bbox=BoundingBox(l=249.36801040909333, t=450.3257424864424, r=418.18417068603975, b=460.25845254128524, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='S', bbox=BoundingBox(l=420.95203069058044, t=450.5349724875977, r=427.06110070060254, b=459.3817425364445, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='referring to', bbox=BoundingBox(l=430.4080207060932, t=450.3257424864424, r=480.58963078841714, b=460.25845254128524, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='the stride. If the calculated result from this equation is not equal to a whole', bbox=BoundingBox(l=134.76501022108476, t=462.28073255245107, r=480.58676078841245, b=472.2134426072938, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='integer then the stride has been incorrectly set, as the neurons will be unable to', bbox=BoundingBox(l=134.76501022108476, t=474.2357126184597, r=480.58679078841254, b=484.1684226733024, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='fit neatly across the given input.', bbox=BoundingBox(l=134.76501022108476, t=486.19070268446836, r=275.6660504522358, b=496.1234127393111, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Where V represents the input volume size ( height × width × depth ), R represents the receptive field size, Z is the amount of zero padding set and S referring to the stride. If the calculated result from this equation is not equal to a whole integer then the stride has been incorrectly set, as the neurons will be unable to fit neatly across the given input.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=6, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8834686279297, t=506.8936462402344, r=480.5930507884228, b=552.9923730533092, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9862993359565735, cells=[Cell(id=53, text='Despite our best efforts so far we will still find that our models are still enor-', bbox=BoundingBox(l=134.76501022108476, t=507.19369280043486, r=480.58691078841275, b=517.1264028552775, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='mous if we use an image input of any', bbox=BoundingBox(l=134.76501022108476, t=519.1486828664436, r=304.7069704998781, b=529.0813929212864, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='real', bbox=BoundingBox(l=307.7520105048735, t=519.0789428660585, r=322.5166005290951, b=528.8721929201313, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='dimensionality. However, methods', bbox=BoundingBox(l=325.5650005340961, t=519.1486828664436, r=480.5930507884228, b=529.0813929212864, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='have been developed as to greatly curtail the overall number of parameters', bbox=BoundingBox(l=134.76500022108476, t=531.1036629324522, r=480.5866107884122, b=541.036372987295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='within the convolutional layer.', bbox=BoundingBox(l=134.76500022108476, t=543.0596629984664, r=269.78809044259293, b=552.9923730533092, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Despite our best efforts so far we will still find that our models are still enormous if we use an image input of any real dimensionality. However, methods have been developed as to greatly curtail the overall number of parameters within the convolutional layer.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=9, page_no=6, cluster=Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8805694580078, t=563.507568359375, r=480.5939607884243, b=622.790283203125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9858874678611755, cells=[Cell(id=59, text='Parameter sharing', bbox=BoundingBox(l=134.76500022108476, t=564.1314031148125, r=217.39481035664068, b=573.8349931683902, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='works on the assumption that if one region feature is useful', bbox=BoundingBox(l=219.77301036054214, t=564.0616731144275, r=480.5939607884243, b=573.9943831692703, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='to compute at a set spatial region, then it is likely to be useful in another region.', bbox=BoundingBox(l=134.76501022108476, t=576.0166631804361, r=480.58679078841254, b=585.9493732352789, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='If we constrain each individual activation map within the output volume to the', bbox=BoundingBox(l=134.76501022108476, t=587.9726532464504, r=480.5867307884124, b=597.9053633012932, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='same weights and bias, then we will see a massive reduction in the number of', bbox=BoundingBox(l=134.76501022108476, t=599.927653312459, r=480.58670078841243, b=609.8603633673018, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='parameters being produced by the convolutional layer.', bbox=BoundingBox(l=134.76501022108476, t=611.8826633784678, r=376.0691506169492, b=621.8153634333105, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Parameter sharing works on the assumption that if one region feature is useful to compute at a set spatial region, then it is likely to be useful in another region. If we constrain each individual activation map within the output volume to the same weights and bias, then we will see a massive reduction in the number of parameters being produced by the convolutional layer.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=10, page_no=6, cluster=Cluster(id=10, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.85092163085938, t=632.0819702148438, r=480.5868507884127, b=666.8784790039062, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9838565587997437, cells=[Cell(id=65, text='As a result of this as the backpropagation stage occurs, each neuron in the out-', bbox=BoundingBox(l=134.76501022108476, t=632.8856634944344, r=480.58676078841245, b=642.8183735492772, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='put will represent the overall gradient of which can be totalled across the depth', bbox=BoundingBox(l=134.76501022108476, t=644.8406635604431, r=480.5868507884127, b=654.7733736152859, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='- thus only updating a single set of weights, as opposed to every single one.', bbox=BoundingBox(l=134.76501022108476, t=656.7956636264519, r=466.95804076605424, b=666.7283736812946, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='As a result of this as the backpropagation stage occurs, each neuron in the output will represent the overall gradient of which can be totalled across the depth - thus only updating a single set of weights, as opposed to every single one.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=2, page_no=7, cluster=Cluster(id=2, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.94790649414062, t=118.79036712646484, r=218.04713439941406, b=129.75680541992188, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9537138938903809, cells=[Cell(id=2, text='2.3', bbox=BoundingBox(l=134.76500022108476, t=119.74151066114439, r=147.21825024151457, b=129.445060714722, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='Pooling layer', bbox=BoundingBox(l=157.18085025785842, t=119.74151066114439, r=217.79330035729438, b=129.445060714722, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2.3 Pooling layer'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=7, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=134.07662963867188, t=147.324462890625, r=480.58676078841245, b=182.26348100635516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9842262864112854, cells=[Cell(id=4, text='Pooling layers aim to gradually reduce the dimensionality of the representa-', bbox=BoundingBox(l=134.76500022108476, t=148.41980081948964, r=480.58676078841245, b=158.3524708743322, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='tion, and thus further reduce the number of parameters and the computational', bbox=BoundingBox(l=134.76500022108476, t=160.37579088550376, r=480.5867307884124, b=170.3084709403464, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='complexity of the model.', bbox=BoundingBox(l=134.76500022108476, t=172.3308109515126, r=244.44325040101418, b=182.26348100635516, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Pooling layers aim to gradually reduce the dimensionality of the representation, and thus further reduce the number of parameters and the computational complexity of the model.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=4, page_no=7, cluster=Cluster(id=4, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.83888244628906, t=190.89772033691406, r=480.61639404296875, b=260.9205314406546, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9867627024650574, cells=[Cell(id=7, text='The pooling layer operates over each activation map in the input, and scales', bbox=BoundingBox(l=134.76500022108476, t=191.21179105576266, r=480.58679078841254, b=201.1444711106052, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='its dimensionality using the “MAX” function. In most CNNs, these come in the', bbox=BoundingBox(l=134.76500022108476, t=203.1668011217714, r=480.58667078841233, b=213.09948117661406, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='form of', bbox=BoundingBox(l=134.76500022108476, t=215.12182118778026, r=168.0400802756732, b=225.0545012426228, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='max-pooling layers', bbox=BoundingBox(l=171.07899028065862, t=215.1915211881651, r=258.78973042454993, b=224.89508124174267, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='with kernels of a dimensionality of', bbox=BoundingBox(l=261.8280004295342, t=215.12182118778026, r=417.961820685675, b=225.0545012426228, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='2', bbox=BoundingBox(l=421.0000006906592, t=215.33099118893517, r=425.9812906988311, b=224.17779123778212, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='×', bbox=BoundingBox(l=428.5979907031238, t=214.77313118585494, r=436.346890715836, b=224.17779123778212, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='2', bbox=BoundingBox(l=438.9629807201278, t=215.33099118893517, r=443.94427072829967, b=224.17779123778212, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='applied', bbox=BoundingBox(l=446.97800073327664, t=215.12182118778026, r=480.59180078842076, b=225.0545012426228, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='with a stride of', bbox=BoundingBox(l=134.76498022108473, t=227.07782125379447, r=205.8581103377145, b=237.01049130863703, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='2', bbox=BoundingBox(l=209.82999034423042, t=227.2869812549493, r=214.81128035240235, b=236.13378130379635, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='along the spatial dimensions of the input. This scales the', bbox=BoundingBox(l=218.78899035892786, t=227.07782125379447, r=480.5960707884277, b=237.01049130863703, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='activation map down to 25% of the original size - whilst maintaining the depth', bbox=BoundingBox(l=134.76498022108473, t=239.03283131980334, r=480.58670078841243, b=248.96551137464587, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='volume to its standard size.', bbox=BoundingBox(l=134.76498022108473, t=250.98785138581206, r=255.8603704197442, b=260.9205314406546, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The pooling layer operates over each activation map in the input, and scales its dimensionality using the “MAX” function. In most CNNs, these come in the form of max-pooling layers with kernels of a dimensionality of 2 × 2 applied with a stride of 2 along the spatial dimensions of the input. This scales the activation map down to 25% of the original size - whilst maintaining the depth volume to its standard size.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=7, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8048095703125, t=269.3749694824219, r=480.6969909667969, b=351.6319580078125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9878619909286499, cells=[Cell(id=21, text='Due to the destructive nature of the pooling layer, there are only two generally', bbox=BoundingBox(l=134.76498022108473, t=269.8688314900621, r=480.58679078841254, b=279.80151154490466, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='observed methods of max-pooling. Usually, the stride and filters of the pooling', bbox=BoundingBox(l=134.76498022108473, t=281.82385155607096, r=480.58682078841264, b=291.7565316109135, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='layers are both set to', bbox=BoundingBox(l=134.76498022108473, t=293.779841622085, r=228.8518103754361, b=303.7125216769276, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='2', bbox=BoundingBox(l=232.17899038089442, t=293.98901162324, r=237.16028038906634, b=302.83581167208695, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='×', bbox=BoundingBox(l=239.99298039371342, t=293.43115162015977, r=247.74190040642566, b=302.83581167208695, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='2', bbox=BoundingBox(l=250.57397041107177, t=293.98901162324, r=255.55527041924367, b=302.83581167208695, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text=', which will allow the layer to extend through the', bbox=BoundingBox(l=255.55498041924324, t=293.779841622085, r=480.590150788418, b=303.7125216769276, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='entirety of the spatial dimensionality of the input. Furthermore', bbox=BoundingBox(l=134.76498022108473, t=305.7348616880938, r=421.52844069152604, b=315.66754174293646, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='overlapping', bbox=BoundingBox(l=425.23898069761333, t=305.80456168847866, r=480.5911907884197, b=315.50811174205614, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='pooling', bbox=BoundingBox(l=134.76498022108473, t=317.7595817544875, r=170.19199027920348, b=327.4631318080651, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='may be utilised, where the stride is set to', bbox=BoundingBox(l=173.6349802848518, t=317.68988175410277, r=360.75253059182194, b=327.6225518089452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='2', bbox=BoundingBox(l=364.1929905974661, t=317.8990417552576, r=369.174290605638, b=326.74584180410454, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='with a kernel size set to', bbox=BoundingBox(l=372.6170006112859, t=317.68988175410277, r=480.5916407884205, b=327.6225518089452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='3', bbox=BoundingBox(l=134.76500022108476, t=329.8540618212663, r=139.74629022925666, b=338.7008318701132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='. Due to the destructive nature of pooling, having a kernel size above', bbox=BoundingBox(l=139.7460002292562, t=329.6448318201111, r=452.0536207416033, b=339.57754187495385, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='3', bbox=BoundingBox(l=455.33002074697833, t=329.8540618212663, r=460.3113107551502, b=338.7008318701132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='will', bbox=BoundingBox(l=463.5860307605225, t=329.6448318201111, r=480.5921907884214, b=339.57754187495385, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='usually greatly decrease the performance of the model.', bbox=BoundingBox(l=134.76501022108476, t=341.59982188611974, r=376.31824061735784, b=351.5325319409625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Due to the destructive nature of the pooling layer, there are only two generally observed methods of max-pooling. Usually, the stride and filters of the pooling layers are both set to 2 × 2 , which will allow the layer to extend through the entirety of the spatial dimensionality of the input. Furthermore overlapping pooling may be utilised, where the stride is set to 2 with a kernel size set to 3 . Due to the destructive nature of pooling, having a kernel size above 3 will usually greatly decrease the performance of the model.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=7, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.9580078125, t=359.5020751953125, r=480.59521078842636, b=418.52410888671875, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9869884252548218, cells=[Cell(id=39, text='It is also important to understand that beyond max-pooling, CNN architectures', bbox=BoundingBox(l=134.76501022108476, t=360.4818119903754, r=480.58679078841254, b=370.4145220452181, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='may contain general-pooling.', bbox=BoundingBox(l=134.76501022108476, t=372.436792056384, r=263.6810304325742, b=382.3695021112268, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='General pooling', bbox=BoundingBox(l=265.9370104362751, t=372.506532056769, r=340.13846055800417, b=382.21011211034676, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='layers are comprised of pooling', bbox=BoundingBox(l=342.3940105617044, t=372.436792056384, r=480.59521078842636, b=382.3695021112268, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='neurons that are able to perform a multitude of common operations including', bbox=BoundingBox(l=134.76501022108476, t=384.39178212239267, r=480.58679078841254, b=394.3244921772354, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='L1/L2-normalisation, and average pooling. However, this tutorial will primar-', bbox=BoundingBox(l=134.76501022108476, t=396.34677218840125, r=480.5868507884127, b=406.2794822432441, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='ily focus on the use of max-pooling.', bbox=BoundingBox(l=134.76501022108476, t=408.30175225440985, r=292.00470047903974, b=418.23446230925265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='It is also important to understand that beyond max-pooling, CNN architectures may contain general-pooling. General pooling layers are comprised of pooling neurons that are able to perform a multitude of common operations including L1/L2-normalisation, and average pooling. However, this tutorial will primarily focus on the use of max-pooling.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=7, page_no=7, cluster=Cluster(id=7, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.02223205566406, t=446.0529479980469, r=255.7871551513672, b=456.9926452636719, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9582642912864685, cells=[Cell(id=46, text='2.4', bbox=BoundingBox(l=134.76501022108476, t=447.08248246853503, r=147.21826024151457, b=456.78607252211265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='Fully-connected layer', bbox=BoundingBox(l=157.18086025785843, t=447.08248246853503, r=255.41211041900885, b=456.78607252211265, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2.4 Fully-connected layer'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=7, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.71995544433594, t=474.6971435546875, r=480.58682078841264, b=521.6184692382812, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9859198331832886, cells=[Cell(id=48, text='The fully-connected layer contains neurons of which are directly connected to', bbox=BoundingBox(l=134.76501022108476, t=475.76174262688556, r=480.58682078841264, b=485.6944526817283, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='the neurons in the two adjacent layers, without being connected to any layers', bbox=BoundingBox(l=134.76501022108476, t=487.7167326928942, r=480.58676078841245, b=497.649442747737, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='within them. This is analogous to way that neurons are arranged in traditional', bbox=BoundingBox(l=134.76501022108476, t=499.6717227589028, r=480.5867307884124, b=509.60443281374563, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=51, text='forms of ANN. (Figure 1)', bbox=BoundingBox(l=134.76501022108476, t=511.62771282491707, r=246.1269404037763, b=521.5604228797598, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The fully-connected layer contains neurons of which are directly connected to the neurons in the two adjacent layers, without being connected to any layers within them. This is analogous to way that neurons are arranged in traditional forms of ANN. (Figure 1)'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=9, page_no=7, cluster=Cluster(id=9, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.84336853027344, t=550.3342895507812, r=195.18660032020762, b=562.74267578125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9564018249511719, cells=[Cell(id=52, text='3', bbox=BoundingBox(l=134.76501022108476, t=550.9896830422515, r=140.74261023089116, b=562.6340631065451, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='Recipes', bbox=BoundingBox(l=152.6978102505039, t=550.9896830422515, r=195.18660032020762, b=562.6340631065451, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='3 Recipes'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=10, page_no=7, cluster=Cluster(id=10, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87806701660156, t=583.9160766601562, r=480.6416015625, b=666.999755859375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9855859875679016, cells=[Cell(id=54, text='Despite the relatively small number of layers required to form a CNN, there', bbox=BoundingBox(l=134.76501022108476, t=585.0647232303944, r=480.58682078841264, b=594.9974332852372, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='is no set way of formulating a CNN architecture. That being said, it would be', bbox=BoundingBox(l=134.76501022108476, t=597.0197232964031, r=480.58679078841254, b=606.9524333512459, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='idiotic to simply throw a few of layers together and expect it to work. Through', bbox=BoundingBox(l=134.76501022108476, t=608.9747333624118, r=480.58682078841264, b=618.9074434172546, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='reading of related literature it is obvious that much like other forms of ANNs,', bbox=BoundingBox(l=134.76501022108476, t=620.9297334284206, r=480.5868507884127, b=630.8624434832634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='CNNs tend to follow a common architecture. This common architecture is illus-', bbox=BoundingBox(l=134.76501022108476, t=632.8857234944347, r=480.58676078841245, b=642.8184335492775, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='trated in Figure 2, where convolutional layers are stacked, followed by pooling', bbox=BoundingBox(l=134.76501022108476, t=644.8407235604434, r=480.58688078841266, b=654.7734336152862, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='layers in a repeated manner before feeding forward to fully-connected layers.', bbox=BoundingBox(l=134.76501022108476, t=656.7957336264523, r=475.0078107792602, b=666.728433681295, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Despite the relatively small number of layers required to form a CNN, there is no set way of formulating a CNN architecture. That being said, it would be idiotic to simply throw a few of layers together and expect it to work. Through reading of related literature it is obvious that much like other forms of ANNs, CNNs tend to follow a common architecture. This common architecture is illustrated in Figure 2, where convolutional layers are stacked, followed by pooling layers in a repeated manner before feeding forward to fully-connected layers.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=8, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.87208557128906, t=118.63634490966797, r=480.66082763671875, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9839420914649963, cells=[Cell(id=2, text='Another common CNN architecture is to stack two convolutional layers before', bbox=BoundingBox(l=134.76501022108476, t=119.67181066075966, r=480.58676078841245, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='each pooling layer, as illustrated in Figure 5. This is strongly recommended as', bbox=BoundingBox(l=134.76501022108476, t=131.6268307267684, r=480.58682078841264, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='stacking multiple convolutional layers allows for more complex features of the', bbox=BoundingBox(l=134.76501022108476, t=143.58184079277714, r=480.5868507884127, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='input vector to be selected.', bbox=BoundingBox(l=134.76501022108476, t=155.53686085878599, r=252.77200041467768, b=165.46954091362863, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Another common CNN architecture is to stack two convolutional layers before each pooling layer, as illustrated in Figure 5. This is strongly recommended as stacking multiple convolutional layers allows for more complex features of the input vector to be selected.'), FigureElement(label=<DocItemLabel.PICTURE: 'picture'>, id=3, page_no=8, cluster=Cluster(id=3, label=<DocItemLabel.PICTURE: 'picture'>, bbox=BoundingBox(l=133.3032989501953, t=188.10928344726562, r=481.8245507904431, b=277.20819153058585, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9117528200149536, cells=[Cell(id=6, text='input', bbox=BoundingBox(l=141.0171202313415, t=254.84454140710648, r=152.0187102493898, b=260.8498514402644, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='convolution w/ ReLu', bbox=BoundingBox(l=175.69994028823936, t=190.0086610491196, r=221.95847036412746, b=196.01397108227752, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='pooling', bbox=BoundingBox(l=232.13184038081707, t=190.21557105026216, r=248.78818040814215, b=196.2333910834891, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='convolution', bbox=BoundingBox(l=350.42499057487936, t=264.6906114614709, r=376.18756061714345, b=270.6959214946288, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='w/ ReLu', bbox=BoundingBox(l=353.9647205806864, t=271.1903614973588, r=372.95895061184683, b=277.20819153058585, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='pooling', bbox=BoundingBox(l=380.33847062395307, t=189.10449104412737, r=396.9493106512036, b=195.10980107728528, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='fully-connected', bbox=BoundingBox(l=399.56500065549466, t=262.8054814510623, r=433.6465807114061, b=268.8107914842202, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='w/ ReLu', bbox=BoundingBox(l=407.1688206679688, t=269.30072148692534, r=426.1585706991219, b=275.3185415201523, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='fully-connected', bbox=BoundingBox(l=420.5636606899434, t=189.10449104412737, r=454.6497807458623, b=195.10980107728528, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=15, text='convolution w/ ReLu', bbox=BoundingBox(l=268.3968804403106, t=188.21289103920446, r=314.67807051623583, b=194.2307110724313, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='pooling', bbox=BoundingBox(l=324.8378305329031, t=188.4489710405079, r=341.4487005601536, b=194.45434107366623, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='0', bbox=BoundingBox(l=472.1900607746376, t=219.15814121006656, r=474.9025007790873, b=225.16345124322447, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=18, text='9', bbox=BoundingBox(l=472.1900607746376, t=243.45690134423035, r=474.9025007790873, b=249.46221137738826, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='output ', bbox=BoundingBox(l=466.57080076541905, t=253.0985113974658, r=481.8245507904431, b=259.11633143069287, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='...', bbox=BoundingBox(l=471.47577077346574, t=230.8848212748146, r=475.54437078014035, b=236.89013130797252, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='', annotations=[], provenance=None, predicted_class=None, confidence=None), TextElement(label=<DocItemLabel.CAPTION: 'caption'>, id=4, page_no=8, cluster=Cluster(id=4, label=<DocItemLabel.CAPTION: 'caption'>, bbox=BoundingBox(l=134.1597137451172, t=287.998046875, r=480.640380859375, b=322.92626953125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9685500264167786, cells=[Cell(id=21, text='Fig. 5: A common form of CNN architecture in which convolutional layers are', bbox=BoundingBox(l=134.76500022108476, t=288.8927615951013, r=480.58682078841264, b=298.825431649944, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='stacked between ReLus continuously before being passed through the pooling', bbox=BoundingBox(l=134.76500022108476, t=300.8477716611102, r=480.5867307884124, b=310.7804517159527, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='layer, before going between one or many fully connected ReLus.', bbox=BoundingBox(l=134.76500022108476, t=312.8027917271189, r=417.1848806844004, b=322.73547178196156, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Fig. 5: A common form of CNN architecture in which convolutional layers are stacked between ReLus continuously before being passed through the pooling layer, before going between one or many fully connected ReLus.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=8, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.85641479492188, t=348.0907897949219, r=480.6982727050781, b=478.8450622558594, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9880061745643616, cells=[Cell(id=24, text='It is also advised to split large convolutional layers up into many smaller sized', bbox=BoundingBox(l=134.76500022108476, t=349.1437619277732, r=480.58682078841264, b=359.0764719826159, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='convolutional layers. This is to reduce the amount of computational complexity', bbox=BoundingBox(l=134.76500022108476, t=361.0987519937818, r=480.5867307884124, b=371.0314620486245, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='within a given convolutional layer. For example, if you were to stack three con-', bbox=BoundingBox(l=134.76500022108476, t=373.0537420597904, r=480.58679078841254, b=382.9864521146332, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='volutional layers on top of each other with a receptive field of', bbox=BoundingBox(l=134.76500022108476, t=385.00973212580453, r=400.48746065700794, b=394.9424421806474, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='3', bbox=BoundingBox(l=402.51202066032926, t=385.2189621269598, r=407.4933206685012, b=394.0657321758067, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='×', bbox=BoundingBox(l=407.9780306692964, t=384.6610421238793, r=415.7269306820086, b=394.0657321758067, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='3', bbox=BoundingBox(l=416.2110306828028, t=385.2189621269598, r=421.19232069097467, b=394.0657321758067, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='. Each neuron', bbox=BoundingBox(l=421.19202069097423, t=385.00973212580453, r=480.5890507884163, b=394.9424421806474, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='of the first convolutional layer will have a', bbox=BoundingBox(l=134.76501022108476, t=396.9647221918132, r=316.8913005198667, b=406.897432246656, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='3', bbox=BoundingBox(l=319.1640005235951, t=397.1739521929685, r=324.145290531767, b=406.02072224181535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='×', bbox=BoundingBox(l=325.55801053408464, t=396.6160221898879, r=333.30692054679685, b=406.02072224181535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='3', bbox=BoundingBox(l=334.72000054911507, t=397.1739521929685, r=339.70129055728694, b=406.02072224181535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='view of the input vector. A neu-', bbox=BoundingBox(l=341.97601056101865, t=396.9647221918132, r=480.59567078842707, b=406.897432246656, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='ron on the second convolutional layer will then have a', bbox=BoundingBox(l=134.76501022108476, t=408.9197022578219, r=376.1587806170962, b=418.8524123126646, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='5', bbox=BoundingBox(l=378.9180306216228, t=409.1289322589771, r=383.8993206297947, b=417.9757023078239, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='×', bbox=BoundingBox(l=386.3090206337479, t=408.5710122558965, r=394.05792064646016, b=417.9757023078239, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='5', bbox=BoundingBox(l=396.4670106504123, t=409.1289322589771, r=401.44830065858423, b=417.9757023078239, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='view of the input', bbox=BoundingBox(l=404.2030006631033, t=408.9197022578219, r=480.5961907884279, b=418.8524123126646, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=42, text='vector. A neuron on the third convolutional layer will then have a', bbox=BoundingBox(l=134.76501022108476, t=420.8746923238305, r=421.9369506921963, b=430.8074023786733, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='7', bbox=BoundingBox(l=424.30200069607616, t=421.0839223249858, r=429.2832907042481, b=429.9306923738326, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='×', bbox=BoundingBox(l=431.026000707107, t=420.5260023219052, r=438.7749007198193, b=429.9306923738326, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=45, text='7', bbox=BoundingBox(l=440.51700072267727, t=421.0839223249858, r=445.49829073084913, b=429.9306923738326, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='view of', bbox=BoundingBox(l=447.86200073472685, t=420.8746923238305, r=480.5891407884164, b=430.8074023786733, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='the input vector. As these stacks feature non-linearities which in turn allows us', bbox=BoundingBox(l=134.76501022108476, t=432.82968238983915, r=480.58679078841254, b=442.76239244468195, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=48, text='to express stronger features of the input with fewer parameters. However, it is', bbox=BoundingBox(l=134.76501022108476, t=444.78466245584775, r=480.58682078841264, b=454.7173725106905, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='important to understand that this does come with a distinct memory allocation', bbox=BoundingBox(l=134.76501022108476, t=456.74066252186196, r=480.58679078841254, b=466.67337257670476, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='problem - especially when making use of the backpropagation algorithm.', bbox=BoundingBox(l=134.76501022108476, t=468.69564258787057, r=457.4038107503804, b=478.62835264271337, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='It is also advised to split large convolutional layers up into many smaller sized convolutional layers. This is to reduce the amount of computational complexity within a given convolutional layer. For example, if you were to stack three convolutional layers on top of each other with a receptive field of 3 × 3 . Each neuron of the first convolutional layer will have a 3 × 3 view of the input vector. A neuron on the second convolutional layer will then have a 5 × 5 view of the input vector. A neuron on the third convolutional layer will then have a 7 × 7 view of the input vector. As these stacks feature non-linearities which in turn allows us to express stronger features of the input with fewer parameters. However, it is important to understand that this does come with a distinct memory allocation problem - especially when making use of the backpropagation algorithm.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=8, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.77923583984375, t=486.6978454589844, r=480.58676078841245, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.966922402381897, cells=[Cell(id=51, text='The input layer should be recursively divisible by two. Common numbers in-', bbox=BoundingBox(l=134.76501022108476, t=487.5596626920269, r=480.58676078841245, b=497.4923727468697, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='clude', bbox=BoundingBox(l=134.76501022108476, t=499.51464275803556, r=158.95421026076767, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='32', bbox=BoundingBox(l=161.44402026485224, t=499.72387275919084, r=171.4066202811961, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='×', bbox=BoundingBox(l=173.62102028482886, t=499.16595275611024, r=181.36993029754112, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=55, text='32', bbox=BoundingBox(l=183.58302030117173, t=499.72387275919084, r=193.54562031751558, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text=',', bbox=BoundingBox(l=193.54602031751622, t=499.51464275803556, r=196.0366703216022, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='64', bbox=BoundingBox(l=198.52702032568766, t=499.72387275919084, r=208.48962034203151, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=58, text='×', bbox=BoundingBox(l=210.7040303456643, t=499.16595275611024, r=218.45294035837657, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='64', bbox=BoundingBox(l=220.66603036200718, t=499.72387275919084, r=230.628630378351, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text=',', bbox=BoundingBox(l=230.6290303783517, t=499.51464275803556, r=233.11967038243762, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='96', bbox=BoundingBox(l=235.6100303865231, t=499.72387275919084, r=245.572630402867, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=62, text='×', bbox=BoundingBox(l=247.7870304064997, t=499.16595275611024, r=255.535950419212, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='96', bbox=BoundingBox(l=257.7500304228442, t=499.72387275919084, r=267.7126204391881, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text=',', bbox=BoundingBox(l=267.7120404391871, t=499.51464275803556, r=270.20270044327305, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=65, text='128', bbox=BoundingBox(l=272.69403044736015, t=499.72387275919084, r=287.637940471876, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='×', bbox=BoundingBox(l=289.85104047550664, t=499.16595275611024, r=297.5999504882188, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='128', bbox=BoundingBox(l=299.81406049185114, t=499.72387275919084, r=314.7579705163669, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=68, text='and', bbox=BoundingBox(l=317.2490505204536, t=499.51464275803556, r=334.1157205481237, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=69, text='224', bbox=BoundingBox(l=336.6060505522091, t=499.72387275919084, r=351.54996057672497, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=70, text='×', bbox=BoundingBox(l=353.76404058035723, t=499.16595275611024, r=361.51294059306946, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=71, text='224', bbox=BoundingBox(l=363.72705059670176, t=499.72387275919084, r=378.6709606212175, b=508.57064280803763, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=72, text='.', bbox=BoundingBox(l=378.67105062121766, t=499.51464275803556, r=381.16171062530367, b=509.4473528128783, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The input layer should be recursively divisible by two. Common numbers include 32 × 32 , 64 × 64 , 96 × 96 , 128 × 128 and 224 × 224 .'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=8, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.83213806152344, t=517.3685302734375, r=480.58688078841266, b=564.4635009765625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.982406735420227, cells=[Cell(id=73, text='Whilst using small filters, set stride to one and make use of zero-padding as to', bbox=BoundingBox(l=134.76505022108483, t=518.3796328621972, r=480.5867307884124, b=528.31234291704, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=74, text='ensure that the convolutional layers do not reconfigure any of the dimension-', bbox=BoundingBox(l=134.76505022108483, t=530.3346229282059, r=480.58682078841264, b=540.2673329830487, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=75, text='ality of the input. The amount of zero-padding to be used should be calculated', bbox=BoundingBox(l=134.76505022108483, t=542.2896129942146, r=480.5868507884127, b=552.2223230490574, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=76, text='by taking one away from the receptive field size and dividing by two.activation', bbox=BoundingBox(l=134.76505022108483, t=554.2445930602232, r=480.58688078841266, b=564.177303115066, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Whilst using small filters, set stride to one and make use of zero-padding as to ensure that the convolutional layers do not reconfigure any of the dimensionality of the input. The amount of zero-padding to be used should be calculated by taking one away from the receptive field size and dividing by two.activation'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=8, page_no=8, cluster=Cluster(id=8, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.7228240966797, t=571.9942626953125, r=480.7103271484375, b=667.2128295898438, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9852842688560486, cells=[Cell(id=77, text='CNNs are extremely powerful machine learning algorithms, however they can', bbox=BoundingBox(l=134.76505022108483, t=573.109583164385, r=480.5868507884127, b=583.0422932192278, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=78, text='be horrendously resource-heavy. An example of this problem could be in filter-', bbox=BoundingBox(l=134.76505022108483, t=585.0646032303937, r=480.58698078841286, b=594.9972932852364, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=79, text='ing a large image (anything over', bbox=BoundingBox(l=134.76505022108483, t=597.0195932964024, r=277.987370456044, b=606.9523033512452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=80, text='128', bbox=BoundingBox(l=280.46405046010705, t=597.2288032975575, r=295.40796048462283, b=606.0755933464045, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=81, text='×', bbox=BoundingBox(l=297.56506048816163, t=596.6708932944771, r=305.31396050087386, b=606.0755933464045, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=82, text='128', bbox=BoundingBox(l=307.470060504411, t=597.2288032975575, r=322.41397052892677, b=606.0755933464045, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=83, text='could be considered large), so if the', bbox=BoundingBox(l=324.8900805329889, t=597.0195932964024, r=480.59555078842686, b=606.9523033512452, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=84, text='input is', bbox=BoundingBox(l=134.7650802210849, t=608.974593362411, r=168.67776027671934, b=618.9073034172538, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=85, text='227', bbox=BoundingBox(l=171.53407028140518, t=609.1838033635662, r=186.47797030592096, b=618.0305934124132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=86, text='×', bbox=BoundingBox(l=188.9600703099929, t=608.6259033604858, r=196.70898032270514, b=618.0305934124132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=87, text='227', bbox=BoundingBox(l=199.19107032677704, t=609.1838033635662, r=214.13496035129282, b=618.0305934124132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=88, text='(as seen with ImageNet) and we’re filtering with 64 kernels', bbox=BoundingBox(l=216.98807035597338, t=608.974593362411, r=480.5884707884153, b=618.9073034172538, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=89, text='each with a zero padding of then the result will be three activation vectors of', bbox=BoundingBox(l=134.7650802210849, t=620.9295934284198, r=480.5868507884127, b=630.8623034832626, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=90, text='size', bbox=BoundingBox(l=134.7650802210849, t=632.885583494434, r=151.64172024877135, b=642.8182935492767, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=91, text='227', bbox=BoundingBox(l=154.46307025339985, t=633.0948034955892, r=169.4069702779156, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=92, text='×', bbox=BoundingBox(l=171.86607028194985, t=632.5368934925087, r=179.6149902946621, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=93, text='227', bbox=BoundingBox(l=182.07407029869626, t=633.0948034955892, r=197.017960323212, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=94, text='×', bbox=BoundingBox(l=199.47707032724625, t=632.5368934925087, r=207.22598033995848, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=95, text='64', bbox=BoundingBox(l=209.68506034399266, t=633.0948034955892, r=219.64766036033652, b=641.9415835444361, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=96, text='- which calculates to roughly 10 million activations - or an', bbox=BoundingBox(l=222.4690603649651, t=632.885583494434, r=480.590120788418, b=642.8182935492767, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=97, text='enormous 70 megabytes of memory per image. In this case you have two op-', bbox=BoundingBox(l=134.76505022108483, t=644.8405935604427, r=480.58679078841254, b=654.7733036152855, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=98, text='tions. Firstly, you can reduce the spatial dimensionality of the input images by', bbox=BoundingBox(l=134.76505022108483, t=656.7955936264514, r=480.58676078841245, b=666.7283036812942, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='CNNs are extremely powerful machine learning algorithms, however they can be horrendously resource-heavy. An example of this problem could be in filtering a large image (anything over 128 × 128 could be considered large), so if the input is 227 × 227 (as seen with ImageNet) and we’re filtering with 64 kernels each with a zero padding of then the result will be three activation vectors of size 227 × 227 × 64 - which calculates to roughly 10 million activations - or an enormous 70 megabytes of memory per image. In this case you have two options. Firstly, you can reduce the spatial dimensionality of the input images by'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=2, page_no=9, cluster=Cluster(id=2, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8414764404297, t=118.74977111816406, r=480.58688078841266, b=153.6955108642578, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9780817627906799, cells=[Cell(id=2, text='resizing the raw images to something a little less heavy. Alternatively, you can', bbox=BoundingBox(l=134.76500022108476, t=119.67181066075966, r=480.58682078841264, b=129.6044907156022, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='go against everything we stated earlier in this document and opt for larger filter', bbox=BoundingBox(l=134.76500022108476, t=131.6268307267684, r=480.58688078841266, b=141.55950078161106, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='sizes with a larger stride (2, as opposed to 1).', bbox=BoundingBox(l=134.76500022108476, t=143.58184079277714, r=331.9746705446113, b=153.5145208476198, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='resizing the raw images to something a little less heavy. Alternatively, you can go against everything we stated earlier in this document and opt for larger filter sizes with a larger stride (2, as opposed to 1).'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=3, page_no=9, cluster=Cluster(id=3, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=134.21324157714844, t=160.7611083984375, r=480.58679078841254, b=207.31256114466203, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9785709977149963, cells=[Cell(id=5, text='In addition to the few rules-of-thumb outlined above, it is also important to ac-', bbox=BoundingBox(l=134.76500022108476, t=161.51483089179294, r=480.58679078841254, b=171.4475109466356, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='knowledge a few ’tricks’ about generalised ANN training techniques. The au-', bbox=BoundingBox(l=134.76500022108476, t=173.4698409578017, r=480.5867307884124, b=183.40252101264423, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=7, text='thors suggest a read of Geoffrey Hinton’s excellent “Practical Guide to Training', bbox=BoundingBox(l=134.76500022108476, t=185.42486102381054, r=480.58670078841243, b=195.35754107865318, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='Restricted Boltzmann Machines”.', bbox=BoundingBox(l=134.76500022108476, t=197.37988108981926, r=281.8328904623527, b=207.31256114466203, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='In addition to the few rules-of-thumb outlined above, it is also important to acknowledge a few ’tricks’ about generalised ANN training techniques. The authors suggest a read of Geoffrey Hinton’s excellent “Practical Guide to Training Restricted Boltzmann Machines”.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=4, page_no=9, cluster=Cluster(id=4, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.84384155273438, t=231.3874969482422, r=215.1159103529021, b=243.4951713444417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9349105358123779, cells=[Cell(id=9, text='4', bbox=BoundingBox(l=134.76500022108476, t=231.85083128014833, r=140.74260023089116, b=243.4951713444417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=10, text='Conclusion', bbox=BoundingBox(l=152.6978002505039, t=231.85083128014833, r=215.1159103529021, b=243.4951713444417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='4 Conclusion'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=5, page_no=9, cluster=Cluster(id=5, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.68148803710938, t=259.8370056152344, r=480.58682078841264, b=306.9059143066406, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.982923686504364, cells=[Cell(id=11, text='Convolutional Neural Networks differ to other forms of Artifical Neural Net-', bbox=BoundingBox(l=134.76500022108476, t=261.03387144128044, r=480.58676078841245, b=270.9665514961231, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='work in that instead of focusing on the entirety of the problem domain, knowl-', bbox=BoundingBox(l=134.76500022108476, t=272.98986150729456, r=480.58682078841264, b=282.9225415621372, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='edge about the specific type of input is exploited. This in turn allows for a much', bbox=BoundingBox(l=134.76500022108476, t=284.9448815733035, r=480.5867307884124, b=294.87756162814617, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='simpler network architecture to be set up.', bbox=BoundingBox(l=134.76500022108476, t=296.89990163931225, r=318.04703052176274, b=306.8325816941549, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Convolutional Neural Networks differ to other forms of Artifical Neural Network in that instead of focusing on the entirety of the problem domain, knowledge about the specific type of input is exploited. This in turn allows for a much simpler network architecture to be set up.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=6, page_no=9, cluster=Cluster(id=6, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.798095703125, t=313.9832458496094, r=480.58679078841254, b=348.8280944824219, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9790396094322205, cells=[Cell(id=15, text='This paper has outlined the basic concepts of Convolutional Neural Networks,', bbox=BoundingBox(l=134.76500022108476, t=314.83288173832796, r=480.58679078841254, b=324.7655617931705, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='explaining the layers required to build one and detailing how best to structure', bbox=BoundingBox(l=134.76500022108476, t=326.7878418043364, r=480.5867307884124, b=336.7205518591792, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='the network in most image analysis tasks.', bbox=BoundingBox(l=134.76500022108476, t=338.74282187034504, r=318.21628052204034, b=348.67553192518784, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='This paper has outlined the basic concepts of Convolutional Neural Networks, explaining the layers required to build one and detailing how best to structure the network in most image analysis tasks.'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=7, page_no=9, cluster=Cluster(id=7, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.8413543701172, t=356.0538024902344, r=480.8612365722656, b=426.3844923542525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9865900278091431, cells=[Cell(id=18, text='Research in the field of image analysis using neural networks has somewhat', bbox=BoundingBox(l=134.76500022108476, t=356.675841969361, r=480.58667078841233, b=366.60855202420373, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='slowed in recent times. This is partly due to the incorrect belief surrounding the', bbox=BoundingBox(l=134.76500022108476, t=368.6308220353696, r=480.58676078841245, b=378.5635320902124, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='level of complexity and knowledge required to begin modelling these superbly', bbox=BoundingBox(l=134.76500022108476, t=380.5858121013783, r=480.58676078841245, b=390.518522156221, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='powerful machine learning algorithms. The authors hope that this paper has', bbox=BoundingBox(l=134.76500022108476, t=392.54080216738686, r=480.58679078841254, b=402.47351222222966, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=22, text='in some way reduced this confusion, and made the field more accessible to', bbox=BoundingBox(l=134.76500022108476, t=404.49679223340104, r=480.58667078841233, b=414.4295022882438, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='beginners.', bbox=BoundingBox(l=134.76500022108476, t=416.4517822994097, r=180.5033002961194, b=426.3844923542525, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Research in the field of image analysis using neural networks has somewhat slowed in recent times. This is partly due to the incorrect belief surrounding the level of complexity and knowledge required to begin modelling these superbly powerful machine learning algorithms. The authors hope that this paper has in some way reduced this confusion, and made the field more accessible to beginners.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=8, page_no=9, cluster=Cluster(id=8, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=133.9271240234375, t=450.19580078125, r=243.67688039975695, b=462.5867004394531, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9575116038322449, cells=[Cell(id=24, text='Acknowledgements', bbox=BoundingBox(l=134.76500022108476, t=450.9227624897389, r=243.67688039975695, b=462.5671325540324, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Acknowledgements'), TextElement(label=<DocItemLabel.TEXT: 'text'>, id=9, page_no=9, cluster=Cluster(id=9, label=<DocItemLabel.TEXT: 'text'>, bbox=BoundingBox(l=133.86122131347656, t=479.3764343261719, r=480.82696533203125, b=502.0371398925781, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9725787043571472, cells=[Cell(id=25, text='The authors would like to thank Dr. Chuan Lu and Nicholas Dimonaco for', bbox=BoundingBox(l=134.76500022108476, t=480.105802650871, r=480.58667078841233, b=490.03851270571374, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=26, text='useful discussion and suggestions.', bbox=BoundingBox(l=134.76500022108476, t=492.0607927168796, r=286.96362047076974, b=501.99350277172243, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='The authors would like to thank Dr. Chuan Lu and Nicholas Dimonaco for useful discussion and suggestions.'), TextElement(label=<DocItemLabel.SECTION_HEADER: 'section_header'>, id=10, page_no=9, cluster=Cluster(id=10, label=<DocItemLabel.SECTION_HEADER: 'section_header'>, bbox=BoundingBox(l=134.29351806640625, t=525.9068603515625, r=194.5290503191289, b=538.1761429715024, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9404545426368713, cells=[Cell(id=27, text='References', bbox=BoundingBox(l=134.76500022108476, t=526.5317629072088, r=194.5290503191289, b=538.1761429715024, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='References'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=11, page_no=9, cluster=Cluster(id=11, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.7662811279297, t=548.0494995117188, r=480.59470078842554, b=579.7995932013234, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.973379373550415, cells=[Cell(id=28, text='1.', bbox=BoundingBox(l=139.2480002284392, t=548.9420730309457, r=146.00546023952495, b=557.8815930803046, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for im-', bbox=BoundingBox(l=148.25793024322016, t=548.9420730309457, r=480.5899007884176, b=557.8815930803046, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='age classification. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE', bbox=BoundingBox(l=150.95399024764313, t=559.9010930914551, r=480.59470078842554, b=568.840603140814, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=31, text='Conference on. pp. 3642-3649. IEEE (2012)', bbox=BoundingBox(l=150.95399024764313, t=570.8600731519643, r=318.41046052235896, b=579.7995932013234, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='1. Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for image classification. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. pp. 3642-3649. IEEE (2012)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=12, page_no=9, cluster=Cluster(id=12, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.2350311279297, t=580.685791015625, r=480.658203125, b=623.1235834405338, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9770330190658569, cells=[Cell(id=32, text='2.', bbox=BoundingBox(l=139.2480002284392, t=581.3070932096468, r=146.07829023964445, b=590.2465832590057, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='Cires¸an, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in', bbox=BoundingBox(l=148.35506024337954, t=581.3070932096468, r=480.5899007884176, b=590.2465832590057, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='breast cancer histology images with deep neural networks. In: Medical Image Com-', bbox=BoundingBox(l=150.95399024764313, t=592.2660832701563, r=480.59467078842545, b=601.2055833195151, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=35, text='puting and Computer-Assisted Intervention-MICCAI 2013, pp. 411-418. Springer', bbox=BoundingBox(l=150.95399024764313, t=603.2250833306656, r=480.59479078842566, b=612.1645833800244, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='(2013)', bbox=BoundingBox(l=150.95399024764313, t=614.1840833911749, r=174.85841028685883, b=623.1235834405338, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2. Cires¸an, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in breast cancer histology images with deep neural networks. In: Medical Image Computing and Computer-Assisted Intervention-MICCAI 2013, pp. 411-418. Springer (2013)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=13, page_no=9, cluster=Cluster(id=13, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.2299041748047, t=624.0062255859375, r=480.5948507884258, b=666.5886840820312, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9793604612350464, cells=[Cell(id=37, text='3.', bbox=BoundingBox(l=139.2480002284392, t=624.6310834488573, r=146.0001202395162, b=633.5705834982161, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible,', bbox=BoundingBox(l=148.25081024320852, t=624.6310834488573, r=480.5899007884176, b=633.5705834982161, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=39, text='high performance convolutional neural networks for image classification. In: IJCAI', bbox=BoundingBox(l=150.95399024764313, t=635.5900835093667, r=480.5948507884258, b=644.5295835587256, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='Proceedings-International Joint Conference on Artificial Intelligence. vol. 22, p. 1237', bbox=BoundingBox(l=150.95399024764313, t=646.5490835698761, r=480.59467078842545, b=655.4885836192349, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='(2011)', bbox=BoundingBox(l=150.95399024764313, t=657.5080836303854, r=174.85841028685883, b=666.4475836797442, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='3. Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible, high performance convolutional neural networks for image classification. In: IJCAI Proceedings-International Joint Conference on Artificial Intelligence. vol. 22, p. 1237 (2011)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=2, page_no=10, cluster=Cluster(id=2, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.15142822265625, t=119.36505126953125, r=480.60302734375, b=162.19952089557341, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.970146894454956, cells=[Cell(id=2, text='4.', bbox=BoundingBox(l=139.24802022843926, t=120.3840906646924, r=145.95924023944914, b=129.32354071405098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=3, text='Cires¸an, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural', bbox=BoundingBox(l=148.19632024311912, t=120.3840906646924, r=480.58994078841766, b=129.32354071405098, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=4, text='network committees for handwritten character classification. In: Document Analysis', bbox=BoundingBox(l=150.95401024764317, t=131.34307072520176, r=480.5947307884256, b=140.2825307745602, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=5, text='and Recognition (ICDAR), 2011 International Conference on. pp. 1135-1139. IEEE', bbox=BoundingBox(l=150.95401024764317, t=142.30206078571098, r=480.5947307884256, b=151.24151083506956, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=6, text='(2011)', bbox=BoundingBox(l=150.95401024764317, t=153.26007084621483, r=174.85843028685886, b=162.19952089557341, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='4. Cires¸an, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural network committees for handwritten character classification. In: Document Analysis and Recognition (ICDAR), 2011 International Conference on. pp. 1135-1139. IEEE (2011)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=3, page_no=10, cluster=Cluster(id=3, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.33560180664062, t=163.44866943359375, r=480.71539306640625, b=184.11749101659188, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9617682695388794, cells=[Cell(id=7, text='5.', bbox=BoundingBox(l=139.24802022843926, t=164.21905090672408, r=145.96364023945637, b=173.15850095608266, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=8, text='Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural net-', bbox=BoundingBox(l=148.20218024312874, t=164.21905090672408, r=480.58978078841744, b=173.15850095608266, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=9, text='worksa review. Pattern recognition 35(10), 2279-2301 (2002)', bbox=BoundingBox(l=150.95401024764317, t=175.1780309672332, r=386.3400006337987, b=184.11749101659188, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='5. Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural networksa review. Pattern recognition 35(10), 2279-2301 (2002)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=4, page_no=10, cluster=Cluster(id=4, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.4720458984375, t=185.40130615234375, r=480.6998291015625, b=227.95343125862905, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9742252230644226, cells=[Cell(id=10, text='6.', bbox=BoundingBox(l=139.24802022843926, t=186.13702102774266, r=146.00636023952646, b=195.07647107710125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=11, text='Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware', bbox=BoundingBox(l=148.25916024322223, t=186.13702102774266, r=480.58987078841767, b=195.07647107710125, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=12, text='accelerated convolutional neural networks for synthetic vision systems. In: Circuits', bbox=BoundingBox(l=150.95401024764317, t=197.0960010882519, r=480.59467078842545, b=206.03546113761047, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=13, text='and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp.', bbox=BoundingBox(l=150.95401024764317, t=208.05499114876113, r=480.5948507884258, b=216.99444119811972, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=14, text='257-260. IEEE (2010)', bbox=BoundingBox(l=150.95401024764317, t=219.0139712092705, r=232.4227303812943, b=227.95343125862905, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='6. Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware accelerated convolutional neural networks for synthetic vision systems. In: Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp. 257-260. IEEE (2010)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=5, page_no=10, cluster=Cluster(id=5, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.4396514892578, t=229.38987731933594, r=480.5898107884175, b=249.87139137964755, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9618682861328125, cells=[Cell(id=15, text='7.', bbox=BoundingBox(l=139.24802022843926, t=229.97296126977972, r=145.9334702394069, b=238.9124113191383, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=16, text='Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum', bbox=BoundingBox(l=148.16196024306274, t=229.97296126977972, r=480.5898107884175, b=238.9124113191383, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=17, text='9(1), 926 (2010)', bbox=BoundingBox(l=150.95401024764317, t=240.93194133028896, r=209.97086034446153, b=249.87139137964755, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='7. Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum 9(1), 926 (2010)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=6, page_no=10, cluster=Cluster(id=6, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.49429321289062, t=251.51890563964844, r=480.5947307884256, b=282.74737156117, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.974453330039978, cells=[Cell(id=18, text='8.', bbox=BoundingBox(l=139.24802022843926, t=251.8909313907983, r=146.05434023960515, b=260.8303814401569, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=19, text='Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Im-', bbox=BoundingBox(l=148.3231202433271, t=251.8909313907983, r=480.5898107884175, b=260.8303814401569, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=20, text='proving neural networks by preventing co-adaptation of feature detectors. arXiv', bbox=BoundingBox(l=150.95401024764317, t=262.84991145130755, r=480.5947307884256, b=271.78936150066613, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=21, text='preprint arXiv:1207.0580 (2012)', bbox=BoundingBox(l=150.95401024764317, t=273.8079215118115, r=274.11649044969374, b=282.74737156117, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='8. Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 (2012)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=7, page_no=10, cluster=Cluster(id=7, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=138.10630798339844, t=284.2738342285156, r=480.59470078842554, b=315.62432174269793, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9755162000656128, cells=[Cell(id=22, text='9.', bbox=BoundingBox(l=139.24802022843926, t=284.76690157232076, r=146.0757602396403, b=293.70636162167943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=23, text='Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action', bbox=BoundingBox(l=148.35168024337398, t=284.76690157232076, r=480.5898107884175, b=293.70636162167943, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=24, text='recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1),', bbox=BoundingBox(l=150.95401024764317, t=295.72589163283, r=480.59470078842554, b=304.66534168218857, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=25, text='221-231 (2013)', bbox=BoundingBox(l=150.95401024764317, t=306.68487169333923, r=208.4824203420197, b=315.62432174269793, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='9. Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1), 221-231 (2013)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=8, page_no=10, cluster=Cluster(id=8, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.7255096435547, t=316.7909851074219, r=480.77935791015625, b=359.46029198473514, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9749436378479004, cells=[Cell(id=26, text='10.', bbox=BoundingBox(l=134.76501022108476, t=317.6438617538487, r=146.3162402400348, b=326.58331180320715, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=27, text='Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-', bbox=BoundingBox(l=148.62648024382477, t=317.6438617538487, r=480.590120788418, b=326.58331180320715, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=28, text='scale video classification with convolutional neural networks. In: Computer Vision', bbox=BoundingBox(l=150.95401024764317, t=328.6028418143578, r=480.59476078842556, b=337.54232186371655, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=29, text='and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725-1732. IEEE', bbox=BoundingBox(l=150.95401024764317, t=339.56179187486697, r=480.5947307884256, b=348.5013119242259, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=30, text='(2014)', bbox=BoundingBox(l=150.95401024764317, t=350.5207819353762, r=174.85843028685886, b=359.46029198473514, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='10. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Largescale video classification with convolutional neural networks. In: Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725-1732. IEEE (2014)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=9, page_no=10, cluster=Cluster(id=9, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.47573852539062, t=360.74285888671875, r=480.59479078842566, b=392.3362721662576, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9674268960952759, cells=[Cell(id=31, text='11.', bbox=BoundingBox(l=134.76501022108476, t=361.4797619958855, r=145.9576402394465, b=370.4192820452444, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=32, text='Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convo-', bbox=BoundingBox(l=148.19617024311887, t=361.4797619958855, r=480.5902407884182, b=370.4192820452444, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=33, text='lutional neural networks. In: Advances in neural information processing systems.', bbox=BoundingBox(l=150.95401024764317, t=372.4387520563948, r=480.59479078842566, b=381.37826210575366, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=34, text='pp. 1097-1105 (2012)', bbox=BoundingBox(l=150.95401024764317, t=383.39675211689865, r=232.70963038176492, b=392.3362721662576, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='11. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems. pp. 1097-1105 (2012)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=10, page_no=10, cluster=Cluster(id=10, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.62811279296875, t=393.55804443359375, r=480.59467078842545, b=425.21322234778535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9672126173973083, cells=[Cell(id=35, text='12.', bbox=BoundingBox(l=134.76501022108476, t=394.355742177408, r=145.86594023929607, b=403.2952522267668, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=36, text='LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel,', bbox=BoundingBox(l=148.08614024293834, t=394.355742177408, r=480.5900307884179, b=403.2952522267668, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=37, text='L.D.: Backpropagation applied to handwritten zip code recognition. Neural compu-', bbox=BoundingBox(l=150.95401024764317, t=405.3147222379172, r=480.59467078842545, b=414.25424228727616, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=38, text='tation 1(4), 541-551 (1989)', bbox=BoundingBox(l=150.95401024764317, t=416.2737122984265, r=253.19788041537635, b=425.21322234778535, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='12. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural computation 1(4), 541-551 (1989)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=11, page_no=10, cluster=Cluster(id=11, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.47447204589844, t=426.6608581542969, r=480.59009078841797, b=447.1311924688039, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9590675234794617, cells=[Cell(id=39, text='13.', bbox=BoundingBox(l=134.76501022108476, t=427.2326923589357, r=145.78894023916976, b=436.1722124082947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=40, text='LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to doc-', bbox=BoundingBox(l=147.99373024278677, t=427.2326923589357, r=480.59009078841797, b=436.1722124082947, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=41, text='ument recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998)', bbox=BoundingBox(l=150.95401024764317, t=438.1916824194451, r=420.4302706897245, b=447.1311924688039, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='13. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=12, page_no=10, cluster=Cluster(id=12, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.4318084716797, t=448.2698059082031, r=480.5900607884179, b=469.0491625898225, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9524788856506348, cells=[Cell(id=42, text='14.', bbox=BoundingBox(l=134.76501022108476, t=449.1506624799543, r=146.421830240208, b=458.0901725293132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=43, text='Nebauer, C.: Evaluation of convolutional neural networks for visual recognition.', bbox=BoundingBox(l=148.75319024403268, t=449.1506624799543, r=480.5900607884179, b=458.0901725293132, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=44, text='Neural Networks, IEEE Transactions on 9(4), 685-696 (1998)', bbox=BoundingBox(l=150.95401024764317, t=460.1096525404636, r=387.7297706360787, b=469.0491625898225, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='14. Nebauer, C.: Evaluation of convolutional neural networks for visual recognition. Neural Networks, IEEE Transactions on 9(4), 685-696 (1998)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=13, page_no=10, cluster=Cluster(id=13, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.57110595703125, t=470.0508117675781, r=480.6988525390625, b=490.96713271084104, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9601881504058838, cells=[Cell(id=45, text='15.', bbox=BoundingBox(l=134.76501022108476, t=471.0686326009729, r=146.22498023988507, b=480.00814265033176, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=46, text='Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural net-', bbox=BoundingBox(l=148.51697024364515, t=471.0686326009729, r=480.590120788418, b=480.00814265033176, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=47, text='works applied to visual document analysis. In: null. p. 958. IEEE (2003)', bbox=BoundingBox(l=150.95401024764317, t=482.0276126614821, r=431.96100070864094, b=490.96713271084104, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='15. Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural networks applied to visual document analysis. In: null. p. 958. IEEE (2003)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=14, page_no=10, cluster=Cluster(id=14, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.7103729248047, t=492.13946533203125, r=480.5900607884179, b=512.8841228318543, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9577903747558594, cells=[Cell(id=48, text='16.', bbox=BoundingBox(l=134.76501022108476, t=492.98562272198603, r=145.95445023944131, b=501.9251427713449, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=49, text='Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of', bbox=BoundingBox(l=148.1923402431126, t=492.98562272198603, r=480.5900607884179, b=501.9251427713449, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=50, text='Toronto (2013)', bbox=BoundingBox(l=150.95401024764317, t=503.9446127824953, r=207.980320341196, b=512.8841228318543, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='16. Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of Toronto (2013)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=15, page_no=10, cluster=Cluster(id=15, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.64859008789062, t=514.1981811523438, r=480.5948207884257, b=545.761073013382, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9668182134628296, cells=[Cell(id=51, text='17.', bbox=BoundingBox(l=134.76501022108476, t=514.9035928430046, r=145.9241202393915, b=523.8431028923634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=52, text='Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with con-', bbox=BoundingBox(l=148.15594024305284, t=514.9035928430046, r=480.590150788418, b=523.8431028923634, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=53, text='volutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings.', bbox=BoundingBox(l=150.95401024764317, t=525.8625729035139, r=480.5948207884257, b=534.8020929528727, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=54, text='IEEE. pp. 224-229. IEEE (2005)', bbox=BoundingBox(l=150.95401024764317, t=536.8215629640231, r=271.6238104456045, b=545.761073013382, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='17. Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with convolutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings. IEEE. pp. 224-229. IEEE (2005)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=16, page_no=10, cluster=Cluster(id=16, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.76501022108476, t=546.84033203125, r=480.5900307884179, b=567.6790431344007, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9606278538703918, cells=[Cell(id=55, text='18.', bbox=BoundingBox(l=134.76501022108476, t=547.7805430245323, r=146.3605302401075, b=556.7200630738914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=56, text='Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In:', bbox=BoundingBox(l=148.67964024391202, t=547.7805430245323, r=480.5900307884179, b=556.7200630738914, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=57, text='Advances in Neural Information Processing Systems. pp. 2553-2561 (2013)', bbox=BoundingBox(l=150.95401024764317, t=558.7395330850417, r=445.8409707314113, b=567.6790431344007, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='18. Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In: Advances in Neural Information Processing Systems. pp. 2553-2561 (2013)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=17, page_no=10, cluster=Cluster(id=17, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.6785888671875, t=568.6400146484375, r=480.60009765625, b=600.720947265625, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9559718370437622, cells=[Cell(id=58, text='19.', bbox=BoundingBox(l=134.76501022108476, t=569.6985131455509, r=146.7078102406772, b=578.6380331949099, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=59, text='Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks', bbox=BoundingBox(l=149.09637024459568, t=569.6985131455509, r=480.590150788418, b=578.6380331949099, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=60, text='(siconnets) and their application of face detection. In: Neural Networks, 2003. Pro-', bbox=BoundingBox(l=150.95401024764317, t=580.6575332060604, r=480.59479078842566, b=589.5970132554191, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=61, text='ceedings of the International Joint Conference on. vol. 3, pp. 2157-2162. IEEE (2003)', bbox=BoundingBox(l=150.95401024764317, t=591.6165132665697, r=480.59479078842566, b=600.5560133159285, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='19. Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks (siconnets) and their application of face detection. In: Neural Networks, 2003. Proceedings of the International Joint Conference on. vol. 3, pp. 2157-2162. IEEE (2003)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=18, page_no=10, cluster=Cluster(id=18, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.3703155517578, t=601.6026611328125, r=480.59009078841797, b=622.4730234369417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.958541750907898, cells=[Cell(id=62, text='20.', bbox=BoundingBox(l=134.76501022108476, t=602.5745233270735, r=146.06206023961784, b=611.5140233764323, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=63, text='Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional', bbox=BoundingBox(l=148.32147024332446, t=602.5745233270735, r=480.59009078841797, b=611.5140233764323, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=64, text='neural networks. arXiv preprint arXiv:1301.3557 (2013)', bbox=BoundingBox(l=150.95401024764317, t=613.5335233875829, r=367.45676060282034, b=622.4730234369417, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='20. Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional neural networks. arXiv preprint arXiv:1301.3557 (2013)'), TextElement(label=<DocItemLabel.LIST_ITEM: 'list_item'>, id=19, page_no=10, cluster=Cluster(id=19, label=<DocItemLabel.LIST_ITEM: 'list_item'>, bbox=BoundingBox(l=134.26535034179688, t=623.5272216796875, r=480.590150788418, b=644.814208984375, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9669718146324158, cells=[Cell(id=65, text='21.', bbox=BoundingBox(l=134.76501022108476, t=624.4925234480922, r=146.0039102395224, b=633.4320234974512, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=66, text='Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In:', bbox=BoundingBox(l=148.25168024320993, t=624.4925234480922, r=480.590150788418, b=633.4320234974512, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>)), Cell(id=67, text='Computer Vision-ECCV 2014, pp. 818-833. Springer (2014)', bbox=BoundingBox(l=150.95401024764317, t=635.4515235086017, r=384.4032306306214, b=644.3910235579605, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='21. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In: Computer Vision-ECCV 2014, pp. 818-833. Springer (2014)')], headers=[TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=11, page_no=0, cluster=Cluster(id=11, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=17.202451705932617, t=264.95001146290315, r=36.33979405961618, b=604.8900433398585, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8815839290618896, cells=[Cell(id=45, text='arXiv:1511.08458v2 [cs.NE] 2 Dec 2015', bbox=BoundingBox(l=18.3402140300875, t=264.95001146290315, r=36.33979405961618, b=604.8900433398585, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='arXiv:1511.08458v2 [cs.NE] 2 Dec 2015'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=1, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=133.6807861328125, t=93.64701080322266, r=139.58908081054688, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7250283360481262, cells=[Cell(id=0, text='2', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='2'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=1, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=167.17018127441406, t=93.5260238647461, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.6552881002426147, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea et al.'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=2, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=255.9642333984375, t=93.53244018554688, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9313411116600037, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=2, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=475.16705322265625, t=93.53336334228516, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.873039722442627, cells=[Cell(id=1, text='3', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='3'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=3, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=133.68788146972656, t=93.82508850097656, r=139.39495849609375, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.6813156604766846, cells=[Cell(id=0, text='4', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='4'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=4, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=255.9207763671875, t=93.53521728515625, r=447.56707763671875, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9285128712654114, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=4, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=475.2783203125, t=93.44747924804688, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.873298704624176, cells=[Cell(id=1, text='5', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='5'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=5, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=133.78485107421875, t=94.09490966796875, r=139.71607971191406, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7533239722251892, cells=[Cell(id=0, text='6', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='6'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=5, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=167.21620178222656, t=93.54159545898438, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7271904349327087, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea et al.'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=6, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=255.95968627929688, t=93.53328704833984, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9278361201286316, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=6, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=475.1293029785156, t=93.63327026367188, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8769420981407166, cells=[Cell(id=1, text='7', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='7'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=7, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=133.78009033203125, t=93.9936294555664, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7759202718734741, cells=[Cell(id=0, text='8', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=139.24820022843954, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='8'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=7, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=167.4324493408203, t=93.5379638671875, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.7988561391830444, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea et al.'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=8, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=255.96002197265625, t=93.55046844482422, r=447.56597900390625, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9279627799987793, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=8, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=474.8158874511719, t=93.79475402832031, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8714832067489624, cells=[Cell(id=1, text='9', bbox=BoundingBox(l=476.10895078106654, t=94.48107052167063, r=480.5921607884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='9'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=9, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=134.48631286621094, t=93.9991683959961, r=143.73849487304688, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8442774415016174, cells=[Cell(id=0, text='10', bbox=BoundingBox(l=134.76500022108476, t=94.48107052167063, r=143.73140023579433, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='10'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=9, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=167.15977478027344, t=93.5068588256836, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.8268627524375916, cells=[Cell(id=1, text='Keiron O’Shea et al.', bbox=BoundingBox(l=167.8151602753042, t=94.48107052167063, r=246.24426040396878, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Keiron O’Shea et al.'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=0, page_no=10, cluster=Cluster(id=0, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=256.0866394042969, t=93.61783599853516, r=447.5473937988281, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.9234234094619751, cells=[Cell(id=0, text='Introduction to Convolutional Neural Networks', bbox=BoundingBox(l=256.4590104207263, t=94.48107052167063, r=447.5419907342018, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='Introduction to Convolutional Neural Networks'), TextElement(label=<DocItemLabel.PAGE_HEADER: 'page_header'>, id=1, page_no=10, cluster=Cluster(id=1, label=<DocItemLabel.PAGE_HEADER: 'page_header'>, bbox=BoundingBox(l=471.0235290527344, t=93.66058349609375, r=480.5921307884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>), confidence=0.889333963394165, cells=[Cell(id=1, text='11', bbox=BoundingBox(l=471.6257307737117, t=94.48107052167063, r=480.5921307884213, b=103.4205305710293, coord_origin=<CoordOrigin.TOPLEFT: 'TOPLEFT'>))]), text='11')]) timings={} document=DoclingDocument(schema_name='DoclingDocument', version='1.0.0', name='1511.08458v2', origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5687491352442750095, filename='1511.08458v2.pdf', uri=None), furniture=GroupItem(self_ref='#/furniture', parent=None, children=[], name='_root_', label=<GroupLabel.UNSPECIFIED: 'unspecified'>), body=GroupItem(self_ref='#/body', parent=None, children=[RefItem(cref='#/texts/0'), RefItem(cref='#/texts/1'), RefItem(cref='#/texts/2'), RefItem(cref='#/groups/0'), RefItem(cref='#/texts/5'), RefItem(cref='#/texts/6'), RefItem(cref='#/texts/7'), RefItem(cref='#/texts/8'), RefItem(cref='#/texts/9'), RefItem(cref='#/texts/10'), RefItem(cref='#/texts/11'), RefItem(cref='#/texts/12'), RefItem(cref='#/texts/13'), RefItem(cref='#/texts/14'), RefItem(cref='#/pictures/0'), RefItem(cref='#/texts/15'), RefItem(cref='#/texts/16'), RefItem(cref='#/texts/17'), RefItem(cref='#/texts/18'), RefItem(cref='#/texts/19'), RefItem(cref='#/texts/20'), RefItem(cref='#/texts/21'), RefItem(cref='#/texts/22'), RefItem(cref='#/texts/23'), RefItem(cref='#/texts/24'), RefItem(cref='#/texts/25'), RefItem(cref='#/texts/26'), RefItem(cref='#/texts/27'), RefItem(cref='#/texts/28'), RefItem(cref='#/texts/29'), RefItem(cref='#/texts/30'), RefItem(cref='#/texts/31'), RefItem(cref='#/texts/32'), RefItem(cref='#/texts/33'), RefItem(cref='#/texts/34'), RefItem(cref='#/texts/35'), RefItem(cref='#/texts/36'), RefItem(cref='#/pictures/1'), RefItem(cref='#/texts/37'), RefItem(cref='#/groups/1'), RefItem(cref='#/texts/40'), RefItem(cref='#/texts/41'), RefItem(cref='#/groups/2'), RefItem(cref='#/texts/45'), RefItem(cref='#/texts/46'), RefItem(cref='#/pictures/2'), RefItem(cref='#/texts/47'), RefItem(cref='#/texts/48'), RefItem(cref='#/texts/49'), RefItem(cref='#/texts/50'), RefItem(cref='#/texts/51'), RefItem(cref='#/texts/52'), RefItem(cref='#/texts/53'), RefItem(cref='#/texts/54'), RefItem(cref='#/pictures/3'), RefItem(cref='#/texts/55'), RefItem(cref='#/texts/56'), RefItem(cref='#/texts/57'), RefItem(cref='#/texts/58'), RefItem(cref='#/texts/59'), RefItem(cref='#/texts/60'), RefItem(cref='#/texts/61'), RefItem(cref='#/texts/62'), RefItem(cref='#/texts/63'), RefItem(cref='#/texts/64'), RefItem(cref='#/texts/65'), RefItem(cref='#/texts/66'), RefItem(cref='#/texts/67'), RefItem(cref='#/texts/68'), RefItem(cref='#/texts/69'), RefItem(cref='#/texts/70'), RefItem(cref='#/texts/71'), RefItem(cref='#/texts/72'), RefItem(cref='#/texts/73'), RefItem(cref='#/texts/74'), RefItem(cref='#/texts/75'), RefItem(cref='#/texts/76'), RefItem(cref='#/texts/77'), RefItem(cref='#/texts/78'), RefItem(cref='#/texts/79'), RefItem(cref='#/texts/80'), RefItem(cref='#/texts/81'), RefItem(cref='#/texts/82'), RefItem(cref='#/texts/83'), RefItem(cref='#/texts/84'), RefItem(cref='#/pictures/4'), RefItem(cref='#/texts/85'), RefItem(cref='#/texts/86'), RefItem(cref='#/texts/87'), RefItem(cref='#/texts/88'), RefItem(cref='#/texts/89'), RefItem(cref='#/texts/90'), RefItem(cref='#/texts/91'), RefItem(cref='#/texts/92'), RefItem(cref='#/texts/93'), RefItem(cref='#/texts/94'), RefItem(cref='#/texts/95'), RefItem(cref='#/texts/96'), RefItem(cref='#/texts/97'), RefItem(cref='#/texts/98'), RefItem(cref='#/texts/99'), RefItem(cref='#/groups/3'), RefItem(cref='#/texts/103'), RefItem(cref='#/texts/104'), RefItem(cref='#/groups/4')], name='_root_', label=<GroupLabel.UNSPECIFIED: 'unspecified'>), groups=[GroupItem(self_ref='#/groups/0', parent=RefItem(cref='#/body'), children=[RefItem(cref='#/texts/3'), RefItem(cref='#/texts/4')], name='list', label=<GroupLabel.LIST: 'list'>), GroupItem(self_ref='#/groups/1', parent=RefItem(cref='#/body'), children=[RefItem(cref='#/texts/38'), RefItem(cref='#/texts/39')], name='list', label=<GroupLabel.LIST: 'list'>), GroupItem(self_ref='#/groups/2', parent=RefItem(cref='#/body'), children=[RefItem(cref='#/texts/42'), RefItem(cref='#/texts/43'), RefItem(cref='#/texts/44')], name='list', label=<GroupLabel.LIST: 'list'>), GroupItem(self_ref='#/groups/3', parent=RefItem(cref='#/body'), children=[RefItem(cref='#/texts/100'), RefItem(cref='#/texts/101'), RefItem(cref='#/texts/102')], name='list', label=<GroupLabel.LIST: 'list'>), GroupItem(self_ref='#/groups/4', parent=RefItem(cref='#/body'), children=[RefItem(cref='#/texts/105'), RefItem(cref='#/texts/106'), RefItem(cref='#/texts/107'), RefItem(cref='#/texts/108'), RefItem(cref='#/texts/109'), RefItem(cref='#/texts/110'), RefItem(cref='#/texts/111'), RefItem(cref='#/texts/112'), RefItem(cref='#/texts/113'), RefItem(cref='#/texts/114'), RefItem(cref='#/texts/115'), RefItem(cref='#/texts/116'), RefItem(cref='#/texts/117'), RefItem(cref='#/texts/118'), RefItem(cref='#/texts/119'), RefItem(cref='#/texts/120'), RefItem(cref='#/texts/121'), RefItem(cref='#/texts/122')], name='list', label=<GroupLabel.LIST: 'list'>)], texts=[TextItem(self_ref='#/texts/0', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PAGE_HEADER: 'page_header'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=17.202451705932617, t=576.9400024414062, r=36.33979415893555, b=236.99996948242188, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 37))], orig='arXiv:1511.08458v2 [cs.NE] 2 Dec 2015', text='arXiv:1511.08458v2 [cs.NE] 2 Dec 2015'), SectionHeaderItem(self_ref='#/texts/1', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.SECTION_HEADER: 'section_header'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=137.64463806152344, t=725.666259765625, r=476.8287353515625, b=710.9338989257812, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 48))], orig='An Introduction to Convolutional Neural Networks', text='An Introduction to Convolutional Neural Networks', level=1), TextItem(self_ref='#/texts/2', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=235.75559997558594, t=681.5888061523438, r=378.7718811035156, b=670.2728271484375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 31))], orig=\"Keiron O'Shea 1 and Ryan Nash 2\", text=\"Keiron O'Shea 1 and Ryan Nash 2\"), ListItem(self_ref='#/texts/3', parent=RefItem(cref='#/groups/0'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=140.86863708496094, t=660.33056640625, r=474.03369140625, b=639.7752075195312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 94))], orig='1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB keo7@aber.ac.uk', text='1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB keo7@aber.ac.uk', enumerated=False, marker='-'), ListItem(self_ref='#/texts/4', parent=RefItem(cref='#/groups/0'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=139.36328125, t=638.429931640625, r=475.5403747558594, b=616.848388671875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 83))], orig='2 School of Computing and Communications, Lancaster University, Lancashire, LA1 4YW', text='2 School of Computing and Communications, Lancaster University, Lancashire, LA1 4YW', enumerated=False, marker='-'), TextItem(self_ref='#/texts/5', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=245.39329528808594, t=615.0885009765625, r=369.54632568359375, b=607.099853515625, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 23))], orig='nashrd@live.lancs.ac.uk', text='nashrd@live.lancs.ac.uk'), TextItem(self_ref='#/texts/6', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=162.13299560546875, t=588.0892944335938, r=452.33795166015625, b=490.3534851074219, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 595))], orig='Abstract. The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs.', text='Abstract. The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs.'), TextItem(self_ref='#/texts/7', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=162.31292724609375, t=482.57501220703125, r=452.2415466308594, b=439.4319152832031, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 280))], orig='This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.', text='This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.'), TextItem(self_ref='#/texts/8', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=162.44024658203125, t=420.5909423828125, r=452.410888671875, b=399.15863037109375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 91))], orig='Keywords: Pattern recognition, artificial neural networks, machine learning, image analysis', text='Keywords: Pattern recognition, artificial neural networks, machine learning, image analysis'), SectionHeaderItem(self_ref='#/texts/9', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.SECTION_HEADER: 'section_header'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=134.2177734375, t=370.3959655761719, r=221.76300048828125, b=358.1449279785156, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 14))], orig='1 Introduction', text='1 Introduction', level=1), TextItem(self_ref='#/texts/10', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=133.661865234375, t=336.8586730957031, r=480.72076416015625, b=265.77435302734375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 399))], orig='Artificial Neural Networks (ANNs) are computational processing systems of which are heavily inspired by way biological nervous systems (such as the human brain) operate. ANNs are mainly comprised of a high number of interconnected computational nodes (referred to as neurons), of which work entwine in a distributed fashion to collectively learn from the input in order to optimise its final output.', text='Artificial Neural Networks (ANNs) are computational processing systems of which are heavily inspired by way biological nervous systems (such as the human brain) operate. ANNs are mainly comprised of a high number of interconnected computational nodes (referred to as neurons), of which work entwine in a distributed fashion to collectively learn from the input in order to optimise its final output.'), TextItem(self_ref='#/texts/11', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=133.71791076660156, t=257.9127197265625, r=480.8759460449219, b=174.66064453125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 508))], orig='The basic structure of a ANN can be modelled as shown in Figure 1. We would load the input, usually in the form of a multidimensional vector to the input layer of which will distribute it to the hidden layers. The hidden layers will then make decisions from the previous layer and weigh up how a stochastic change within itself detriments or improves the final output, and this is referred to as the process of learning. Having multiple hidden layers stacked upon each-other is commonly called deep learning.', text='The basic structure of a ANN can be modelled as shown in Figure 1. We would load the input, usually in the form of a multidimensional vector to the input layer of which will distribute it to the hidden layers. The hidden layers will then make decisions from the previous layer and weigh up how a stochastic change within itself detriments or improves the final output, and this is referred to as the process of learning. Having multiple hidden layers stacked upon each-other is commonly called deep learning.'), TextItem(self_ref='#/texts/12', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PAGE_HEADER: 'page_header'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=133.6807861328125, t=748.2429809570312, r=139.58908081054688, b=738.469482421875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 1))], orig='2', text='2'), TextItem(self_ref='#/texts/13', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PAGE_HEADER: 'page_header'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=167.17018127441406, t=748.364013671875, r=246.2442626953125, b=738.469482421875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 20))], orig='Keiron O’Shea et al.', text='Keiron O’Shea et al.'), TextItem(self_ref='#/texts/14', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.CAPTION: 'caption'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=133.7849884033203, t=563.7962646484375, r=480.5867919921875, b=505.6305236816406, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 335))], orig='Fig. 1: A simple three layered feedforward neural network (FNN), comprised of a input layer, a hidden layer and an output layer. This structure is the basis of a number of common ANN architectures, included but not limited to Feedforward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and Recurrent Neural Networks (RNNs).', text='Fig. 1: A simple three layered feedforward neural network (FNN), comprised of a input layer, a hidden layer and an output layer. This structure is the basis of a number of common ANN architectures, included but not limited to Feedforward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and Recurrent Neural Networks (RNNs).'), TextItem(self_ref='#/texts/15', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=133.74380493164062, t=476.2651062011719, r=480.59271240234375, b=394.2167663574219, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 475))], orig='The two key learning paradigms in image processing tasks are supervised and unsupervised learning. Supervised learning is learning through pre-labelled inputs, which act as targets. For each training example there will be a set of input values (vectors) and one or more associated designated output values. The goal of this form of training is to reduce the models overall classification error, through correct calculation of the output value of training example by training.', text='The two key learning paradigms in image processing tasks are supervised and unsupervised learning. Supervised learning is learning through pre-labelled inputs, which act as targets. For each training example there will be a set of input values (vectors) and one or more associated designated output values. The goal of this form of training is to reduce the models overall classification error, through correct calculation of the output value of training example by training.'), TextItem(self_ref='#/texts/16', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=133.91444396972656, t=383.7518615722656, r=480.5906982421875, b=324.7509765625, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 338))], orig='Unsupervised learning differs in that the training set does not include any labels. Success is usually determined by whether the network is able to reduce or increase an associated cost function. However, it is important to note that most image-focused pattern-recognition tasks usually depend on classification using supervised learning.', text='Unsupervised learning differs in that the training set does not include any labels. Success is usually determined by whether the network is able to reduce or increase an associated cost function. However, it is important to note that most image-focused pattern-recognition tasks usually depend on classification using supervised learning.'), TextItem(self_ref='#/texts/17', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=133.6765899658203, t=314.57537841796875, r=480.7476806640625, b=220.25634765625, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 623))], orig='Convolutional Neural Networks (CNNs) are analogous to traditional ANNs in that they are comprised of neurons that self-optimise through learning. Each neuron will still receive an input and perform a operation (such as a scalar product followed by a non-linear function) - the basis of countless ANNs. From the input raw image vectors to the final output of the class score, the entire of the network will still express a single perceptive score function (the weight). The last layer will contain loss functions associated with the classes, and all of the regular tips and tricks developed for traditional ANNs still apply.', text='Convolutional Neural Networks (CNNs) are analogous to traditional ANNs in that they are comprised of neurons that self-optimise through learning. Each neuron will still receive an input and perform a operation (such as a scalar product followed by a non-linear function) - the basis of countless ANNs. From the input raw image vectors to the final output of the class score, the entire of the network will still express a single perceptive score function (the weight). The last layer will contain loss functions associated with the classes, and all of the regular tips and tricks developed for traditional ANNs still apply.'), TextItem(self_ref='#/texts/18', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=133.68446350097656, t=209.895751953125, r=480.58685302734375, b=175.1072998046875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 235))], orig='The only notable difference between CNNs and traditional ANNs is that CNNs are primarily used in the field of pattern recognition within images. This allows us to encode image-specific features into the architecture, making the network', text='The only notable difference between CNNs and traditional ANNs is that CNNs are primarily used in the field of pattern recognition within images. This allows us to encode image-specific features into the architecture, making the network'), TextItem(self_ref='#/texts/19', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PAGE_HEADER: 'page_header'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=255.9642333984375, t=748.3575439453125, r=447.5419921875, b=738.469482421875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 45))], orig='Introduction to Convolutional Neural Networks', text='Introduction to Convolutional Neural Networks'), TextItem(self_ref='#/texts/20', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PAGE_HEADER: 'page_header'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=475.16705322265625, t=748.3566284179688, r=480.5921630859375, b=738.469482421875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 1))], orig='3', text='3'), TextItem(self_ref='#/texts/21', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=134.2552032470703, t=723.0224609375, r=480.5867004394531, b=700.3305053710938, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 106))], orig='more suited for image-focused tasks - whilst further reducing the parameters required to set up the model.', text='more suited for image-focused tasks - whilst further reducing the parameters required to set up the model.'), TextItem(self_ref='#/texts/22', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=133.69602966308594, t=691.5694580078125, r=481.4434814453125, b=597.1464233398438, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 573))], orig='One of the largest limitations of traditional forms of ANN is that they tend to struggle with the computational complexity required to compute image data. Common machine learning benchmarking datasets such as the MNIST database of handwritten digits are suitable for most forms of ANN, due to its relatively small image dimensionality of just 28 × 28 . With this dataset a single neuron in the first hidden layer will contain 784 weights ( 28 × 28 × 1 where 1 bare in mind that MNIST is normalised to just black and white values), which is manageable for most forms of ANN.', text='One of the largest limitations of traditional forms of ANN is that they tend to struggle with the computational complexity required to compute image data. Common machine learning benchmarking datasets such as the MNIST database of handwritten digits are suitable for most forms of ANN, due to its relatively small image dimensionality of just 28 × 28 . With this dataset a single neuron in the first hidden layer will contain 784 weights ( 28 × 28 × 1 where 1 bare in mind that MNIST is normalised to just black and white values), which is manageable for most forms of ANN.'), TextItem(self_ref='#/texts/23', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=133.93231201171875, t=588.4321899414062, r=480.5915222167969, b=529.69775390625, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 398))], orig='If you consider a more substantial coloured image input of 64 × 64 , the number of weights on just a single neuron of the first layer increases substantially to 12 , 288 . Also take into account that to deal with this scale of input, the network will also need to be a lot larger than one used to classify colour-normalised MNIST digits, then you will understand the drawbacks of using such models.', text='If you consider a more substantial coloured image input of 64 × 64 , the number of weights on just a single neuron of the first layer increases substantially to 12 , 288 . Also take into account that to deal with this scale of input, the network will also need to be a lot larger than one used to classify colour-normalised MNIST digits, then you will understand the drawbacks of using such models.'), SectionHeaderItem(self_ref='#/texts/24', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.SECTION_HEADER: 'section_header'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=134.107177734375, t=499.7483825683594, r=207.76548767089844, b=489.11602783203125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 15))], orig='1.1 Overfitting', text='1.1 Overfitting', level=1), TextItem(self_ref='#/texts/25', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=133.87643432617188, t=469.1485290527344, r=480.58685302734375, b=410.846435546875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 332))], orig='But why does it matter? Surely we could just increase the number of hidden layers in our network, and perhaps increase the number of neurons within them? The simple answer to this question is no. This is down to two reasons, one being the simple problem of not having unlimited computational power and time to train these huge ANNs.', text='But why does it matter? Surely we could just increase the number of hidden layers in our network, and perhaps increase the number of neurons within them? The simple answer to this question is no. This is down to two reasons, one being the simple problem of not having unlimited computational power and time to train these huge ANNs.'), TextItem(self_ref='#/texts/26', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=133.83572387695312, t=401.69970703125, r=480.64300537109375, b=319.45880126953125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 512))], orig='The second reason is stopping or reducing the effects of overfitting. Overfitting is basically when a network is unable to learn effectively due to a number of reasons. It is an important concept of most, if not all machine learning algorithms and it is important that every precaution is taken as to reduce its effects. If our models were to exhibit signs of overfitting then we may see a reduced ability to pinpoint generalised features for not only our training dataset, but also our test and prediction sets.', text='The second reason is stopping or reducing the effects of overfitting. Overfitting is basically when a network is unable to learn effectively due to a number of reasons. It is an important concept of most, if not all machine learning algorithms and it is important that every precaution is taken as to reduce its effects. If our models were to exhibit signs of overfitting then we may see a reduced ability to pinpoint generalised features for not only our training dataset, but also our test and prediction sets.'), TextItem(self_ref='#/texts/27', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=133.8554229736328, t=310.45977783203125, r=480.5868835449219, b=276.2095642089844, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 213))], orig='This is the main reason behind reducing the complexity of our ANNs. The less parameters required to train, the less likely the network will overfit - and of course, improve the predictive performance of the model.', text='This is the main reason behind reducing the complexity of our ANNs. The less parameters required to train, the less likely the network will overfit - and of course, improve the predictive performance of the model.'), SectionHeaderItem(self_ref='#/texts/28', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.SECTION_HEADER: 'section_header'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=134.10931396484375, t=245.74603271484375, r=248.64112854003906, b=233.28492736816406, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 18))], orig='2 CNN architecture', text='2 CNN architecture', level=1), TextItem(self_ref='#/texts/29', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=133.93942260742188, t=209.9462890625, r=480.8302001953125, b=175.13427734375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 211))], orig='As noted earlier, CNNs primarily focus on the basis that the input will be comprised of images. This focuses the architecture to be set up in way to best suit the need for dealing with the specific type of data.', text='As noted earlier, CNNs primarily focus on the basis that the input will be comprised of images. This focuses the architecture to be set up in way to best suit the need for dealing with the specific type of data.'), TextItem(self_ref='#/texts/30', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PAGE_HEADER: 'page_header'>, prov=[ProvenanceItem(page_no=4, bbox=BoundingBox(l=133.68788146972656, t=748.06494140625, r=139.39495849609375, b=738.469482421875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 1))], orig='4', text='4'), TextItem(self_ref='#/texts/31', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=4, bbox=BoundingBox(l=167.81515502929688, t=747.408935546875, r=246.2442626953125, b=738.469482421875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 20))], orig=\"Keiron O'Shea et al.\", text=\"Keiron O'Shea et al.\"), TextItem(self_ref='#/texts/32', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=4, bbox=BoundingBox(l=133.76507568359375, t=723.2340698242188, r=480.6532897949219, b=652.2295532226562, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 455))], orig='One of the key differences is that the neurons that the layers within the CNN are comprised of neurons organised into three dimensions, the spatial dimensionality of the input ( height and the width ) and the depth . The depth does not refer to the total number of layers within the ANN, but the third dimension of a activation volume. Unlike standard ANNS, the neurons within any given layer will only connect to a small region of the layer preceding it.', text='One of the key differences is that the neurons that the layers within the CNN are comprised of neurons organised into three dimensions, the spatial dimensionality of the input ( height and the width ) and the depth . The depth does not refer to the total number of layers within the ANN, but the third dimension of a activation volume. Unlike standard ANNS, the neurons within any given layer will only connect to a small region of the layer preceding it.'), TextItem(self_ref='#/texts/33', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=4, bbox=BoundingBox(l=133.84022521972656, t=645.1727905273438, r=480.62689208984375, b=574.0120849609375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 407))], orig=\"In practice this would mean that for the example given earlier, the input 'volume' will have a dimensionality of 64 × 64 × 3 (height, width and depth), leading to a final output layer comprised of a dimensionality of 1 × 1 × n (where n represents the possible number of classes) as we would have condensed the full input dimensionality into a smaller volume of class scores filed across the depth dimension.\", text=\"In practice this would mean that for the example given earlier, the input 'volume' will have a dimensionality of 64 × 64 × 3 (height, width and depth), leading to a final output layer comprised of a dimensionality of 1 × 1 × n (where n represents the possible number of classes) as we would have condensed the full input dimensionality into a smaller volume of class scores filed across the depth dimension.\"), SectionHeaderItem(self_ref='#/texts/34', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.SECTION_HEADER: 'section_header'>, prov=[ProvenanceItem(page_no=4, bbox=BoundingBox(l=134.0306854248047, t=547.65576171875, r=247.65118408203125, b=537.4288330078125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 24))], orig='2.1 Overall architecture', text='2.1 Overall architecture', level=1), TextItem(self_ref='#/texts/35', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=4, bbox=BoundingBox(l=133.74267578125, t=520.6640625, r=480.75616455078125, b=474.25042724609375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 270))], orig='CNNs are comprised of three types of layers. These are convolutional layers, pooling layers and fully-connected layers . When these layers are stacked, a CNN architecture has been formed. A simplified CNN architecture for MNIST classification is illustrated in Figure 2.', text='CNNs are comprised of three types of layers. These are convolutional layers, pooling layers and fully-connected layers . When these layers are stacked, a CNN architecture has been formed. A simplified CNN architecture for MNIST classification is illustrated in Figure 2.'), TextItem(self_ref='#/texts/36', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.CAPTION: 'caption'>, prov=[ProvenanceItem(page_no=4, bbox=BoundingBox(l=165.54881286621094, t=317.5498046875, r=449.3086853027344, b=307.25872802734375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 65))], orig='Fig. 2: An simple CNN architecture, comprised of just five layers', text='Fig. 2: An simple CNN architecture, comprised of just five layers'), TextItem(self_ref='#/texts/37', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=4, bbox=BoundingBox(l=133.9955596923828, t=282.42083740234375, r=480.58673095703125, b=259.68157958984375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 88))], orig='The basic functionality of the example CNN above can be broken down into four key areas.', text='The basic functionality of the example CNN above can be broken down into four key areas.'), ListItem(self_ref='#/texts/38', parent=RefItem(cref='#/groups/1'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=4, bbox=BoundingBox(l=138.5282745361328, t=251.7421875, r=480.59588623046875, b=229.0966796875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 91))], orig='1. As found in other forms of ANN, the input layer will hold the pixel values of the image.', text='1. As found in other forms of ANN, the input layer will hold the pixel values of the image.', enumerated=False, marker='-'), ListItem(self_ref='#/texts/39', parent=RefItem(cref='#/groups/1'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=4, bbox=BoundingBox(l=138.4064483642578, t=221.89215087890625, r=480.59625244140625, b=175.1615753173828, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 299))], orig='2. The convolutional layer will determine the output of neurons of which are connected to local regions of the input through the calculation of the scalar product between their weights and the region connected to the input volume. The rectified linear unit (commonly shortened to ReLu) aims to apply', text='2. The convolutional layer will determine the output of neurons of which are connected to local regions of the input through the calculation of the scalar product between their weights and the region connected to the input volume. The rectified linear unit (commonly shortened to ReLu) aims to apply', enumerated=False, marker='-'), TextItem(self_ref='#/texts/40', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PAGE_HEADER: 'page_header'>, prov=[ProvenanceItem(page_no=5, bbox=BoundingBox(l=255.9207763671875, t=748.3547973632812, r=447.56707763671875, b=738.469482421875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 45))], orig='Introduction to Convolutional Neural Networks', text='Introduction to Convolutional Neural Networks'), TextItem(self_ref='#/texts/41', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PAGE_HEADER: 'page_header'>, prov=[ProvenanceItem(page_no=5, bbox=BoundingBox(l=475.2783203125, t=748.4425048828125, r=480.5921630859375, b=738.469482421875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 1))], orig='5', text='5'), ListItem(self_ref='#/texts/42', parent=RefItem(cref='#/groups/2'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=5, bbox=BoundingBox(l=150.73934936523438, t=723.2891845703125, r=480.59619140625, b=700.3113403320312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 116))], orig=\"an 'elementwise' activation function such as sigmoid to the output of the activation produced by the previous layer.\", text=\"an 'elementwise' activation function such as sigmoid to the output of the activation produced by the previous layer.\", enumerated=False, marker='-'), ListItem(self_ref='#/texts/43', parent=RefItem(cref='#/groups/2'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=5, bbox=BoundingBox(l=138.22279357910156, t=691.7444458007812, r=480.5964050292969, b=657.0234985351562, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 177))], orig='3. The pooling layer will then simply perform downsampling along the spatial dimensionality of the given input, further reducing the number of parameters within that activation.', text='3. The pooling layer will then simply perform downsampling along the spatial dimensionality of the given input, further reducing the number of parameters within that activation.', enumerated=False, marker='-'), ListItem(self_ref='#/texts/44', parent=RefItem(cref='#/groups/2'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=5, bbox=BoundingBox(l=138.19285583496094, t=648.51904296875, r=480.5962829589844, b=601.6806640625, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 267))], orig='4. The fully-connected layers will then perform the same duties found in standard ANNs and attempt to produce class scores from the activations, to be used for classification. It is also suggested that ReLu may be used between these layers, as to improve performance.', text='4. The fully-connected layers will then perform the same duties found in standard ANNs and attempt to produce class scores from the activations, to be used for classification. It is also suggested that ReLu may be used between these layers, as to improve performance.', enumerated=False, marker='-'), TextItem(self_ref='#/texts/45', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=5, bbox=BoundingBox(l=133.94287109375, t=593.2196044921875, r=480.58685302734375, b=558.2871704101562, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 222))], orig='Through this simple method of transformation, CNNs are able to transform the original input layer by layer using convolutional and downsampling techniques to produce class scores for classification and regression purposes.', text='Through this simple method of transformation, CNNs are able to transform the original input layer by layer using convolutional and downsampling techniques to produce class scores for classification and regression purposes.'), TextItem(self_ref='#/texts/46', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.CAPTION: 'caption'>, prov=[ProvenanceItem(page_no=5, bbox=BoundingBox(l=133.96603393554688, t=388.8979797363281, r=480.5867919921875, b=342.2826843261719, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 279))], orig='Fig. 3: Activations taken from the first convolutional layer of a simplistic deep CNN, after training on the MNIST database of handwritten digits. If you look carefully, you can see that the network has successfully picked up on characteristics unique to specific numeric digits.', text='Fig. 3: Activations taken from the first convolutional layer of a simplistic deep CNN, after training on the MNIST database of handwritten digits. If you look carefully, you can see that the network has successfully picked up on characteristics unique to specific numeric digits.'), TextItem(self_ref='#/texts/47', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=5, bbox=BoundingBox(l=133.87347412109375, t=316.361328125, r=480.5867919921875, b=257.6666259765625, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 328))], orig='However, it is important to note that simply understanding the overall architecture of a CNN architecture will not suffice. The creation and optimisation of these models can take quite some time, and can be quite confusing. We will now explore in detail the individual layers, detailing their hyperparameters and connectivities.', text='However, it is important to note that simply understanding the overall architecture of a CNN architecture will not suffice. The creation and optimisation of these models can take quite some time, and can be quite confusing. We will now explore in detail the individual layers, detailing their hyperparameters and connectivities.'), SectionHeaderItem(self_ref='#/texts/48', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.SECTION_HEADER: 'section_header'>, prov=[ProvenanceItem(page_no=5, bbox=BoundingBox(l=133.96798706054688, t=228.07293701171875, r=248.6064910888672, b=217.20751953125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 23))], orig='2.2 Convolutional layer', text='2.2 Convolutional layer', level=1), TextItem(self_ref='#/texts/49', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=5, bbox=BoundingBox(l=133.87005615234375, t=198.13641357421875, r=480.5868835449219, b=174.95159912109375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 150))], orig='As the name implies, the convolutional layer plays a vital role in how CNNs operate. The layers parameters focus around the use of learnable kernels .', text='As the name implies, the convolutional layer plays a vital role in how CNNs operate. The layers parameters focus around the use of learnable kernels .'), TextItem(self_ref='#/texts/50', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PAGE_HEADER: 'page_header'>, prov=[ProvenanceItem(page_no=6, bbox=BoundingBox(l=133.78485107421875, t=747.7951049804688, r=139.71607971191406, b=738.469482421875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 1))], orig='6', text='6'), TextItem(self_ref='#/texts/51', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PAGE_HEADER: 'page_header'>, prov=[ProvenanceItem(page_no=6, bbox=BoundingBox(l=167.21620178222656, t=748.348388671875, r=246.2442626953125, b=738.469482421875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 20))], orig='Keiron O’Shea et al.', text='Keiron O’Shea et al.'), TextItem(self_ref='#/texts/52', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=6, bbox=BoundingBox(l=133.7237091064453, t=723.1614379882812, r=480.7948913574219, b=664.4654541015625, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 331))], orig='These kernels are usually small in spatial dimensionality, but spreads along the entirety of the depth of the input. When the data hits a convolutional layer, the layer convolves each filter across the spatial dimensionality of the input to produce a 2D activation map. These activation maps can be visualised, as seen in Figure 3.', text='These kernels are usually small in spatial dimensionality, but spreads along the entirety of the depth of the input. When the data hits a convolutional layer, the layer convolves each filter across the spatial dimensionality of the input to produce a 2D activation map. These activation maps can be visualised, as seen in Figure 3.'), TextItem(self_ref='#/texts/53', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=6, bbox=BoundingBox(l=133.92149353027344, t=655.136962890625, r=480.5972595214844, b=608.3823852539062, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 275))], orig=\"As we glide through the input, the scalar product is calculated for each value in that kernel. (Figure 4) From this the network will learn kernels that 'fire' when they see a specific feature at a given spatial position of the input. These are commonly known as activations .\", text=\"As we glide through the input, the scalar product is calculated for each value in that kernel. (Figure 4) From this the network will learn kernels that 'fire' when they see a specific feature at a given spatial position of the input. These are commonly known as activations .\"), TextItem(self_ref='#/texts/54', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.CAPTION: 'caption'>, prov=[ProvenanceItem(page_no=6, bbox=BoundingBox(l=134.05441284179688, t=487.0065612792969, r=480.5867614746094, b=452.8345642089844, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 217))], orig='Fig. 4: A visual representation of a convolutional layer. The centre element of the kernel is placed over the input vector, of which is then calculated and replaced with a weighted sum of itself and any nearby pixels.', text='Fig. 4: A visual representation of a convolutional layer. The centre element of the kernel is placed over the input vector, of which is then calculated and replaced with a weighted sum of itself and any nearby pixels.'), TextItem(self_ref='#/texts/55', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=6, bbox=BoundingBox(l=133.9464569091797, t=425.8910827636719, r=480.7759704589844, b=391.0392761230469, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 166))], orig='Every kernel will have a corresponding activation map, of which will be stacked along the depth dimension to form the full output volume from the convolutional layer.', text='Every kernel will have a corresponding activation map, of which will be stacked along the depth dimension to form the full output volume from the convolutional layer.'), TextItem(self_ref='#/texts/56', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=6, bbox=BoundingBox(l=133.86756896972656, t=381.7151184082031, r=480.5912780761719, b=298.9527587890625, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 519))], orig='As we alluded to earlier, training ANNs on inputs such as images results in models of which are too big to train effectively. This comes down to the fullyconnected manner of standard ANN neurons, so to mitigate against this every neuron in a convolutional layer is only connected to small region of the input volume. The dimensionality of this region is commonly referred to as the receptive field size of the neuron. The magnitude of the connectivity through the depth is nearly always equal to the depth of the input.', text='As we alluded to earlier, training ANNs on inputs such as images results in models of which are too big to train effectively. This comes down to the fullyconnected manner of standard ANN neurons, so to mitigate against this every neuron in a convolutional layer is only connected to small region of the input volume. The dimensionality of this region is commonly referred to as the receptive field size of the neuron. The magnitude of the connectivity through the depth is nearly always equal to the depth of the input.'), TextItem(self_ref='#/texts/57', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=6, bbox=BoundingBox(l=133.9088134765625, t=289.67852783203125, r=480.5946960449219, b=219.24505615234375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 456))], orig='For example, if the input to the network is an image of size 64 × 64 × 3 (a RGBcoloured image with a dimensionality of 64 × 64 ) and we set the receptive field size as 6 × 6 , we would have a total of 108 weights on each neuron within the convolutional layer. ( 6 × 6 × 3 where 3 is the magnitude of connectivity across the depth of the volume) To put this into perspective, a standard neuron seen in other forms of ANN would contain 12 , 288 weights each.', text='For example, if the input to the network is an image of size 64 × 64 × 3 (a RGBcoloured image with a dimensionality of 64 × 64 ) and we set the receptive field size as 6 × 6 , we would have a total of 108 weights on each neuron within the convolutional layer. ( 6 × 6 × 3 where 3 is the magnitude of connectivity across the depth of the volume) To put this into perspective, a standard neuron seen in other forms of ANN would contain 12 , 288 weights each.'), TextItem(self_ref='#/texts/58', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=6, bbox=BoundingBox(l=133.94898986816406, t=209.80560302734375, r=480.58673095703125, b=175.01348876953125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 227))], orig='Convolutional layers are also able to significantly reduce the complexity of the model through the optimisation of its output. These are optimised through three hyperparameters, the depth , the stride and setting zero-padding .', text='Convolutional layers are also able to significantly reduce the complexity of the model through the optimisation of its output. These are optimised through three hyperparameters, the depth , the stride and setting zero-padding .'), TextItem(self_ref='#/texts/59', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PAGE_HEADER: 'page_header'>, prov=[ProvenanceItem(page_no=7, bbox=BoundingBox(l=255.95968627929688, t=748.3567504882812, r=447.5419921875, b=738.469482421875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 45))], orig='Introduction to Convolutional Neural Networks', text='Introduction to Convolutional Neural Networks'), TextItem(self_ref='#/texts/60', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PAGE_HEADER: 'page_header'>, prov=[ProvenanceItem(page_no=7, bbox=BoundingBox(l=475.1293029785156, t=748.2567138671875, r=480.5921630859375, b=738.469482421875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 1))], orig='7', text='7'), TextItem(self_ref='#/texts/61', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=7, bbox=BoundingBox(l=133.85671997070312, t=723.3087158203125, r=480.73211669921875, b=640.0972900390625, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 507))], orig='The depth of the output volume produced by the convolutional layers can be manually set through the number of neurons within the layer to a the same region of the input. This can be seen with other forms of ANNs, where the all of the neurons in the hidden layer are directly connected to every single neuron beforehand. Reducing this hyperparameter can significantly minimise the total number of neurons of the network, but it can also significantly reduce the pattern recognition capabilities of the model.', text='The depth of the output volume produced by the convolutional layers can be manually set through the number of neurons within the layer to a the same region of the input. This can be seen with other forms of ANNs, where the all of the neurons in the hidden layer are directly connected to every single neuron beforehand. Reducing this hyperparameter can significantly minimise the total number of neurons of the network, but it can also significantly reduce the pattern recognition capabilities of the model.'), TextItem(self_ref='#/texts/62', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=7, bbox=BoundingBox(l=133.76483154296875, t=630.474365234375, r=480.6409912109375, b=559.7763671875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 432))], orig='We are also able to define the stride in which we set the depth around the spatial dimensionality of the input in order to place the receptive field. For example if we were to set a stride as 1, then we would have a heavily overlapped receptive field producing extremely large activations. Alternatively, setting the stride to a greater number will reduce the amount of overlapping and produce an output of lower spatial dimensions.', text='We are also able to define the stride in which we set the depth around the spatial dimensionality of the input in order to place the receptive field. For example if we were to set a stride as 1, then we would have a heavily overlapped receptive field producing extremely large activations. Alternatively, setting the stride to a greater number will reduce the amount of overlapping and produce an output of lower spatial dimensions.'), TextItem(self_ref='#/texts/63', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=7, bbox=BoundingBox(l=133.788818359375, t=549.264892578125, r=480.5876159667969, b=514.8633422851562, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 169))], orig='Zero-padding is the simple process of padding the border of the input, and is an effective method to give further control as to the dimensionality of the output volumes.', text='Zero-padding is the simple process of padding the border of the input, and is an effective method to give further control as to the dimensionality of the output volumes.'), TextItem(self_ref='#/texts/64', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=7, bbox=BoundingBox(l=133.85841369628906, t=504.53253173828125, r=480.78466796875, b=469.6905822753906, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 205))], orig='It is important to understand that through using these techniques, we will alter the spatial dimensionality of the convolutional layers output. To calculate this, you can make use of the following formula:', text='It is important to understand that through using these techniques, we will alter the spatial dimensionality of the convolutional layers output. To calculate this, you can make use of the following formula:'), TextItem(self_ref='#/texts/65', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.FORMULA: 'formula'>, prov=[ProvenanceItem(page_no=7, bbox=BoundingBox(l=276.7643127441406, t=446.41644287109375, r=338.3543395996094, b=423.17724609375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 21))], orig='( V - R ) + 2 Z S + 1', text='( V - R ) + 2 Z S + 1'), TextItem(self_ref='#/texts/66', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=7, bbox=BoundingBox(l=134.1217041015625, t=404.2350158691406, r=480.6238708496094, b=345.4455261230469, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 360))], orig='Where V represents the input volume size ( height × width × depth ), R represents the receptive field size, Z is the amount of zero padding set and S referring to the stride. If the calculated result from this equation is not equal to a whole integer then the stride has been incorrectly set, as the neurons will be unable to fit neatly across the given input.', text='Where V represents the input volume size ( height × width × depth ), R represents the receptive field size, Z is the amount of zero padding set and S referring to the stride. If the calculated result from this equation is not equal to a whole integer then the stride has been incorrectly set, as the neurons will be unable to fit neatly across the given input.'), TextItem(self_ref='#/texts/67', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=7, bbox=BoundingBox(l=133.8834686279297, t=334.9963684082031, r=480.5930480957031, b=288.89764404296875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 262))], orig='Despite our best efforts so far we will still find that our models are still enormous if we use an image input of any real dimensionality. However, methods have been developed as to greatly curtail the overall number of parameters within the convolutional layer.', text='Despite our best efforts so far we will still find that our models are still enormous if we use an image input of any real dimensionality. However, methods have been developed as to greatly curtail the overall number of parameters within the convolutional layer.'), TextItem(self_ref='#/texts/68', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=7, bbox=BoundingBox(l=133.8805694580078, t=278.3824462890625, r=480.5939636230469, b=219.0997314453125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 374))], orig='Parameter sharing works on the assumption that if one region feature is useful to compute at a set spatial region, then it is likely to be useful in another region. If we constrain each individual activation map within the output volume to the same weights and bias, then we will see a massive reduction in the number of parameters being produced by the convolutional layer.', text='Parameter sharing works on the assumption that if one region feature is useful to compute at a set spatial region, then it is likely to be useful in another region. If we constrain each individual activation map within the output volume to the same weights and bias, then we will see a massive reduction in the number of parameters being produced by the convolutional layer.'), TextItem(self_ref='#/texts/69', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=7, bbox=BoundingBox(l=133.85092163085938, t=209.80804443359375, r=480.58685302734375, b=175.01153564453125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 238))], orig='As a result of this as the backpropagation stage occurs, each neuron in the output will represent the overall gradient of which can be totalled across the depth - thus only updating a single set of weights, as opposed to every single one.', text='As a result of this as the backpropagation stage occurs, each neuron in the output will represent the overall gradient of which can be totalled across the depth - thus only updating a single set of weights, as opposed to every single one.'), TextItem(self_ref='#/texts/70', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PAGE_HEADER: 'page_header'>, prov=[ProvenanceItem(page_no=8, bbox=BoundingBox(l=133.78009033203125, t=747.8963623046875, r=139.24819946289062, b=738.469482421875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 1))], orig='8', text='8'), TextItem(self_ref='#/texts/71', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PAGE_HEADER: 'page_header'>, prov=[ProvenanceItem(page_no=8, bbox=BoundingBox(l=167.4324493408203, t=748.35205078125, r=246.2442626953125, b=738.469482421875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 20))], orig='Keiron O’Shea et al.', text='Keiron O’Shea et al.'), SectionHeaderItem(self_ref='#/texts/72', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.SECTION_HEADER: 'section_header'>, prov=[ProvenanceItem(page_no=8, bbox=BoundingBox(l=133.94790649414062, t=723.0996704101562, r=218.04713439941406, b=712.1331787109375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 17))], orig='2.3 Pooling layer', text='2.3 Pooling layer', level=1), TextItem(self_ref='#/texts/73', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=8, bbox=BoundingBox(l=134.07662963867188, t=694.5655517578125, r=480.5867614746094, b=659.6265258789062, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 176))], orig='Pooling layers aim to gradually reduce the dimensionality of the representation, and thus further reduce the number of parameters and the computational complexity of the model.', text='Pooling layers aim to gradually reduce the dimensionality of the representation, and thus further reduce the number of parameters and the computational complexity of the model.'), TextItem(self_ref='#/texts/74', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=8, bbox=BoundingBox(l=133.83888244628906, t=650.9923095703125, r=480.61639404296875, b=580.969482421875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 416))], orig='The pooling layer operates over each activation map in the input, and scales its dimensionality using the \"MAX\" function. In most CNNs, these come in the form of max-pooling layers with kernels of a dimensionality of 2 × 2 applied with a stride of 2 along the spatial dimensions of the input. This scales the activation map down to 25% of the original size - whilst maintaining the depth volume to its standard size.', text='The pooling layer operates over each activation map in the input, and scales its dimensionality using the \"MAX\" function. In most CNNs, these come in the form of max-pooling layers with kernels of a dimensionality of 2 × 2 applied with a stride of 2 along the spatial dimensions of the input. This scales the activation map down to 25% of the original size - whilst maintaining the depth volume to its standard size.'), TextItem(self_ref='#/texts/75', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=8, bbox=BoundingBox(l=133.8048095703125, t=572.5150146484375, r=480.6969909667969, b=490.258056640625, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 532))], orig='Due to the destructive nature of the pooling layer, there are only two generally observed methods of max-pooling. Usually, the stride and filters of the pooling layers are both set to 2 × 2 , which will allow the layer to extend through the entirety of the spatial dimensionality of the input. Furthermore overlapping pooling may be utilised, where the stride is set to 2 with a kernel size set to 3 . Due to the destructive nature of pooling, having a kernel size above 3 will usually greatly decrease the performance of the model.', text='Due to the destructive nature of the pooling layer, there are only two generally observed methods of max-pooling. Usually, the stride and filters of the pooling layers are both set to 2 × 2 , which will allow the layer to extend through the entirety of the spatial dimensionality of the input. Furthermore overlapping pooling may be utilised, where the stride is set to 2 with a kernel size set to 3 . Due to the destructive nature of pooling, having a kernel size above 3 will usually greatly decrease the performance of the model.'), TextItem(self_ref='#/texts/76', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=8, bbox=BoundingBox(l=133.9580078125, t=482.387939453125, r=480.59521484375, b=423.36590576171875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 343))], orig='It is also important to understand that beyond max-pooling, CNN architectures may contain general-pooling. General pooling layers are comprised of pooling neurons that are able to perform a multitude of common operations including L1/L2-normalisation, and average pooling. However, this tutorial will primarily focus on the use of max-pooling.', text='It is also important to understand that beyond max-pooling, CNN architectures may contain general-pooling. General pooling layers are comprised of pooling neurons that are able to perform a multitude of common operations including L1/L2-normalisation, and average pooling. However, this tutorial will primarily focus on the use of max-pooling.'), SectionHeaderItem(self_ref='#/texts/77', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.SECTION_HEADER: 'section_header'>, prov=[ProvenanceItem(page_no=8, bbox=BoundingBox(l=134.02223205566406, t=395.8370666503906, r=255.7871551513672, b=384.8973693847656, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 25))], orig='2.4 Fully-connected layer', text='2.4 Fully-connected layer', level=1), TextItem(self_ref='#/texts/78', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=8, bbox=BoundingBox(l=133.71995544433594, t=367.19287109375, r=480.5868225097656, b=320.27154541015625, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 259))], orig='The fully-connected layer contains neurons of which are directly connected to the neurons in the two adjacent layers, without being connected to any layers within them. This is analogous to way that neurons are arranged in traditional forms of ANN. (Figure 1)', text='The fully-connected layer contains neurons of which are directly connected to the neurons in the two adjacent layers, without being connected to any layers within them. This is analogous to way that neurons are arranged in traditional forms of ANN. (Figure 1)'), SectionHeaderItem(self_ref='#/texts/79', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.SECTION_HEADER: 'section_header'>, prov=[ProvenanceItem(page_no=8, bbox=BoundingBox(l=133.84336853027344, t=291.55572509765625, r=195.1865997314453, b=279.1473388671875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 9))], orig='3 Recipes', text='3 Recipes', level=1), TextItem(self_ref='#/texts/80', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=8, bbox=BoundingBox(l=133.87806701660156, t=257.97393798828125, r=480.6416015625, b=174.8902587890625, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 547))], orig='Despite the relatively small number of layers required to form a CNN, there is no set way of formulating a CNN architecture. That being said, it would be idiotic to simply throw a few of layers together and expect it to work. Through reading of related literature it is obvious that much like other forms of ANNs, CNNs tend to follow a common architecture. This common architecture is illustrated in Figure 2, where convolutional layers are stacked, followed by pooling layers in a repeated manner before feeding forward to fully-connected layers.', text='Despite the relatively small number of layers required to form a CNN, there is no set way of formulating a CNN architecture. That being said, it would be idiotic to simply throw a few of layers together and expect it to work. Through reading of related literature it is obvious that much like other forms of ANNs, CNNs tend to follow a common architecture. This common architecture is illustrated in Figure 2, where convolutional layers are stacked, followed by pooling layers in a repeated manner before feeding forward to fully-connected layers.'), TextItem(self_ref='#/texts/81', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PAGE_HEADER: 'page_header'>, prov=[ProvenanceItem(page_no=9, bbox=BoundingBox(l=255.96002197265625, t=748.3395385742188, r=447.56597900390625, b=738.469482421875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 45))], orig='Introduction to Convolutional Neural Networks', text='Introduction to Convolutional Neural Networks'), TextItem(self_ref='#/texts/82', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PAGE_HEADER: 'page_header'>, prov=[ProvenanceItem(page_no=9, bbox=BoundingBox(l=474.8158874511719, t=748.0952758789062, r=480.5921630859375, b=738.469482421875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 1))], orig='9', text='9'), TextItem(self_ref='#/texts/83', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=9, bbox=BoundingBox(l=133.87208557128906, t=723.253662109375, r=480.66082763671875, b=676.4204711914062, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 263))], orig='Another common CNN architecture is to stack two convolutional layers before each pooling layer, as illustrated in Figure 5. This is strongly recommended as stacking multiple convolutional layers allows for more complex features of the input vector to be selected.', text='Another common CNN architecture is to stack two convolutional layers before each pooling layer, as illustrated in Figure 5. This is strongly recommended as stacking multiple convolutional layers allows for more complex features of the input vector to be selected.'), TextItem(self_ref='#/texts/84', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.CAPTION: 'caption'>, prov=[ProvenanceItem(page_no=9, bbox=BoundingBox(l=134.1597137451172, t=553.8919677734375, r=480.640380859375, b=518.9637451171875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 213))], orig='Fig. 5: A common form of CNN architecture in which convolutional layers are stacked between ReLus continuously before being passed through the pooling layer, before going between one or many fully connected ReLus.', text='Fig. 5: A common form of CNN architecture in which convolutional layers are stacked between ReLus continuously before being passed through the pooling layer, before going between one or many fully connected ReLus.'), TextItem(self_ref='#/texts/85', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=9, bbox=BoundingBox(l=133.85641479492188, t=493.7992248535156, r=480.6982727050781, b=363.0449523925781, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 877))], orig='It is also advised to split large convolutional layers up into many smaller sized convolutional layers. This is to reduce the amount of computational complexity within a given convolutional layer. For example, if you were to stack three convolutional layers on top of each other with a receptive field of 3 × 3 . Each neuron of the first convolutional layer will have a 3 × 3 view of the input vector. A neuron on the second convolutional layer will then have a 5 × 5 view of the input vector. A neuron on the third convolutional layer will then have a 7 × 7 view of the input vector. As these stacks feature non-linearities which in turn allows us to express stronger features of the input with fewer parameters. However, it is important to understand that this does come with a distinct memory allocation problem - especially when making use of the backpropagation algorithm.', text='It is also advised to split large convolutional layers up into many smaller sized convolutional layers. This is to reduce the amount of computational complexity within a given convolutional layer. For example, if you were to stack three convolutional layers on top of each other with a receptive field of 3 × 3 . Each neuron of the first convolutional layer will have a 3 × 3 view of the input vector. A neuron on the second convolutional layer will then have a 5 × 5 view of the input vector. A neuron on the third convolutional layer will then have a 7 × 7 view of the input vector. As these stacks feature non-linearities which in turn allows us to express stronger features of the input with fewer parameters. However, it is important to understand that this does come with a distinct memory allocation problem - especially when making use of the backpropagation algorithm.'), TextItem(self_ref='#/texts/86', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=9, bbox=BoundingBox(l=133.77923583984375, t=355.1921691894531, r=480.5867614746094, b=332.4426574707031, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 134))], orig='The input layer should be recursively divisible by two. Common numbers include 32 × 32 , 64 × 64 , 96 × 96 , 128 × 128 and 224 × 224 .', text='The input layer should be recursively divisible by two. Common numbers include 32 × 32 , 64 × 64 , 96 × 96 , 128 × 128 and 224 × 224 .'), TextItem(self_ref='#/texts/87', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=9, bbox=BoundingBox(l=133.83213806152344, t=324.521484375, r=480.5868835449219, b=277.426513671875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 315))], orig='Whilst using small filters, set stride to one and make use of zero-padding as to ensure that the convolutional layers do not reconfigure any of the dimensionality of the input. The amount of zero-padding to be used should be calculated by taking one away from the receptive field size and dividing by two.activation', text='Whilst using small filters, set stride to one and make use of zero-padding as to ensure that the convolutional layers do not reconfigure any of the dimensionality of the input. The amount of zero-padding to be used should be calculated by taking one away from the receptive field size and dividing by two.activation'), TextItem(self_ref='#/texts/88', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=9, bbox=BoundingBox(l=133.7228240966797, t=269.895751953125, r=480.7103271484375, b=174.67718505859375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 623))], orig=\"CNNs are extremely powerful machine learning algorithms, however they can be horrendously resource-heavy. An example of this problem could be in filtering a large image (anything over 128 × 128 could be considered large), so if the input is 227 × 227 (as seen with ImageNet) and we're filtering with 64 kernels each with a zero padding of then the result will be three activation vectors of size 227 × 227 × 64 - which calculates to roughly 10 million activations - or an enormous 70 megabytes of memory per image. In this case you have two options. Firstly, you can reduce the spatial dimensionality of the input images by\", text=\"CNNs are extremely powerful machine learning algorithms, however they can be horrendously resource-heavy. An example of this problem could be in filtering a large image (anything over 128 × 128 could be considered large), so if the input is 227 × 227 (as seen with ImageNet) and we're filtering with 64 kernels each with a zero padding of then the result will be three activation vectors of size 227 × 227 × 64 - which calculates to roughly 10 million activations - or an enormous 70 megabytes of memory per image. In this case you have two options. Firstly, you can reduce the spatial dimensionality of the input images by\"), TextItem(self_ref='#/texts/89', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PAGE_HEADER: 'page_header'>, prov=[ProvenanceItem(page_no=10, bbox=BoundingBox(l=134.48631286621094, t=747.890869140625, r=143.73849487304688, b=738.469482421875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 2))], orig='10', text='10'), TextItem(self_ref='#/texts/90', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PAGE_HEADER: 'page_header'>, prov=[ProvenanceItem(page_no=10, bbox=BoundingBox(l=167.15977478027344, t=748.3831787109375, r=246.2442626953125, b=738.469482421875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 20))], orig='Keiron O’Shea et al.', text='Keiron O’Shea et al.'), TextItem(self_ref='#/texts/91', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=10, bbox=BoundingBox(l=133.8414764404297, t=723.1402587890625, r=480.5868835449219, b=688.1945190429688, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 212))], orig='resizing the raw images to something a little less heavy. Alternatively, you can go against everything we stated earlier in this document and opt for larger filter sizes with a larger stride (2, as opposed to 1).', text='resizing the raw images to something a little less heavy. Alternatively, you can go against everything we stated earlier in this document and opt for larger filter sizes with a larger stride (2, as opposed to 1).'), TextItem(self_ref='#/texts/92', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=10, bbox=BoundingBox(l=134.21324157714844, t=681.12890625, r=480.5867919921875, b=634.5774536132812, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 266))], orig='In addition to the few rules-of-thumb outlined above, it is also important to acknowledge a few \\'tricks\\' about generalised ANN training techniques. The authors suggest a read of Geoffrey Hinton\\'s excellent \"Practical Guide to Training Restricted Boltzmann Machines\".', text='In addition to the few rules-of-thumb outlined above, it is also important to acknowledge a few \\'tricks\\' about generalised ANN training techniques. The authors suggest a read of Geoffrey Hinton\\'s excellent \"Practical Guide to Training Restricted Boltzmann Machines\".'), SectionHeaderItem(self_ref='#/texts/93', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.SECTION_HEADER: 'section_header'>, prov=[ProvenanceItem(page_no=10, bbox=BoundingBox(l=133.84384155273438, t=610.5025024414062, r=215.11590576171875, b=598.3948364257812, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 12))], orig='4 Conclusion', text='4 Conclusion', level=1), TextItem(self_ref='#/texts/94', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=10, bbox=BoundingBox(l=133.68148803710938, t=582.052978515625, r=480.5868225097656, b=534.984130859375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 277))], orig='Convolutional Neural Networks differ to other forms of Artifical Neural Network in that instead of focusing on the entirety of the problem domain, knowledge about the specific type of input is exploited. This in turn allows for a much simpler network architecture to be set up.', text='Convolutional Neural Networks differ to other forms of Artifical Neural Network in that instead of focusing on the entirety of the problem domain, knowledge about the specific type of input is exploited. This in turn allows for a much simpler network architecture to be set up.'), TextItem(self_ref='#/texts/95', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=10, bbox=BoundingBox(l=133.798095703125, t=527.90673828125, r=480.5867919921875, b=493.0619201660156, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 198))], orig='This paper has outlined the basic concepts of Convolutional Neural Networks, explaining the layers required to build one and detailing how best to structure the network in most image analysis tasks.', text='This paper has outlined the basic concepts of Convolutional Neural Networks, explaining the layers required to build one and detailing how best to structure the network in most image analysis tasks.'), TextItem(self_ref='#/texts/96', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=10, bbox=BoundingBox(l=133.8413543701172, t=485.8362121582031, r=480.8612365722656, b=415.5055236816406, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 394))], orig='Research in the field of image analysis using neural networks has somewhat slowed in recent times. This is partly due to the incorrect belief surrounding the level of complexity and knowledge required to begin modelling these superbly powerful machine learning algorithms. The authors hope that this paper has in some way reduced this confusion, and made the field more accessible to beginners.', text='Research in the field of image analysis using neural networks has somewhat slowed in recent times. This is partly due to the incorrect belief surrounding the level of complexity and knowledge required to begin modelling these superbly powerful machine learning algorithms. The authors hope that this paper has in some way reduced this confusion, and made the field more accessible to beginners.'), SectionHeaderItem(self_ref='#/texts/97', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.SECTION_HEADER: 'section_header'>, prov=[ProvenanceItem(page_no=10, bbox=BoundingBox(l=133.9271240234375, t=391.6942138671875, r=243.6768798828125, b=379.3033142089844, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 16))], orig='Acknowledgements', text='Acknowledgements', level=1), TextItem(self_ref='#/texts/98', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=10, bbox=BoundingBox(l=133.86122131347656, t=362.5135803222656, r=480.82696533203125, b=339.8528747558594, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 105))], orig='The authors would like to thank Dr. Chuan Lu and Nicholas Dimonaco for useful discussion and suggestions.', text='The authors would like to thank Dr. Chuan Lu and Nicholas Dimonaco for useful discussion and suggestions.'), SectionHeaderItem(self_ref='#/texts/99', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.SECTION_HEADER: 'section_header'>, prov=[ProvenanceItem(page_no=10, bbox=BoundingBox(l=134.29351806640625, t=315.983154296875, r=194.529052734375, b=303.7138671875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 10))], orig='References', text='References', level=1), ListItem(self_ref='#/texts/100', parent=RefItem(cref='#/groups/3'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=10, bbox=BoundingBox(l=138.7662811279297, t=293.84051513671875, r=480.5946960449219, b=262.0904235839844, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 207))], orig='1. Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for image classification. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. pp. 3642-3649. IEEE (2012)', text='1. Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for image classification. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. pp. 3642-3649. IEEE (2012)', enumerated=False, marker='-'), ListItem(self_ref='#/texts/101', parent=RefItem(cref='#/groups/3'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=10, bbox=BoundingBox(l=138.2350311279297, t=261.2042236328125, r=480.658203125, b=218.7664337158203, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 250))], orig='2. Cires¸an, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in breast cancer histology images with deep neural networks. In: Medical Image Computing and Computer-Assisted Intervention-MICCAI 2013, pp. 411-418. Springer (2013)', text='2. Cires¸an, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in breast cancer histology images with deep neural networks. In: Medical Image Computing and Computer-Assisted Intervention-MICCAI 2013, pp. 411-418. Springer (2013)', enumerated=False, marker='-'), ListItem(self_ref='#/texts/102', parent=RefItem(cref='#/groups/3'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=10, bbox=BoundingBox(l=138.2299041748047, t=217.8837890625, r=480.5948486328125, b=175.30133056640625, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 267))], orig='3. Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible, high performance convolutional neural networks for image classification. In: IJCAI Proceedings-International Joint Conference on Artificial Intelligence. vol. 22, p. 1237 (2011)', text='3. Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible, high performance convolutional neural networks for image classification. In: IJCAI Proceedings-International Joint Conference on Artificial Intelligence. vol. 22, p. 1237 (2011)', enumerated=False, marker='-'), TextItem(self_ref='#/texts/103', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PAGE_HEADER: 'page_header'>, prov=[ProvenanceItem(page_no=11, bbox=BoundingBox(l=256.0866394042969, t=748.2721557617188, r=447.5473937988281, b=738.469482421875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 45))], orig='Introduction to Convolutional Neural Networks', text='Introduction to Convolutional Neural Networks'), TextItem(self_ref='#/texts/104', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PAGE_HEADER: 'page_header'>, prov=[ProvenanceItem(page_no=11, bbox=BoundingBox(l=471.0235290527344, t=748.2294311523438, r=480.5921325683594, b=738.469482421875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 2))], orig='11', text='11'), ListItem(self_ref='#/texts/105', parent=RefItem(cref='#/groups/4'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=11, bbox=BoundingBox(l=138.15142822265625, t=722.5249633789062, r=480.60302734375, b=679.6904907226562, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 255))], orig='4. Cires¸an, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural network committees for handwritten character classification. In: Document Analysis and Recognition (ICDAR), 2011 International Conference on. pp. 1135-1139. IEEE (2011)', text='4. Cires¸an, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural network committees for handwritten character classification. In: Document Analysis and Recognition (ICDAR), 2011 International Conference on. pp. 1135-1139. IEEE (2011)', enumerated=False, marker='-'), ListItem(self_ref='#/texts/106', parent=RefItem(cref='#/groups/4'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=11, bbox=BoundingBox(l=138.33560180664062, t=678.4413452148438, r=480.71539306640625, b=657.7725219726562, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 143))], orig='5. Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural networksa review. Pattern recognition 35(10), 2279-2301 (2002)', text='5. Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural networksa review. Pattern recognition 35(10), 2279-2301 (2002)', enumerated=False, marker='-'), ListItem(self_ref='#/texts/107', parent=RefItem(cref='#/groups/4'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=11, bbox=BoundingBox(l=138.4720458984375, t=656.4887084960938, r=480.6998291015625, b=613.9365844726562, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 274))], orig='6. Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware accelerated convolutional neural networks for synthetic vision systems. In: Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp. 257-260. IEEE (2010)', text='6. Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware accelerated convolutional neural networks for synthetic vision systems. In: Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp. 257-260. IEEE (2010)', enumerated=False, marker='-'), ListItem(self_ref='#/texts/108', parent=RefItem(cref='#/groups/4'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=11, bbox=BoundingBox(l=138.4396514892578, t=612.5001220703125, r=480.5898132324219, b=592.0186157226562, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 101))], orig='7. Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum 9(1), 926 (2010)', text='7. Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum 9(1), 926 (2010)', enumerated=False, marker='-'), ListItem(self_ref='#/texts/109', parent=RefItem(cref='#/groups/4'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=11, bbox=BoundingBox(l=138.49429321289062, t=590.37109375, r=480.5947265625, b=559.1426391601562, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 198))], orig='8. Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 (2012)', text='8. Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 (2012)', enumerated=False, marker='-'), ListItem(self_ref='#/texts/110', parent=RefItem(cref='#/groups/4'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=11, bbox=BoundingBox(l=138.10630798339844, t=557.6162109375, r=480.5946960449219, b=526.2656860351562, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 185))], orig='9. Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1), 221-231 (2013)', text='9. Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1), 221-231 (2013)', enumerated=False, marker='-'), ListItem(self_ref='#/texts/111', parent=RefItem(cref='#/groups/4'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=11, bbox=BoundingBox(l=134.7255096435547, t=525.0989990234375, r=480.77935791015625, b=482.4297180175781, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 255))], orig='10. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Largescale video classification with convolutional neural networks. In: Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725-1732. IEEE (2014)', text='10. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Largescale video classification with convolutional neural networks. In: Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725-1732. IEEE (2014)', enumerated=False, marker='-'), ListItem(self_ref='#/texts/112', parent=RefItem(cref='#/groups/4'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=11, bbox=BoundingBox(l=134.47573852539062, t=481.14715576171875, r=480.59478759765625, b=449.5537414550781, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 189))], orig='11. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems. pp. 1097-1105 (2012)', text='11. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems. pp. 1097-1105 (2012)', enumerated=False, marker='-'), ListItem(self_ref='#/texts/113', parent=RefItem(cref='#/groups/4'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=11, bbox=BoundingBox(l=134.62811279296875, t=448.33197021484375, r=480.59466552734375, b=416.6767883300781, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 196))], orig='12. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural computation 1(4), 541-551 (1989)', text='12. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural computation 1(4), 541-551 (1989)', enumerated=False, marker='-'), ListItem(self_ref='#/texts/114', parent=RefItem(cref='#/groups/4'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=11, bbox=BoundingBox(l=134.47447204589844, t=415.2291564941406, r=480.590087890625, b=394.7588195800781, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 157))], orig='13. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998)', text='13. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998)', enumerated=False, marker='-'), ListItem(self_ref='#/texts/115', parent=RefItem(cref='#/groups/4'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=11, bbox=BoundingBox(l=134.4318084716797, t=393.6202087402344, r=480.5900573730469, b=372.8408508300781, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 143))], orig='14. Nebauer, C.: Evaluation of convolutional neural networks for visual recognition. Neural Networks, IEEE Transactions on 9(4), 685-696 (1998)', text='14. Nebauer, C.: Evaluation of convolutional neural networks for visual recognition. Neural Networks, IEEE Transactions on 9(4), 685-696 (1998)', enumerated=False, marker='-'), ListItem(self_ref='#/texts/116', parent=RefItem(cref='#/groups/4'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=11, bbox=BoundingBox(l=134.57110595703125, t=371.8392028808594, r=480.6988525390625, b=350.9228820800781, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 162))], orig='15. Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural networks applied to visual document analysis. In: null. p. 958. IEEE (2003)', text='15. Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural networks applied to visual document analysis. In: null. p. 958. IEEE (2003)', enumerated=False, marker='-'), ListItem(self_ref='#/texts/117', parent=RefItem(cref='#/groups/4'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=11, bbox=BoundingBox(l=134.7103729248047, t=349.75054931640625, r=480.5900573730469, b=329.0058898925781, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 102))], orig='16. Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of Toronto (2013)', text='16. Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of Toronto (2013)', enumerated=False, marker='-'), ListItem(self_ref='#/texts/118', parent=RefItem(cref='#/groups/4'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=11, bbox=BoundingBox(l=134.64859008789062, t=327.69183349609375, r=480.5948181152344, b=296.1289367675781, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 199))], orig='17. Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with convolutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings. IEEE. pp. 224-229. IEEE (2005)', text='17. Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with convolutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings. IEEE. pp. 224-229. IEEE (2005)', enumerated=False, marker='-'), ListItem(self_ref='#/texts/119', parent=RefItem(cref='#/groups/4'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=11, bbox=BoundingBox(l=134.7650146484375, t=295.0496826171875, r=480.59002685546875, b=274.2109680175781, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 158))], orig='18. Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In: Advances in Neural Information Processing Systems. pp. 2553-2561 (2013)', text='18. Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In: Advances in Neural Information Processing Systems. pp. 2553-2561 (2013)', enumerated=False, marker='-'), ListItem(self_ref='#/texts/120', parent=RefItem(cref='#/groups/4'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=11, bbox=BoundingBox(l=134.6785888671875, t=273.25, r=480.60009765625, b=241.1690673828125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 249))], orig='19. Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks (siconnets) and their application of face detection. In: Neural Networks, 2003. Proceedings of the International Joint Conference on. vol. 3, pp. 2157-2162. IEEE (2003)', text='19. Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks (siconnets) and their application of face detection. In: Neural Networks, 2003. Proceedings of the International Joint Conference on. vol. 3, pp. 2157-2162. IEEE (2003)', enumerated=False, marker='-'), ListItem(self_ref='#/texts/121', parent=RefItem(cref='#/groups/4'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=11, bbox=BoundingBox(l=134.3703155517578, t=240.287353515625, r=480.590087890625, b=219.4169921875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 144))], orig='20. Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional neural networks. arXiv preprint arXiv:1301.3557 (2013)', text='20. Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional neural networks. arXiv preprint arXiv:1301.3557 (2013)', enumerated=False, marker='-'), ListItem(self_ref='#/texts/122', parent=RefItem(cref='#/groups/4'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=11, bbox=BoundingBox(l=134.26535034179688, t=218.36279296875, r=480.59014892578125, b=197.0758056640625, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 143))], orig='21. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In: Computer Vision-ECCV 2014, pp. 818-833. Springer (2014)', text='21. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In: Computer Vision-ECCV 2014, pp. 818-833. Springer (2014)', enumerated=False, marker='-')], pictures=[PictureItem(self_ref='#/pictures/0', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PICTURE: 'picture'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=193.8206024169922, t=725.920654296875, r=420.3355407714844, b=574.5050659179688, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 335))], captions=[RefItem(cref='#/texts/14')], references=[], footnotes=[], image=None, annotations=[]), PictureItem(self_ref='#/pictures/1', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PICTURE: 'picture'>, prov=[ProvenanceItem(page_no=4, bbox=BoundingBox(l=194.3790283203125, t=452.28253173828125, r=421.985595703125, b=329.37823486328125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 65))], captions=[RefItem(cref='#/texts/36')], references=[], footnotes=[], image=None, annotations=[]), PictureItem(self_ref='#/pictures/2', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PICTURE: 'picture'>, prov=[ProvenanceItem(page_no=5, bbox=BoundingBox(l=135.41848754882812, t=533.9510498046875, r=479.3868713378906, b=399.3819274902344, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 279))], captions=[RefItem(cref='#/texts/46')], references=[], footnotes=[], image=None, annotations=[]), PictureItem(self_ref='#/pictures/3', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PICTURE: 'picture'>, prov=[ProvenanceItem(page_no=6, bbox=BoundingBox(l=168.14492797851562, t=584.790283203125, r=446.1294250488281, b=497.7923889160156, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 217))], captions=[RefItem(cref='#/texts/54')], references=[], footnotes=[], image=None, annotations=[]), PictureItem(self_ref='#/pictures/4', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.PICTURE: 'picture'>, prov=[ProvenanceItem(page_no=9, bbox=BoundingBox(l=133.3032989501953, t=653.78076171875, r=481.8245544433594, b=564.6818237304688, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 213))], captions=[RefItem(cref='#/texts/84')], references=[], footnotes=[], image=None, annotations=[])], tables=[], key_value_items=[], pages={1: PageItem(size=Size(width=595.2760009765625, height=841.8900146484375), image=None, page_no=1), 2: PageItem(size=Size(width=595.2760009765625, height=841.8900146484375), image=None, page_no=2), 3: PageItem(size=Size(width=595.2760009765625, height=841.8900146484375), image=None, page_no=3), 4: PageItem(size=Size(width=595.2760009765625, height=841.8900146484375), image=None, page_no=4), 5: PageItem(size=Size(width=595.2760009765625, height=841.8900146484375), image=None, page_no=5), 6: PageItem(size=Size(width=595.2760009765625, height=841.8900146484375), image=None, page_no=6), 7: PageItem(size=Size(width=595.2760009765625, height=841.8900146484375), image=None, page_no=7), 8: PageItem(size=Size(width=595.2760009765625, height=841.8900146484375), image=None, page_no=8), 9: PageItem(size=Size(width=595.2760009765625, height=841.8900146484375), image=None, page_no=9), 10: PageItem(size=Size(width=595.2760009765625, height=841.8900146484375), image=None, page_no=10), 11: PageItem(size=Size(width=595.2760009765625, height=841.8900146484375), image=None, page_no=11)})\n"
     ]
    }
   ],
   "source": [
    "print(conv_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: unspecified with name=_root_\n",
      "  1: page_header\n",
      "  2: section_header\n",
      "  3: text\n",
      "  4: list with name=list\n",
      "   5: list_item\n",
      "   6: list_item\n",
      "  7: text\n",
      "  8: text\n",
      "  9: text\n",
      "  10: text\n",
      "  11: section_header\n",
      "  12: text\n",
      "  13: text\n",
      "  14: page_header\n",
      "  15: page_header\n",
      "  16: caption\n",
      "  17: picture\n",
      "  18: text\n",
      "  19: text\n",
      "  20: text\n",
      "  21: text\n",
      "  22: page_header\n",
      "  23: page_header\n",
      "  24: text\n",
      "  25: text\n",
      "  26: text\n",
      "  27: section_header\n",
      "  28: text\n",
      "  29: text\n",
      "  30: text\n",
      "  31: section_header\n",
      "  32: text\n",
      "  33: page_header\n",
      "  34: text\n",
      "  35: text\n",
      "  36: text\n",
      "  37: section_header\n",
      "  38: text\n",
      "  39: caption\n",
      "  40: picture\n",
      "  41: text\n",
      "  42: list with name=list\n",
      "   43: list_item\n",
      "   44: list_item\n",
      "  45: page_header\n",
      "  46: page_header\n",
      "  47: list with name=list\n",
      "   48: list_item\n",
      "   49: list_item\n",
      "   50: list_item\n",
      "  51: text\n",
      "  52: caption\n",
      "  53: picture\n",
      "  54: text\n",
      "  55: section_header\n",
      "  56: text\n",
      "  57: page_header\n",
      "  58: page_header\n",
      "  59: text\n",
      "  60: text\n",
      "  61: caption\n",
      "  62: picture\n",
      "  63: text\n",
      "  64: text\n",
      "  65: text\n",
      "  66: text\n",
      "  67: page_header\n",
      "  68: page_header\n",
      "  69: text\n",
      "  70: text\n",
      "  71: text\n",
      "  72: text\n",
      "  73: formula\n",
      "  74: text\n",
      "  75: text\n",
      "  76: text\n",
      "  77: text\n",
      "  78: page_header\n",
      "  79: page_header\n",
      "  80: section_header\n",
      "  81: text\n",
      "  82: text\n",
      "  83: text\n",
      "  84: text\n",
      "  85: section_header\n",
      "  86: text\n",
      "  87: section_header\n",
      "  88: text\n",
      "  89: page_header\n",
      "  90: page_header\n",
      "  91: text\n",
      "  92: caption\n",
      "  93: picture\n",
      "  94: text\n",
      "  95: text\n",
      "  96: text\n",
      "  97: text\n",
      "  98: page_header\n",
      "  99: page_header\n",
      "  100: text\n",
      "  101: text\n",
      "  102: section_header\n",
      "  103: text\n",
      "  104: text\n",
      "  105: text\n",
      "  106: section_header\n",
      "  107: text\n",
      "  108: section_header\n",
      "  109: list with name=list\n",
      "   110: list_item\n",
      "   111: list_item\n",
      "   112: list_item\n",
      "  113: page_header\n",
      "  114: page_header\n",
      "  115: list with name=list\n",
      "   116: list_item\n",
      "   117: list_item\n",
      "   118: list_item\n",
      "   119: list_item\n",
      "   120: list_item\n",
      "   121: list_item\n",
      "   122: list_item\n",
      "   123: list_item\n",
      "   124: list_item\n",
      "   125: list_item\n",
      "   126: list_item\n",
      "   127: list_item\n",
      "   128: list_item\n",
      "   129: list_item\n",
      "   130: list_item\n",
      "   131: list_item\n",
      "   132: list_item\n",
      "   133: list_item\n"
     ]
    }
   ],
   "source": [
    "conv_result.document.print_element_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Output:\n",
      "{\n",
      "    \"schema_name\": \"DoclingDocument\",\n",
      "    \"version\": \"1.0.0\",\n",
      "    \"name\": \"1511.08458v2\",\n",
      "    \"origin\": {\n",
      "        \"mimetype\": \"application/pdf\",\n",
      "        \"binary_hash\": 5687491352442750095,\n",
      "        \"filename\": \"1511.08458v2.pdf\"\n",
      "    },\n",
      "    \"furniture\": {\n",
      "        \"self_ref\": \"#/furniture\",\n",
      "        \"children\": [],\n",
      "        \"name\": \"_root_\",\n",
      "        \"label\": \"unspecified\"\n",
      "    },\n",
      "    \"body\": {\n",
      "        \"self_ref\": \"#/body\",\n",
      "        \"children\": [\n",
      "            {\n",
      "                \"$ref\": \"#/texts/0\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/1\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/2\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/groups/0\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/5\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/6\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/7\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/8\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/9\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/10\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/11\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/12\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/13\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/14\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/pictures/0\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/15\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/16\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/17\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/18\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/19\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/20\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/21\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/22\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/23\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/24\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/25\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/26\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/27\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/28\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/29\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/30\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/31\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/32\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/33\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/34\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/35\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/36\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/pictures/1\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/37\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/groups/1\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/40\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/41\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/groups/2\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/45\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/46\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/pictures/2\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/47\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/48\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/49\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/50\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/51\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/52\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/53\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/54\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/pictures/3\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/55\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/56\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/57\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/58\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/59\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/60\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/61\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/62\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/63\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/64\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/65\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/66\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/67\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/68\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/69\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/70\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/71\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/72\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/73\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/74\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/75\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/76\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/77\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/78\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/79\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/80\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/81\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/82\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/83\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/84\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/pictures/4\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/85\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/86\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/87\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/88\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/89\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/90\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/91\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/92\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/93\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/94\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/95\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/96\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/97\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/98\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/99\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/groups/3\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/103\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/texts/104\"\n",
      "            },\n",
      "            {\n",
      "                \"$ref\": \"#/groups/4\"\n",
      "            }\n",
      "        ],\n",
      "        \"name\": \"_root_\",\n",
      "        \"label\": \"unspecified\"\n",
      "    },\n",
      "    \"groups\": [\n",
      "        {\n",
      "            \"self_ref\": \"#/groups/0\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/3\"\n",
      "                },\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/4\"\n",
      "                }\n",
      "            ],\n",
      "            \"name\": \"list\",\n",
      "            \"label\": \"list\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/groups/1\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/38\"\n",
      "                },\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/39\"\n",
      "                }\n",
      "            ],\n",
      "            \"name\": \"list\",\n",
      "            \"label\": \"list\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/groups/2\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/42\"\n",
      "                },\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/43\"\n",
      "                },\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/44\"\n",
      "                }\n",
      "            ],\n",
      "            \"name\": \"list\",\n",
      "            \"label\": \"list\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/groups/3\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/100\"\n",
      "                },\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/101\"\n",
      "                },\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/102\"\n",
      "                }\n",
      "            ],\n",
      "            \"name\": \"list\",\n",
      "            \"label\": \"list\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/groups/4\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/105\"\n",
      "                },\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/106\"\n",
      "                },\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/107\"\n",
      "                },\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/108\"\n",
      "                },\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/109\"\n",
      "                },\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/110\"\n",
      "                },\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/111\"\n",
      "                },\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/112\"\n",
      "                },\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/113\"\n",
      "                },\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/114\"\n",
      "                },\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/115\"\n",
      "                },\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/116\"\n",
      "                },\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/117\"\n",
      "                },\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/118\"\n",
      "                },\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/119\"\n",
      "                },\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/120\"\n",
      "                },\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/121\"\n",
      "                },\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/122\"\n",
      "                }\n",
      "            ],\n",
      "            \"name\": \"list\",\n",
      "            \"label\": \"list\"\n",
      "        }\n",
      "    ],\n",
      "    \"texts\": [\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/0\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"page_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 1,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 17.202451705932617,\n",
      "                        \"t\": 576.9400024414062,\n",
      "                        \"r\": 36.33979415893555,\n",
      "                        \"b\": 236.99996948242188,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        37\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"arXiv:1511.08458v2 [cs.NE] 2 Dec 2015\",\n",
      "            \"text\": \"arXiv:1511.08458v2 [cs.NE] 2 Dec 2015\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/1\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"section_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 1,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 137.64463806152344,\n",
      "                        \"t\": 725.666259765625,\n",
      "                        \"r\": 476.8287353515625,\n",
      "                        \"b\": 710.9338989257812,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        48\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"An Introduction to Convolutional Neural Networks\",\n",
      "            \"text\": \"An Introduction to Convolutional Neural Networks\",\n",
      "            \"level\": 1\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/2\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 1,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 235.75559997558594,\n",
      "                        \"t\": 681.5888061523438,\n",
      "                        \"r\": 378.7718811035156,\n",
      "                        \"b\": 670.2728271484375,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        31\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Keiron O'Shea 1 and Ryan Nash 2\",\n",
      "            \"text\": \"Keiron O'Shea 1 and Ryan Nash 2\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/3\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/0\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 1,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 140.86863708496094,\n",
      "                        \"t\": 660.33056640625,\n",
      "                        \"r\": 474.03369140625,\n",
      "                        \"b\": 639.7752075195312,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        94\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB keo7@aber.ac.uk\",\n",
      "            \"text\": \"1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB keo7@aber.ac.uk\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/4\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/0\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 1,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 139.36328125,\n",
      "                        \"t\": 638.429931640625,\n",
      "                        \"r\": 475.5403747558594,\n",
      "                        \"b\": 616.848388671875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        83\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"2 School of Computing and Communications, Lancaster University, Lancashire, LA1 4YW\",\n",
      "            \"text\": \"2 School of Computing and Communications, Lancaster University, Lancashire, LA1 4YW\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/5\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 1,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 245.39329528808594,\n",
      "                        \"t\": 615.0885009765625,\n",
      "                        \"r\": 369.54632568359375,\n",
      "                        \"b\": 607.099853515625,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        23\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"nashrd@live.lancs.ac.uk\",\n",
      "            \"text\": \"nashrd@live.lancs.ac.uk\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/6\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 1,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 162.13299560546875,\n",
      "                        \"t\": 588.0892944335938,\n",
      "                        \"r\": 452.33795166015625,\n",
      "                        \"b\": 490.3534851074219,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        595\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Abstract. The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs.\",\n",
      "            \"text\": \"Abstract. The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/7\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 1,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 162.31292724609375,\n",
      "                        \"t\": 482.57501220703125,\n",
      "                        \"r\": 452.2415466308594,\n",
      "                        \"b\": 439.4319152832031,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        280\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.\",\n",
      "            \"text\": \"This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/8\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 1,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 162.44024658203125,\n",
      "                        \"t\": 420.5909423828125,\n",
      "                        \"r\": 452.410888671875,\n",
      "                        \"b\": 399.15863037109375,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        91\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Keywords: Pattern recognition, artificial neural networks, machine learning, image analysis\",\n",
      "            \"text\": \"Keywords: Pattern recognition, artificial neural networks, machine learning, image analysis\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/9\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"section_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 1,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.2177734375,\n",
      "                        \"t\": 370.3959655761719,\n",
      "                        \"r\": 221.76300048828125,\n",
      "                        \"b\": 358.1449279785156,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        14\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"1 Introduction\",\n",
      "            \"text\": \"1 Introduction\",\n",
      "            \"level\": 1\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/10\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 1,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.661865234375,\n",
      "                        \"t\": 336.8586730957031,\n",
      "                        \"r\": 480.72076416015625,\n",
      "                        \"b\": 265.77435302734375,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        399\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Artificial Neural Networks (ANNs) are computational processing systems of which are heavily inspired by way biological nervous systems (such as the human brain) operate. ANNs are mainly comprised of a high number of interconnected computational nodes (referred to as neurons), of which work entwine in a distributed fashion to collectively learn from the input in order to optimise its final output.\",\n",
      "            \"text\": \"Artificial Neural Networks (ANNs) are computational processing systems of which are heavily inspired by way biological nervous systems (such as the human brain) operate. ANNs are mainly comprised of a high number of interconnected computational nodes (referred to as neurons), of which work entwine in a distributed fashion to collectively learn from the input in order to optimise its final output.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/11\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 1,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.71791076660156,\n",
      "                        \"t\": 257.9127197265625,\n",
      "                        \"r\": 480.8759460449219,\n",
      "                        \"b\": 174.66064453125,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        508\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"The basic structure of a ANN can be modelled as shown in Figure 1. We would load the input, usually in the form of a multidimensional vector to the input layer of which will distribute it to the hidden layers. The hidden layers will then make decisions from the previous layer and weigh up how a stochastic change within itself detriments or improves the final output, and this is referred to as the process of learning. Having multiple hidden layers stacked upon each-other is commonly called deep learning.\",\n",
      "            \"text\": \"The basic structure of a ANN can be modelled as shown in Figure 1. We would load the input, usually in the form of a multidimensional vector to the input layer of which will distribute it to the hidden layers. The hidden layers will then make decisions from the previous layer and weigh up how a stochastic change within itself detriments or improves the final output, and this is referred to as the process of learning. Having multiple hidden layers stacked upon each-other is commonly called deep learning.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/12\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"page_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 2,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.6807861328125,\n",
      "                        \"t\": 748.2429809570312,\n",
      "                        \"r\": 139.58908081054688,\n",
      "                        \"b\": 738.469482421875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        1\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"2\",\n",
      "            \"text\": \"2\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/13\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"page_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 2,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 167.17018127441406,\n",
      "                        \"t\": 748.364013671875,\n",
      "                        \"r\": 246.2442626953125,\n",
      "                        \"b\": 738.469482421875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        20\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Keiron O\\u2019Shea et al.\",\n",
      "            \"text\": \"Keiron O\\u2019Shea et al.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/14\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"caption\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 2,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.7849884033203,\n",
      "                        \"t\": 563.7962646484375,\n",
      "                        \"r\": 480.5867919921875,\n",
      "                        \"b\": 505.6305236816406,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        335\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Fig. 1: A simple three layered feedforward neural network (FNN), comprised of a input layer, a hidden layer and an output layer. This structure is the basis of a number of common ANN architectures, included but not limited to Feedforward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and Recurrent Neural Networks (RNNs).\",\n",
      "            \"text\": \"Fig. 1: A simple three layered feedforward neural network (FNN), comprised of a input layer, a hidden layer and an output layer. This structure is the basis of a number of common ANN architectures, included but not limited to Feedforward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and Recurrent Neural Networks (RNNs).\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/15\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 2,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.74380493164062,\n",
      "                        \"t\": 476.2651062011719,\n",
      "                        \"r\": 480.59271240234375,\n",
      "                        \"b\": 394.2167663574219,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        475\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"The two key learning paradigms in image processing tasks are supervised and unsupervised learning. Supervised learning is learning through pre-labelled inputs, which act as targets. For each training example there will be a set of input values (vectors) and one or more associated designated output values. The goal of this form of training is to reduce the models overall classification error, through correct calculation of the output value of training example by training.\",\n",
      "            \"text\": \"The two key learning paradigms in image processing tasks are supervised and unsupervised learning. Supervised learning is learning through pre-labelled inputs, which act as targets. For each training example there will be a set of input values (vectors) and one or more associated designated output values. The goal of this form of training is to reduce the models overall classification error, through correct calculation of the output value of training example by training.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/16\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 2,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.91444396972656,\n",
      "                        \"t\": 383.7518615722656,\n",
      "                        \"r\": 480.5906982421875,\n",
      "                        \"b\": 324.7509765625,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        338\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Unsupervised learning differs in that the training set does not include any labels. Success is usually determined by whether the network is able to reduce or increase an associated cost function. However, it is important to note that most image-focused pattern-recognition tasks usually depend on classification using supervised learning.\",\n",
      "            \"text\": \"Unsupervised learning differs in that the training set does not include any labels. Success is usually determined by whether the network is able to reduce or increase an associated cost function. However, it is important to note that most image-focused pattern-recognition tasks usually depend on classification using supervised learning.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/17\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 2,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.6765899658203,\n",
      "                        \"t\": 314.57537841796875,\n",
      "                        \"r\": 480.7476806640625,\n",
      "                        \"b\": 220.25634765625,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        623\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Convolutional Neural Networks (CNNs) are analogous to traditional ANNs in that they are comprised of neurons that self-optimise through learning. Each neuron will still receive an input and perform a operation (such as a scalar product followed by a non-linear function) - the basis of countless ANNs. From the input raw image vectors to the final output of the class score, the entire of the network will still express a single perceptive score function (the weight). The last layer will contain loss functions associated with the classes, and all of the regular tips and tricks developed for traditional ANNs still apply.\",\n",
      "            \"text\": \"Convolutional Neural Networks (CNNs) are analogous to traditional ANNs in that they are comprised of neurons that self-optimise through learning. Each neuron will still receive an input and perform a operation (such as a scalar product followed by a non-linear function) - the basis of countless ANNs. From the input raw image vectors to the final output of the class score, the entire of the network will still express a single perceptive score function (the weight). The last layer will contain loss functions associated with the classes, and all of the regular tips and tricks developed for traditional ANNs still apply.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/18\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 2,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.68446350097656,\n",
      "                        \"t\": 209.895751953125,\n",
      "                        \"r\": 480.58685302734375,\n",
      "                        \"b\": 175.1072998046875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        235\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"The only notable difference between CNNs and traditional ANNs is that CNNs are primarily used in the field of pattern recognition within images. This allows us to encode image-specific features into the architecture, making the network\",\n",
      "            \"text\": \"The only notable difference between CNNs and traditional ANNs is that CNNs are primarily used in the field of pattern recognition within images. This allows us to encode image-specific features into the architecture, making the network\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/19\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"page_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 3,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 255.9642333984375,\n",
      "                        \"t\": 748.3575439453125,\n",
      "                        \"r\": 447.5419921875,\n",
      "                        \"b\": 738.469482421875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        45\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Introduction to Convolutional Neural Networks\",\n",
      "            \"text\": \"Introduction to Convolutional Neural Networks\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/20\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"page_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 3,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 475.16705322265625,\n",
      "                        \"t\": 748.3566284179688,\n",
      "                        \"r\": 480.5921630859375,\n",
      "                        \"b\": 738.469482421875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        1\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"3\",\n",
      "            \"text\": \"3\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/21\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 3,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.2552032470703,\n",
      "                        \"t\": 723.0224609375,\n",
      "                        \"r\": 480.5867004394531,\n",
      "                        \"b\": 700.3305053710938,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        106\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"more suited for image-focused tasks - whilst further reducing the parameters required to set up the model.\",\n",
      "            \"text\": \"more suited for image-focused tasks - whilst further reducing the parameters required to set up the model.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/22\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 3,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.69602966308594,\n",
      "                        \"t\": 691.5694580078125,\n",
      "                        \"r\": 481.4434814453125,\n",
      "                        \"b\": 597.1464233398438,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        573\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"One of the largest limitations of traditional forms of ANN is that they tend to struggle with the computational complexity required to compute image data. Common machine learning benchmarking datasets such as the MNIST database of handwritten digits are suitable for most forms of ANN, due to its relatively small image dimensionality of just 28 \\u00d7 28 . With this dataset a single neuron in the first hidden layer will contain 784 weights ( 28 \\u00d7 28 \\u00d7 1 where 1 bare in mind that MNIST is normalised to just black and white values), which is manageable for most forms of ANN.\",\n",
      "            \"text\": \"One of the largest limitations of traditional forms of ANN is that they tend to struggle with the computational complexity required to compute image data. Common machine learning benchmarking datasets such as the MNIST database of handwritten digits are suitable for most forms of ANN, due to its relatively small image dimensionality of just 28 \\u00d7 28 . With this dataset a single neuron in the first hidden layer will contain 784 weights ( 28 \\u00d7 28 \\u00d7 1 where 1 bare in mind that MNIST is normalised to just black and white values), which is manageable for most forms of ANN.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/23\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 3,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.93231201171875,\n",
      "                        \"t\": 588.4321899414062,\n",
      "                        \"r\": 480.5915222167969,\n",
      "                        \"b\": 529.69775390625,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        398\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"If you consider a more substantial coloured image input of 64 \\u00d7 64 , the number of weights on just a single neuron of the first layer increases substantially to 12 , 288 . Also take into account that to deal with this scale of input, the network will also need to be a lot larger than one used to classify colour-normalised MNIST digits, then you will understand the drawbacks of using such models.\",\n",
      "            \"text\": \"If you consider a more substantial coloured image input of 64 \\u00d7 64 , the number of weights on just a single neuron of the first layer increases substantially to 12 , 288 . Also take into account that to deal with this scale of input, the network will also need to be a lot larger than one used to classify colour-normalised MNIST digits, then you will understand the drawbacks of using such models.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/24\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"section_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 3,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.107177734375,\n",
      "                        \"t\": 499.7483825683594,\n",
      "                        \"r\": 207.76548767089844,\n",
      "                        \"b\": 489.11602783203125,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        15\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"1.1 Overfitting\",\n",
      "            \"text\": \"1.1 Overfitting\",\n",
      "            \"level\": 1\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/25\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 3,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.87643432617188,\n",
      "                        \"t\": 469.1485290527344,\n",
      "                        \"r\": 480.58685302734375,\n",
      "                        \"b\": 410.846435546875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        332\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"But why does it matter? Surely we could just increase the number of hidden layers in our network, and perhaps increase the number of neurons within them? The simple answer to this question is no. This is down to two reasons, one being the simple problem of not having unlimited computational power and time to train these huge ANNs.\",\n",
      "            \"text\": \"But why does it matter? Surely we could just increase the number of hidden layers in our network, and perhaps increase the number of neurons within them? The simple answer to this question is no. This is down to two reasons, one being the simple problem of not having unlimited computational power and time to train these huge ANNs.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/26\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 3,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.83572387695312,\n",
      "                        \"t\": 401.69970703125,\n",
      "                        \"r\": 480.64300537109375,\n",
      "                        \"b\": 319.45880126953125,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        512\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"The second reason is stopping or reducing the effects of overfitting. Overfitting is basically when a network is unable to learn effectively due to a number of reasons. It is an important concept of most, if not all machine learning algorithms and it is important that every precaution is taken as to reduce its effects. If our models were to exhibit signs of overfitting then we may see a reduced ability to pinpoint generalised features for not only our training dataset, but also our test and prediction sets.\",\n",
      "            \"text\": \"The second reason is stopping or reducing the effects of overfitting. Overfitting is basically when a network is unable to learn effectively due to a number of reasons. It is an important concept of most, if not all machine learning algorithms and it is important that every precaution is taken as to reduce its effects. If our models were to exhibit signs of overfitting then we may see a reduced ability to pinpoint generalised features for not only our training dataset, but also our test and prediction sets.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/27\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 3,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.8554229736328,\n",
      "                        \"t\": 310.45977783203125,\n",
      "                        \"r\": 480.5868835449219,\n",
      "                        \"b\": 276.2095642089844,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        213\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"This is the main reason behind reducing the complexity of our ANNs. The less parameters required to train, the less likely the network will overfit - and of course, improve the predictive performance of the model.\",\n",
      "            \"text\": \"This is the main reason behind reducing the complexity of our ANNs. The less parameters required to train, the less likely the network will overfit - and of course, improve the predictive performance of the model.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/28\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"section_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 3,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.10931396484375,\n",
      "                        \"t\": 245.74603271484375,\n",
      "                        \"r\": 248.64112854003906,\n",
      "                        \"b\": 233.28492736816406,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        18\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"2 CNN architecture\",\n",
      "            \"text\": \"2 CNN architecture\",\n",
      "            \"level\": 1\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/29\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 3,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.93942260742188,\n",
      "                        \"t\": 209.9462890625,\n",
      "                        \"r\": 480.8302001953125,\n",
      "                        \"b\": 175.13427734375,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        211\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"As noted earlier, CNNs primarily focus on the basis that the input will be comprised of images. This focuses the architecture to be set up in way to best suit the need for dealing with the specific type of data.\",\n",
      "            \"text\": \"As noted earlier, CNNs primarily focus on the basis that the input will be comprised of images. This focuses the architecture to be set up in way to best suit the need for dealing with the specific type of data.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/30\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"page_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 4,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.68788146972656,\n",
      "                        \"t\": 748.06494140625,\n",
      "                        \"r\": 139.39495849609375,\n",
      "                        \"b\": 738.469482421875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        1\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"4\",\n",
      "            \"text\": \"4\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/31\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 4,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 167.81515502929688,\n",
      "                        \"t\": 747.408935546875,\n",
      "                        \"r\": 246.2442626953125,\n",
      "                        \"b\": 738.469482421875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        20\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Keiron O'Shea et al.\",\n",
      "            \"text\": \"Keiron O'Shea et al.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/32\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 4,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.76507568359375,\n",
      "                        \"t\": 723.2340698242188,\n",
      "                        \"r\": 480.6532897949219,\n",
      "                        \"b\": 652.2295532226562,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        455\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"One of the key differences is that the neurons that the layers within the CNN are comprised of neurons organised into three dimensions, the spatial dimensionality of the input ( height and the width ) and the depth . The depth does not refer to the total number of layers within the ANN, but the third dimension of a activation volume. Unlike standard ANNS, the neurons within any given layer will only connect to a small region of the layer preceding it.\",\n",
      "            \"text\": \"One of the key differences is that the neurons that the layers within the CNN are comprised of neurons organised into three dimensions, the spatial dimensionality of the input ( height and the width ) and the depth . The depth does not refer to the total number of layers within the ANN, but the third dimension of a activation volume. Unlike standard ANNS, the neurons within any given layer will only connect to a small region of the layer preceding it.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/33\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 4,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.84022521972656,\n",
      "                        \"t\": 645.1727905273438,\n",
      "                        \"r\": 480.62689208984375,\n",
      "                        \"b\": 574.0120849609375,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        407\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"In practice this would mean that for the example given earlier, the input 'volume' will have a dimensionality of 64 \\u00d7 64 \\u00d7 3 (height, width and depth), leading to a final output layer comprised of a dimensionality of 1 \\u00d7 1 \\u00d7 n (where n represents the possible number of classes) as we would have condensed the full input dimensionality into a smaller volume of class scores filed across the depth dimension.\",\n",
      "            \"text\": \"In practice this would mean that for the example given earlier, the input 'volume' will have a dimensionality of 64 \\u00d7 64 \\u00d7 3 (height, width and depth), leading to a final output layer comprised of a dimensionality of 1 \\u00d7 1 \\u00d7 n (where n represents the possible number of classes) as we would have condensed the full input dimensionality into a smaller volume of class scores filed across the depth dimension.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/34\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"section_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 4,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.0306854248047,\n",
      "                        \"t\": 547.65576171875,\n",
      "                        \"r\": 247.65118408203125,\n",
      "                        \"b\": 537.4288330078125,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        24\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"2.1 Overall architecture\",\n",
      "            \"text\": \"2.1 Overall architecture\",\n",
      "            \"level\": 1\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/35\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 4,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.74267578125,\n",
      "                        \"t\": 520.6640625,\n",
      "                        \"r\": 480.75616455078125,\n",
      "                        \"b\": 474.25042724609375,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        270\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"CNNs are comprised of three types of layers. These are convolutional layers, pooling layers and fully-connected layers . When these layers are stacked, a CNN architecture has been formed. A simplified CNN architecture for MNIST classification is illustrated in Figure 2.\",\n",
      "            \"text\": \"CNNs are comprised of three types of layers. These are convolutional layers, pooling layers and fully-connected layers . When these layers are stacked, a CNN architecture has been formed. A simplified CNN architecture for MNIST classification is illustrated in Figure 2.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/36\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"caption\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 4,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 165.54881286621094,\n",
      "                        \"t\": 317.5498046875,\n",
      "                        \"r\": 449.3086853027344,\n",
      "                        \"b\": 307.25872802734375,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        65\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Fig. 2: An simple CNN architecture, comprised of just five layers\",\n",
      "            \"text\": \"Fig. 2: An simple CNN architecture, comprised of just five layers\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/37\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 4,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.9955596923828,\n",
      "                        \"t\": 282.42083740234375,\n",
      "                        \"r\": 480.58673095703125,\n",
      "                        \"b\": 259.68157958984375,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        88\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"The basic functionality of the example CNN above can be broken down into four key areas.\",\n",
      "            \"text\": \"The basic functionality of the example CNN above can be broken down into four key areas.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/38\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/1\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 4,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 138.5282745361328,\n",
      "                        \"t\": 251.7421875,\n",
      "                        \"r\": 480.59588623046875,\n",
      "                        \"b\": 229.0966796875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        91\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"1. As found in other forms of ANN, the input layer will hold the pixel values of the image.\",\n",
      "            \"text\": \"1. As found in other forms of ANN, the input layer will hold the pixel values of the image.\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/39\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/1\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 4,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 138.4064483642578,\n",
      "                        \"t\": 221.89215087890625,\n",
      "                        \"r\": 480.59625244140625,\n",
      "                        \"b\": 175.1615753173828,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        299\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"2. The convolutional layer will determine the output of neurons of which are connected to local regions of the input through the calculation of the scalar product between their weights and the region connected to the input volume. The rectified linear unit (commonly shortened to ReLu) aims to apply\",\n",
      "            \"text\": \"2. The convolutional layer will determine the output of neurons of which are connected to local regions of the input through the calculation of the scalar product between their weights and the region connected to the input volume. The rectified linear unit (commonly shortened to ReLu) aims to apply\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/40\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"page_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 5,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 255.9207763671875,\n",
      "                        \"t\": 748.3547973632812,\n",
      "                        \"r\": 447.56707763671875,\n",
      "                        \"b\": 738.469482421875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        45\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Introduction to Convolutional Neural Networks\",\n",
      "            \"text\": \"Introduction to Convolutional Neural Networks\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/41\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"page_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 5,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 475.2783203125,\n",
      "                        \"t\": 748.4425048828125,\n",
      "                        \"r\": 480.5921630859375,\n",
      "                        \"b\": 738.469482421875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        1\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"5\",\n",
      "            \"text\": \"5\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/42\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/2\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 5,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 150.73934936523438,\n",
      "                        \"t\": 723.2891845703125,\n",
      "                        \"r\": 480.59619140625,\n",
      "                        \"b\": 700.3113403320312,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        116\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"an 'elementwise' activation function such as sigmoid to the output of the activation produced by the previous layer.\",\n",
      "            \"text\": \"an 'elementwise' activation function such as sigmoid to the output of the activation produced by the previous layer.\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/43\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/2\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 5,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 138.22279357910156,\n",
      "                        \"t\": 691.7444458007812,\n",
      "                        \"r\": 480.5964050292969,\n",
      "                        \"b\": 657.0234985351562,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        177\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"3. The pooling layer will then simply perform downsampling along the spatial dimensionality of the given input, further reducing the number of parameters within that activation.\",\n",
      "            \"text\": \"3. The pooling layer will then simply perform downsampling along the spatial dimensionality of the given input, further reducing the number of parameters within that activation.\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/44\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/2\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 5,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 138.19285583496094,\n",
      "                        \"t\": 648.51904296875,\n",
      "                        \"r\": 480.5962829589844,\n",
      "                        \"b\": 601.6806640625,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        267\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"4. The fully-connected layers will then perform the same duties found in standard ANNs and attempt to produce class scores from the activations, to be used for classification. It is also suggested that ReLu may be used between these layers, as to improve performance.\",\n",
      "            \"text\": \"4. The fully-connected layers will then perform the same duties found in standard ANNs and attempt to produce class scores from the activations, to be used for classification. It is also suggested that ReLu may be used between these layers, as to improve performance.\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/45\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 5,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.94287109375,\n",
      "                        \"t\": 593.2196044921875,\n",
      "                        \"r\": 480.58685302734375,\n",
      "                        \"b\": 558.2871704101562,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        222\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Through this simple method of transformation, CNNs are able to transform the original input layer by layer using convolutional and downsampling techniques to produce class scores for classification and regression purposes.\",\n",
      "            \"text\": \"Through this simple method of transformation, CNNs are able to transform the original input layer by layer using convolutional and downsampling techniques to produce class scores for classification and regression purposes.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/46\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"caption\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 5,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.96603393554688,\n",
      "                        \"t\": 388.8979797363281,\n",
      "                        \"r\": 480.5867919921875,\n",
      "                        \"b\": 342.2826843261719,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        279\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Fig. 3: Activations taken from the first convolutional layer of a simplistic deep CNN, after training on the MNIST database of handwritten digits. If you look carefully, you can see that the network has successfully picked up on characteristics unique to specific numeric digits.\",\n",
      "            \"text\": \"Fig. 3: Activations taken from the first convolutional layer of a simplistic deep CNN, after training on the MNIST database of handwritten digits. If you look carefully, you can see that the network has successfully picked up on characteristics unique to specific numeric digits.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/47\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 5,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.87347412109375,\n",
      "                        \"t\": 316.361328125,\n",
      "                        \"r\": 480.5867919921875,\n",
      "                        \"b\": 257.6666259765625,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        328\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"However, it is important to note that simply understanding the overall architecture of a CNN architecture will not suffice. The creation and optimisation of these models can take quite some time, and can be quite confusing. We will now explore in detail the individual layers, detailing their hyperparameters and connectivities.\",\n",
      "            \"text\": \"However, it is important to note that simply understanding the overall architecture of a CNN architecture will not suffice. The creation and optimisation of these models can take quite some time, and can be quite confusing. We will now explore in detail the individual layers, detailing their hyperparameters and connectivities.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/48\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"section_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 5,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.96798706054688,\n",
      "                        \"t\": 228.07293701171875,\n",
      "                        \"r\": 248.6064910888672,\n",
      "                        \"b\": 217.20751953125,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        23\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"2.2 Convolutional layer\",\n",
      "            \"text\": \"2.2 Convolutional layer\",\n",
      "            \"level\": 1\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/49\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 5,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.87005615234375,\n",
      "                        \"t\": 198.13641357421875,\n",
      "                        \"r\": 480.5868835449219,\n",
      "                        \"b\": 174.95159912109375,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        150\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"As the name implies, the convolutional layer plays a vital role in how CNNs operate. The layers parameters focus around the use of learnable kernels .\",\n",
      "            \"text\": \"As the name implies, the convolutional layer plays a vital role in how CNNs operate. The layers parameters focus around the use of learnable kernels .\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/50\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"page_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 6,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.78485107421875,\n",
      "                        \"t\": 747.7951049804688,\n",
      "                        \"r\": 139.71607971191406,\n",
      "                        \"b\": 738.469482421875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        1\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"6\",\n",
      "            \"text\": \"6\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/51\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"page_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 6,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 167.21620178222656,\n",
      "                        \"t\": 748.348388671875,\n",
      "                        \"r\": 246.2442626953125,\n",
      "                        \"b\": 738.469482421875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        20\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Keiron O\\u2019Shea et al.\",\n",
      "            \"text\": \"Keiron O\\u2019Shea et al.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/52\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 6,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.7237091064453,\n",
      "                        \"t\": 723.1614379882812,\n",
      "                        \"r\": 480.7948913574219,\n",
      "                        \"b\": 664.4654541015625,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        331\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"These kernels are usually small in spatial dimensionality, but spreads along the entirety of the depth of the input. When the data hits a convolutional layer, the layer convolves each filter across the spatial dimensionality of the input to produce a 2D activation map. These activation maps can be visualised, as seen in Figure 3.\",\n",
      "            \"text\": \"These kernels are usually small in spatial dimensionality, but spreads along the entirety of the depth of the input. When the data hits a convolutional layer, the layer convolves each filter across the spatial dimensionality of the input to produce a 2D activation map. These activation maps can be visualised, as seen in Figure 3.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/53\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 6,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.92149353027344,\n",
      "                        \"t\": 655.136962890625,\n",
      "                        \"r\": 480.5972595214844,\n",
      "                        \"b\": 608.3823852539062,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        275\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"As we glide through the input, the scalar product is calculated for each value in that kernel. (Figure 4) From this the network will learn kernels that 'fire' when they see a specific feature at a given spatial position of the input. These are commonly known as activations .\",\n",
      "            \"text\": \"As we glide through the input, the scalar product is calculated for each value in that kernel. (Figure 4) From this the network will learn kernels that 'fire' when they see a specific feature at a given spatial position of the input. These are commonly known as activations .\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/54\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"caption\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 6,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.05441284179688,\n",
      "                        \"t\": 487.0065612792969,\n",
      "                        \"r\": 480.5867614746094,\n",
      "                        \"b\": 452.8345642089844,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        217\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Fig. 4: A visual representation of a convolutional layer. The centre element of the kernel is placed over the input vector, of which is then calculated and replaced with a weighted sum of itself and any nearby pixels.\",\n",
      "            \"text\": \"Fig. 4: A visual representation of a convolutional layer. The centre element of the kernel is placed over the input vector, of which is then calculated and replaced with a weighted sum of itself and any nearby pixels.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/55\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 6,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.9464569091797,\n",
      "                        \"t\": 425.8910827636719,\n",
      "                        \"r\": 480.7759704589844,\n",
      "                        \"b\": 391.0392761230469,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        166\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Every kernel will have a corresponding activation map, of which will be stacked along the depth dimension to form the full output volume from the convolutional layer.\",\n",
      "            \"text\": \"Every kernel will have a corresponding activation map, of which will be stacked along the depth dimension to form the full output volume from the convolutional layer.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/56\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 6,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.86756896972656,\n",
      "                        \"t\": 381.7151184082031,\n",
      "                        \"r\": 480.5912780761719,\n",
      "                        \"b\": 298.9527587890625,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        519\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"As we alluded to earlier, training ANNs on inputs such as images results in models of which are too big to train effectively. This comes down to the fullyconnected manner of standard ANN neurons, so to mitigate against this every neuron in a convolutional layer is only connected to small region of the input volume. The dimensionality of this region is commonly referred to as the receptive field size of the neuron. The magnitude of the connectivity through the depth is nearly always equal to the depth of the input.\",\n",
      "            \"text\": \"As we alluded to earlier, training ANNs on inputs such as images results in models of which are too big to train effectively. This comes down to the fullyconnected manner of standard ANN neurons, so to mitigate against this every neuron in a convolutional layer is only connected to small region of the input volume. The dimensionality of this region is commonly referred to as the receptive field size of the neuron. The magnitude of the connectivity through the depth is nearly always equal to the depth of the input.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/57\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 6,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.9088134765625,\n",
      "                        \"t\": 289.67852783203125,\n",
      "                        \"r\": 480.5946960449219,\n",
      "                        \"b\": 219.24505615234375,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        456\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"For example, if the input to the network is an image of size 64 \\u00d7 64 \\u00d7 3 (a RGBcoloured image with a dimensionality of 64 \\u00d7 64 ) and we set the receptive field size as 6 \\u00d7 6 , we would have a total of 108 weights on each neuron within the convolutional layer. ( 6 \\u00d7 6 \\u00d7 3 where 3 is the magnitude of connectivity across the depth of the volume) To put this into perspective, a standard neuron seen in other forms of ANN would contain 12 , 288 weights each.\",\n",
      "            \"text\": \"For example, if the input to the network is an image of size 64 \\u00d7 64 \\u00d7 3 (a RGBcoloured image with a dimensionality of 64 \\u00d7 64 ) and we set the receptive field size as 6 \\u00d7 6 , we would have a total of 108 weights on each neuron within the convolutional layer. ( 6 \\u00d7 6 \\u00d7 3 where 3 is the magnitude of connectivity across the depth of the volume) To put this into perspective, a standard neuron seen in other forms of ANN would contain 12 , 288 weights each.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/58\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 6,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.94898986816406,\n",
      "                        \"t\": 209.80560302734375,\n",
      "                        \"r\": 480.58673095703125,\n",
      "                        \"b\": 175.01348876953125,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        227\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Convolutional layers are also able to significantly reduce the complexity of the model through the optimisation of its output. These are optimised through three hyperparameters, the depth , the stride and setting zero-padding .\",\n",
      "            \"text\": \"Convolutional layers are also able to significantly reduce the complexity of the model through the optimisation of its output. These are optimised through three hyperparameters, the depth , the stride and setting zero-padding .\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/59\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"page_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 7,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 255.95968627929688,\n",
      "                        \"t\": 748.3567504882812,\n",
      "                        \"r\": 447.5419921875,\n",
      "                        \"b\": 738.469482421875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        45\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Introduction to Convolutional Neural Networks\",\n",
      "            \"text\": \"Introduction to Convolutional Neural Networks\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/60\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"page_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 7,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 475.1293029785156,\n",
      "                        \"t\": 748.2567138671875,\n",
      "                        \"r\": 480.5921630859375,\n",
      "                        \"b\": 738.469482421875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        1\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"7\",\n",
      "            \"text\": \"7\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/61\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 7,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.85671997070312,\n",
      "                        \"t\": 723.3087158203125,\n",
      "                        \"r\": 480.73211669921875,\n",
      "                        \"b\": 640.0972900390625,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        507\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"The depth of the output volume produced by the convolutional layers can be manually set through the number of neurons within the layer to a the same region of the input. This can be seen with other forms of ANNs, where the all of the neurons in the hidden layer are directly connected to every single neuron beforehand. Reducing this hyperparameter can significantly minimise the total number of neurons of the network, but it can also significantly reduce the pattern recognition capabilities of the model.\",\n",
      "            \"text\": \"The depth of the output volume produced by the convolutional layers can be manually set through the number of neurons within the layer to a the same region of the input. This can be seen with other forms of ANNs, where the all of the neurons in the hidden layer are directly connected to every single neuron beforehand. Reducing this hyperparameter can significantly minimise the total number of neurons of the network, but it can also significantly reduce the pattern recognition capabilities of the model.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/62\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 7,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.76483154296875,\n",
      "                        \"t\": 630.474365234375,\n",
      "                        \"r\": 480.6409912109375,\n",
      "                        \"b\": 559.7763671875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        432\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"We are also able to define the stride in which we set the depth around the spatial dimensionality of the input in order to place the receptive field. For example if we were to set a stride as 1, then we would have a heavily overlapped receptive field producing extremely large activations. Alternatively, setting the stride to a greater number will reduce the amount of overlapping and produce an output of lower spatial dimensions.\",\n",
      "            \"text\": \"We are also able to define the stride in which we set the depth around the spatial dimensionality of the input in order to place the receptive field. For example if we were to set a stride as 1, then we would have a heavily overlapped receptive field producing extremely large activations. Alternatively, setting the stride to a greater number will reduce the amount of overlapping and produce an output of lower spatial dimensions.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/63\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 7,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.788818359375,\n",
      "                        \"t\": 549.264892578125,\n",
      "                        \"r\": 480.5876159667969,\n",
      "                        \"b\": 514.8633422851562,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        169\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Zero-padding is the simple process of padding the border of the input, and is an effective method to give further control as to the dimensionality of the output volumes.\",\n",
      "            \"text\": \"Zero-padding is the simple process of padding the border of the input, and is an effective method to give further control as to the dimensionality of the output volumes.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/64\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 7,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.85841369628906,\n",
      "                        \"t\": 504.53253173828125,\n",
      "                        \"r\": 480.78466796875,\n",
      "                        \"b\": 469.6905822753906,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        205\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"It is important to understand that through using these techniques, we will alter the spatial dimensionality of the convolutional layers output. To calculate this, you can make use of the following formula:\",\n",
      "            \"text\": \"It is important to understand that through using these techniques, we will alter the spatial dimensionality of the convolutional layers output. To calculate this, you can make use of the following formula:\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/65\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"formula\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 7,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 276.7643127441406,\n",
      "                        \"t\": 446.41644287109375,\n",
      "                        \"r\": 338.3543395996094,\n",
      "                        \"b\": 423.17724609375,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        21\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"( V - R ) + 2 Z S + 1\",\n",
      "            \"text\": \"( V - R ) + 2 Z S + 1\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/66\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 7,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.1217041015625,\n",
      "                        \"t\": 404.2350158691406,\n",
      "                        \"r\": 480.6238708496094,\n",
      "                        \"b\": 345.4455261230469,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        360\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Where V represents the input volume size ( height \\u00d7 width \\u00d7 depth ), R represents the receptive field size, Z is the amount of zero padding set and S referring to the stride. If the calculated result from this equation is not equal to a whole integer then the stride has been incorrectly set, as the neurons will be unable to fit neatly across the given input.\",\n",
      "            \"text\": \"Where V represents the input volume size ( height \\u00d7 width \\u00d7 depth ), R represents the receptive field size, Z is the amount of zero padding set and S referring to the stride. If the calculated result from this equation is not equal to a whole integer then the stride has been incorrectly set, as the neurons will be unable to fit neatly across the given input.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/67\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 7,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.8834686279297,\n",
      "                        \"t\": 334.9963684082031,\n",
      "                        \"r\": 480.5930480957031,\n",
      "                        \"b\": 288.89764404296875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        262\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Despite our best efforts so far we will still find that our models are still enormous if we use an image input of any real dimensionality. However, methods have been developed as to greatly curtail the overall number of parameters within the convolutional layer.\",\n",
      "            \"text\": \"Despite our best efforts so far we will still find that our models are still enormous if we use an image input of any real dimensionality. However, methods have been developed as to greatly curtail the overall number of parameters within the convolutional layer.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/68\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 7,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.8805694580078,\n",
      "                        \"t\": 278.3824462890625,\n",
      "                        \"r\": 480.5939636230469,\n",
      "                        \"b\": 219.0997314453125,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        374\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Parameter sharing works on the assumption that if one region feature is useful to compute at a set spatial region, then it is likely to be useful in another region. If we constrain each individual activation map within the output volume to the same weights and bias, then we will see a massive reduction in the number of parameters being produced by the convolutional layer.\",\n",
      "            \"text\": \"Parameter sharing works on the assumption that if one region feature is useful to compute at a set spatial region, then it is likely to be useful in another region. If we constrain each individual activation map within the output volume to the same weights and bias, then we will see a massive reduction in the number of parameters being produced by the convolutional layer.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/69\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 7,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.85092163085938,\n",
      "                        \"t\": 209.80804443359375,\n",
      "                        \"r\": 480.58685302734375,\n",
      "                        \"b\": 175.01153564453125,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        238\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"As a result of this as the backpropagation stage occurs, each neuron in the output will represent the overall gradient of which can be totalled across the depth - thus only updating a single set of weights, as opposed to every single one.\",\n",
      "            \"text\": \"As a result of this as the backpropagation stage occurs, each neuron in the output will represent the overall gradient of which can be totalled across the depth - thus only updating a single set of weights, as opposed to every single one.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/70\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"page_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 8,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.78009033203125,\n",
      "                        \"t\": 747.8963623046875,\n",
      "                        \"r\": 139.24819946289062,\n",
      "                        \"b\": 738.469482421875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        1\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"8\",\n",
      "            \"text\": \"8\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/71\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"page_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 8,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 167.4324493408203,\n",
      "                        \"t\": 748.35205078125,\n",
      "                        \"r\": 246.2442626953125,\n",
      "                        \"b\": 738.469482421875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        20\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Keiron O\\u2019Shea et al.\",\n",
      "            \"text\": \"Keiron O\\u2019Shea et al.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/72\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"section_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 8,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.94790649414062,\n",
      "                        \"t\": 723.0996704101562,\n",
      "                        \"r\": 218.04713439941406,\n",
      "                        \"b\": 712.1331787109375,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        17\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"2.3 Pooling layer\",\n",
      "            \"text\": \"2.3 Pooling layer\",\n",
      "            \"level\": 1\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/73\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 8,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.07662963867188,\n",
      "                        \"t\": 694.5655517578125,\n",
      "                        \"r\": 480.5867614746094,\n",
      "                        \"b\": 659.6265258789062,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        176\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Pooling layers aim to gradually reduce the dimensionality of the representation, and thus further reduce the number of parameters and the computational complexity of the model.\",\n",
      "            \"text\": \"Pooling layers aim to gradually reduce the dimensionality of the representation, and thus further reduce the number of parameters and the computational complexity of the model.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/74\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 8,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.83888244628906,\n",
      "                        \"t\": 650.9923095703125,\n",
      "                        \"r\": 480.61639404296875,\n",
      "                        \"b\": 580.969482421875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        416\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"The pooling layer operates over each activation map in the input, and scales its dimensionality using the \\\"MAX\\\" function. In most CNNs, these come in the form of max-pooling layers with kernels of a dimensionality of 2 \\u00d7 2 applied with a stride of 2 along the spatial dimensions of the input. This scales the activation map down to 25% of the original size - whilst maintaining the depth volume to its standard size.\",\n",
      "            \"text\": \"The pooling layer operates over each activation map in the input, and scales its dimensionality using the \\\"MAX\\\" function. In most CNNs, these come in the form of max-pooling layers with kernels of a dimensionality of 2 \\u00d7 2 applied with a stride of 2 along the spatial dimensions of the input. This scales the activation map down to 25% of the original size - whilst maintaining the depth volume to its standard size.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/75\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 8,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.8048095703125,\n",
      "                        \"t\": 572.5150146484375,\n",
      "                        \"r\": 480.6969909667969,\n",
      "                        \"b\": 490.258056640625,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        532\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Due to the destructive nature of the pooling layer, there are only two generally observed methods of max-pooling. Usually, the stride and filters of the pooling layers are both set to 2 \\u00d7 2 , which will allow the layer to extend through the entirety of the spatial dimensionality of the input. Furthermore overlapping pooling may be utilised, where the stride is set to 2 with a kernel size set to 3 . Due to the destructive nature of pooling, having a kernel size above 3 will usually greatly decrease the performance of the model.\",\n",
      "            \"text\": \"Due to the destructive nature of the pooling layer, there are only two generally observed methods of max-pooling. Usually, the stride and filters of the pooling layers are both set to 2 \\u00d7 2 , which will allow the layer to extend through the entirety of the spatial dimensionality of the input. Furthermore overlapping pooling may be utilised, where the stride is set to 2 with a kernel size set to 3 . Due to the destructive nature of pooling, having a kernel size above 3 will usually greatly decrease the performance of the model.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/76\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 8,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.9580078125,\n",
      "                        \"t\": 482.387939453125,\n",
      "                        \"r\": 480.59521484375,\n",
      "                        \"b\": 423.36590576171875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        343\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"It is also important to understand that beyond max-pooling, CNN architectures may contain general-pooling. General pooling layers are comprised of pooling neurons that are able to perform a multitude of common operations including L1/L2-normalisation, and average pooling. However, this tutorial will primarily focus on the use of max-pooling.\",\n",
      "            \"text\": \"It is also important to understand that beyond max-pooling, CNN architectures may contain general-pooling. General pooling layers are comprised of pooling neurons that are able to perform a multitude of common operations including L1/L2-normalisation, and average pooling. However, this tutorial will primarily focus on the use of max-pooling.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/77\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"section_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 8,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.02223205566406,\n",
      "                        \"t\": 395.8370666503906,\n",
      "                        \"r\": 255.7871551513672,\n",
      "                        \"b\": 384.8973693847656,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        25\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"2.4 Fully-connected layer\",\n",
      "            \"text\": \"2.4 Fully-connected layer\",\n",
      "            \"level\": 1\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/78\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 8,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.71995544433594,\n",
      "                        \"t\": 367.19287109375,\n",
      "                        \"r\": 480.5868225097656,\n",
      "                        \"b\": 320.27154541015625,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        259\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"The fully-connected layer contains neurons of which are directly connected to the neurons in the two adjacent layers, without being connected to any layers within them. This is analogous to way that neurons are arranged in traditional forms of ANN. (Figure 1)\",\n",
      "            \"text\": \"The fully-connected layer contains neurons of which are directly connected to the neurons in the two adjacent layers, without being connected to any layers within them. This is analogous to way that neurons are arranged in traditional forms of ANN. (Figure 1)\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/79\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"section_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 8,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.84336853027344,\n",
      "                        \"t\": 291.55572509765625,\n",
      "                        \"r\": 195.1865997314453,\n",
      "                        \"b\": 279.1473388671875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        9\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"3 Recipes\",\n",
      "            \"text\": \"3 Recipes\",\n",
      "            \"level\": 1\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/80\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 8,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.87806701660156,\n",
      "                        \"t\": 257.97393798828125,\n",
      "                        \"r\": 480.6416015625,\n",
      "                        \"b\": 174.8902587890625,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        547\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Despite the relatively small number of layers required to form a CNN, there is no set way of formulating a CNN architecture. That being said, it would be idiotic to simply throw a few of layers together and expect it to work. Through reading of related literature it is obvious that much like other forms of ANNs, CNNs tend to follow a common architecture. This common architecture is illustrated in Figure 2, where convolutional layers are stacked, followed by pooling layers in a repeated manner before feeding forward to fully-connected layers.\",\n",
      "            \"text\": \"Despite the relatively small number of layers required to form a CNN, there is no set way of formulating a CNN architecture. That being said, it would be idiotic to simply throw a few of layers together and expect it to work. Through reading of related literature it is obvious that much like other forms of ANNs, CNNs tend to follow a common architecture. This common architecture is illustrated in Figure 2, where convolutional layers are stacked, followed by pooling layers in a repeated manner before feeding forward to fully-connected layers.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/81\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"page_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 9,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 255.96002197265625,\n",
      "                        \"t\": 748.3395385742188,\n",
      "                        \"r\": 447.56597900390625,\n",
      "                        \"b\": 738.469482421875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        45\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Introduction to Convolutional Neural Networks\",\n",
      "            \"text\": \"Introduction to Convolutional Neural Networks\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/82\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"page_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 9,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 474.8158874511719,\n",
      "                        \"t\": 748.0952758789062,\n",
      "                        \"r\": 480.5921630859375,\n",
      "                        \"b\": 738.469482421875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        1\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"9\",\n",
      "            \"text\": \"9\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/83\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 9,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.87208557128906,\n",
      "                        \"t\": 723.253662109375,\n",
      "                        \"r\": 480.66082763671875,\n",
      "                        \"b\": 676.4204711914062,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        263\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Another common CNN architecture is to stack two convolutional layers before each pooling layer, as illustrated in Figure 5. This is strongly recommended as stacking multiple convolutional layers allows for more complex features of the input vector to be selected.\",\n",
      "            \"text\": \"Another common CNN architecture is to stack two convolutional layers before each pooling layer, as illustrated in Figure 5. This is strongly recommended as stacking multiple convolutional layers allows for more complex features of the input vector to be selected.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/84\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"caption\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 9,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.1597137451172,\n",
      "                        \"t\": 553.8919677734375,\n",
      "                        \"r\": 480.640380859375,\n",
      "                        \"b\": 518.9637451171875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        213\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Fig. 5: A common form of CNN architecture in which convolutional layers are stacked between ReLus continuously before being passed through the pooling layer, before going between one or many fully connected ReLus.\",\n",
      "            \"text\": \"Fig. 5: A common form of CNN architecture in which convolutional layers are stacked between ReLus continuously before being passed through the pooling layer, before going between one or many fully connected ReLus.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/85\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 9,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.85641479492188,\n",
      "                        \"t\": 493.7992248535156,\n",
      "                        \"r\": 480.6982727050781,\n",
      "                        \"b\": 363.0449523925781,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        877\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"It is also advised to split large convolutional layers up into many smaller sized convolutional layers. This is to reduce the amount of computational complexity within a given convolutional layer. For example, if you were to stack three convolutional layers on top of each other with a receptive field of 3 \\u00d7 3 . Each neuron of the first convolutional layer will have a 3 \\u00d7 3 view of the input vector. A neuron on the second convolutional layer will then have a 5 \\u00d7 5 view of the input vector. A neuron on the third convolutional layer will then have a 7 \\u00d7 7 view of the input vector. As these stacks feature non-linearities which in turn allows us to express stronger features of the input with fewer parameters. However, it is important to understand that this does come with a distinct memory allocation problem - especially when making use of the backpropagation algorithm.\",\n",
      "            \"text\": \"It is also advised to split large convolutional layers up into many smaller sized convolutional layers. This is to reduce the amount of computational complexity within a given convolutional layer. For example, if you were to stack three convolutional layers on top of each other with a receptive field of 3 \\u00d7 3 . Each neuron of the first convolutional layer will have a 3 \\u00d7 3 view of the input vector. A neuron on the second convolutional layer will then have a 5 \\u00d7 5 view of the input vector. A neuron on the third convolutional layer will then have a 7 \\u00d7 7 view of the input vector. As these stacks feature non-linearities which in turn allows us to express stronger features of the input with fewer parameters. However, it is important to understand that this does come with a distinct memory allocation problem - especially when making use of the backpropagation algorithm.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/86\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 9,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.77923583984375,\n",
      "                        \"t\": 355.1921691894531,\n",
      "                        \"r\": 480.5867614746094,\n",
      "                        \"b\": 332.4426574707031,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        134\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"The input layer should be recursively divisible by two. Common numbers include 32 \\u00d7 32 , 64 \\u00d7 64 , 96 \\u00d7 96 , 128 \\u00d7 128 and 224 \\u00d7 224 .\",\n",
      "            \"text\": \"The input layer should be recursively divisible by two. Common numbers include 32 \\u00d7 32 , 64 \\u00d7 64 , 96 \\u00d7 96 , 128 \\u00d7 128 and 224 \\u00d7 224 .\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/87\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 9,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.83213806152344,\n",
      "                        \"t\": 324.521484375,\n",
      "                        \"r\": 480.5868835449219,\n",
      "                        \"b\": 277.426513671875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        315\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Whilst using small filters, set stride to one and make use of zero-padding as to ensure that the convolutional layers do not reconfigure any of the dimensionality of the input. The amount of zero-padding to be used should be calculated by taking one away from the receptive field size and dividing by two.activation\",\n",
      "            \"text\": \"Whilst using small filters, set stride to one and make use of zero-padding as to ensure that the convolutional layers do not reconfigure any of the dimensionality of the input. The amount of zero-padding to be used should be calculated by taking one away from the receptive field size and dividing by two.activation\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/88\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 9,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.7228240966797,\n",
      "                        \"t\": 269.895751953125,\n",
      "                        \"r\": 480.7103271484375,\n",
      "                        \"b\": 174.67718505859375,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        623\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"CNNs are extremely powerful machine learning algorithms, however they can be horrendously resource-heavy. An example of this problem could be in filtering a large image (anything over 128 \\u00d7 128 could be considered large), so if the input is 227 \\u00d7 227 (as seen with ImageNet) and we're filtering with 64 kernels each with a zero padding of then the result will be three activation vectors of size 227 \\u00d7 227 \\u00d7 64 - which calculates to roughly 10 million activations - or an enormous 70 megabytes of memory per image. In this case you have two options. Firstly, you can reduce the spatial dimensionality of the input images by\",\n",
      "            \"text\": \"CNNs are extremely powerful machine learning algorithms, however they can be horrendously resource-heavy. An example of this problem could be in filtering a large image (anything over 128 \\u00d7 128 could be considered large), so if the input is 227 \\u00d7 227 (as seen with ImageNet) and we're filtering with 64 kernels each with a zero padding of then the result will be three activation vectors of size 227 \\u00d7 227 \\u00d7 64 - which calculates to roughly 10 million activations - or an enormous 70 megabytes of memory per image. In this case you have two options. Firstly, you can reduce the spatial dimensionality of the input images by\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/89\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"page_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 10,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.48631286621094,\n",
      "                        \"t\": 747.890869140625,\n",
      "                        \"r\": 143.73849487304688,\n",
      "                        \"b\": 738.469482421875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        2\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"10\",\n",
      "            \"text\": \"10\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/90\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"page_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 10,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 167.15977478027344,\n",
      "                        \"t\": 748.3831787109375,\n",
      "                        \"r\": 246.2442626953125,\n",
      "                        \"b\": 738.469482421875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        20\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Keiron O\\u2019Shea et al.\",\n",
      "            \"text\": \"Keiron O\\u2019Shea et al.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/91\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 10,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.8414764404297,\n",
      "                        \"t\": 723.1402587890625,\n",
      "                        \"r\": 480.5868835449219,\n",
      "                        \"b\": 688.1945190429688,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        212\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"resizing the raw images to something a little less heavy. Alternatively, you can go against everything we stated earlier in this document and opt for larger filter sizes with a larger stride (2, as opposed to 1).\",\n",
      "            \"text\": \"resizing the raw images to something a little less heavy. Alternatively, you can go against everything we stated earlier in this document and opt for larger filter sizes with a larger stride (2, as opposed to 1).\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/92\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 10,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.21324157714844,\n",
      "                        \"t\": 681.12890625,\n",
      "                        \"r\": 480.5867919921875,\n",
      "                        \"b\": 634.5774536132812,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        266\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"In addition to the few rules-of-thumb outlined above, it is also important to acknowledge a few 'tricks' about generalised ANN training techniques. The authors suggest a read of Geoffrey Hinton's excellent \\\"Practical Guide to Training Restricted Boltzmann Machines\\\".\",\n",
      "            \"text\": \"In addition to the few rules-of-thumb outlined above, it is also important to acknowledge a few 'tricks' about generalised ANN training techniques. The authors suggest a read of Geoffrey Hinton's excellent \\\"Practical Guide to Training Restricted Boltzmann Machines\\\".\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/93\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"section_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 10,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.84384155273438,\n",
      "                        \"t\": 610.5025024414062,\n",
      "                        \"r\": 215.11590576171875,\n",
      "                        \"b\": 598.3948364257812,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        12\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"4 Conclusion\",\n",
      "            \"text\": \"4 Conclusion\",\n",
      "            \"level\": 1\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/94\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 10,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.68148803710938,\n",
      "                        \"t\": 582.052978515625,\n",
      "                        \"r\": 480.5868225097656,\n",
      "                        \"b\": 534.984130859375,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        277\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Convolutional Neural Networks differ to other forms of Artifical Neural Network in that instead of focusing on the entirety of the problem domain, knowledge about the specific type of input is exploited. This in turn allows for a much simpler network architecture to be set up.\",\n",
      "            \"text\": \"Convolutional Neural Networks differ to other forms of Artifical Neural Network in that instead of focusing on the entirety of the problem domain, knowledge about the specific type of input is exploited. This in turn allows for a much simpler network architecture to be set up.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/95\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 10,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.798095703125,\n",
      "                        \"t\": 527.90673828125,\n",
      "                        \"r\": 480.5867919921875,\n",
      "                        \"b\": 493.0619201660156,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        198\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"This paper has outlined the basic concepts of Convolutional Neural Networks, explaining the layers required to build one and detailing how best to structure the network in most image analysis tasks.\",\n",
      "            \"text\": \"This paper has outlined the basic concepts of Convolutional Neural Networks, explaining the layers required to build one and detailing how best to structure the network in most image analysis tasks.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/96\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 10,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.8413543701172,\n",
      "                        \"t\": 485.8362121582031,\n",
      "                        \"r\": 480.8612365722656,\n",
      "                        \"b\": 415.5055236816406,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        394\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Research in the field of image analysis using neural networks has somewhat slowed in recent times. This is partly due to the incorrect belief surrounding the level of complexity and knowledge required to begin modelling these superbly powerful machine learning algorithms. The authors hope that this paper has in some way reduced this confusion, and made the field more accessible to beginners.\",\n",
      "            \"text\": \"Research in the field of image analysis using neural networks has somewhat slowed in recent times. This is partly due to the incorrect belief surrounding the level of complexity and knowledge required to begin modelling these superbly powerful machine learning algorithms. The authors hope that this paper has in some way reduced this confusion, and made the field more accessible to beginners.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/97\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"section_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 10,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.9271240234375,\n",
      "                        \"t\": 391.6942138671875,\n",
      "                        \"r\": 243.6768798828125,\n",
      "                        \"b\": 379.3033142089844,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        16\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Acknowledgements\",\n",
      "            \"text\": \"Acknowledgements\",\n",
      "            \"level\": 1\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/98\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"text\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 10,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.86122131347656,\n",
      "                        \"t\": 362.5135803222656,\n",
      "                        \"r\": 480.82696533203125,\n",
      "                        \"b\": 339.8528747558594,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        105\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"The authors would like to thank Dr. Chuan Lu and Nicholas Dimonaco for useful discussion and suggestions.\",\n",
      "            \"text\": \"The authors would like to thank Dr. Chuan Lu and Nicholas Dimonaco for useful discussion and suggestions.\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/99\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"section_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 10,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.29351806640625,\n",
      "                        \"t\": 315.983154296875,\n",
      "                        \"r\": 194.529052734375,\n",
      "                        \"b\": 303.7138671875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        10\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"References\",\n",
      "            \"text\": \"References\",\n",
      "            \"level\": 1\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/100\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/3\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 10,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 138.7662811279297,\n",
      "                        \"t\": 293.84051513671875,\n",
      "                        \"r\": 480.5946960449219,\n",
      "                        \"b\": 262.0904235839844,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        207\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"1. Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for image classification. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. pp. 3642-3649. IEEE (2012)\",\n",
      "            \"text\": \"1. Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for image classification. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. pp. 3642-3649. IEEE (2012)\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/101\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/3\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 10,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 138.2350311279297,\n",
      "                        \"t\": 261.2042236328125,\n",
      "                        \"r\": 480.658203125,\n",
      "                        \"b\": 218.7664337158203,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        250\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"2. Cires\\u00b8an, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in breast cancer histology images with deep neural networks. In: Medical Image Computing and Computer-Assisted Intervention-MICCAI 2013, pp. 411-418. Springer (2013)\",\n",
      "            \"text\": \"2. Cires\\u00b8an, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in breast cancer histology images with deep neural networks. In: Medical Image Computing and Computer-Assisted Intervention-MICCAI 2013, pp. 411-418. Springer (2013)\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/102\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/3\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 10,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 138.2299041748047,\n",
      "                        \"t\": 217.8837890625,\n",
      "                        \"r\": 480.5948486328125,\n",
      "                        \"b\": 175.30133056640625,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        267\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"3. Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible, high performance convolutional neural networks for image classification. In: IJCAI Proceedings-International Joint Conference on Artificial Intelligence. vol. 22, p. 1237 (2011)\",\n",
      "            \"text\": \"3. Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible, high performance convolutional neural networks for image classification. In: IJCAI Proceedings-International Joint Conference on Artificial Intelligence. vol. 22, p. 1237 (2011)\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/103\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"page_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 11,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 256.0866394042969,\n",
      "                        \"t\": 748.2721557617188,\n",
      "                        \"r\": 447.5473937988281,\n",
      "                        \"b\": 738.469482421875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        45\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"Introduction to Convolutional Neural Networks\",\n",
      "            \"text\": \"Introduction to Convolutional Neural Networks\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/104\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"page_header\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 11,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 471.0235290527344,\n",
      "                        \"t\": 748.2294311523438,\n",
      "                        \"r\": 480.5921325683594,\n",
      "                        \"b\": 738.469482421875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        2\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"11\",\n",
      "            \"text\": \"11\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/105\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/4\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 11,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 138.15142822265625,\n",
      "                        \"t\": 722.5249633789062,\n",
      "                        \"r\": 480.60302734375,\n",
      "                        \"b\": 679.6904907226562,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        255\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"4. Cires\\u00b8an, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural network committees for handwritten character classification. In: Document Analysis and Recognition (ICDAR), 2011 International Conference on. pp. 1135-1139. IEEE (2011)\",\n",
      "            \"text\": \"4. Cires\\u00b8an, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural network committees for handwritten character classification. In: Document Analysis and Recognition (ICDAR), 2011 International Conference on. pp. 1135-1139. IEEE (2011)\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/106\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/4\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 11,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 138.33560180664062,\n",
      "                        \"t\": 678.4413452148438,\n",
      "                        \"r\": 480.71539306640625,\n",
      "                        \"b\": 657.7725219726562,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        143\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"5. Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural networksa review. Pattern recognition 35(10), 2279-2301 (2002)\",\n",
      "            \"text\": \"5. Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural networksa review. Pattern recognition 35(10), 2279-2301 (2002)\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/107\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/4\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 11,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 138.4720458984375,\n",
      "                        \"t\": 656.4887084960938,\n",
      "                        \"r\": 480.6998291015625,\n",
      "                        \"b\": 613.9365844726562,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        274\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"6. Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware accelerated convolutional neural networks for synthetic vision systems. In: Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp. 257-260. IEEE (2010)\",\n",
      "            \"text\": \"6. Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware accelerated convolutional neural networks for synthetic vision systems. In: Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp. 257-260. IEEE (2010)\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/108\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/4\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 11,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 138.4396514892578,\n",
      "                        \"t\": 612.5001220703125,\n",
      "                        \"r\": 480.5898132324219,\n",
      "                        \"b\": 592.0186157226562,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        101\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"7. Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum 9(1), 926 (2010)\",\n",
      "            \"text\": \"7. Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum 9(1), 926 (2010)\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/109\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/4\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 11,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 138.49429321289062,\n",
      "                        \"t\": 590.37109375,\n",
      "                        \"r\": 480.5947265625,\n",
      "                        \"b\": 559.1426391601562,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        198\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"8. Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 (2012)\",\n",
      "            \"text\": \"8. Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 (2012)\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/110\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/4\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 11,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 138.10630798339844,\n",
      "                        \"t\": 557.6162109375,\n",
      "                        \"r\": 480.5946960449219,\n",
      "                        \"b\": 526.2656860351562,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        185\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"9. Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1), 221-231 (2013)\",\n",
      "            \"text\": \"9. Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1), 221-231 (2013)\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/111\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/4\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 11,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.7255096435547,\n",
      "                        \"t\": 525.0989990234375,\n",
      "                        \"r\": 480.77935791015625,\n",
      "                        \"b\": 482.4297180175781,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        255\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"10. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Largescale video classification with convolutional neural networks. In: Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725-1732. IEEE (2014)\",\n",
      "            \"text\": \"10. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Largescale video classification with convolutional neural networks. In: Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725-1732. IEEE (2014)\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/112\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/4\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 11,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.47573852539062,\n",
      "                        \"t\": 481.14715576171875,\n",
      "                        \"r\": 480.59478759765625,\n",
      "                        \"b\": 449.5537414550781,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        189\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"11. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems. pp. 1097-1105 (2012)\",\n",
      "            \"text\": \"11. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems. pp. 1097-1105 (2012)\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/113\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/4\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 11,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.62811279296875,\n",
      "                        \"t\": 448.33197021484375,\n",
      "                        \"r\": 480.59466552734375,\n",
      "                        \"b\": 416.6767883300781,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        196\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"12. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural computation 1(4), 541-551 (1989)\",\n",
      "            \"text\": \"12. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural computation 1(4), 541-551 (1989)\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/114\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/4\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 11,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.47447204589844,\n",
      "                        \"t\": 415.2291564941406,\n",
      "                        \"r\": 480.590087890625,\n",
      "                        \"b\": 394.7588195800781,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        157\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"13. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998)\",\n",
      "            \"text\": \"13. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998)\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/115\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/4\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 11,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.4318084716797,\n",
      "                        \"t\": 393.6202087402344,\n",
      "                        \"r\": 480.5900573730469,\n",
      "                        \"b\": 372.8408508300781,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        143\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"14. Nebauer, C.: Evaluation of convolutional neural networks for visual recognition. Neural Networks, IEEE Transactions on 9(4), 685-696 (1998)\",\n",
      "            \"text\": \"14. Nebauer, C.: Evaluation of convolutional neural networks for visual recognition. Neural Networks, IEEE Transactions on 9(4), 685-696 (1998)\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/116\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/4\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 11,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.57110595703125,\n",
      "                        \"t\": 371.8392028808594,\n",
      "                        \"r\": 480.6988525390625,\n",
      "                        \"b\": 350.9228820800781,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        162\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"15. Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural networks applied to visual document analysis. In: null. p. 958. IEEE (2003)\",\n",
      "            \"text\": \"15. Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural networks applied to visual document analysis. In: null. p. 958. IEEE (2003)\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/117\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/4\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 11,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.7103729248047,\n",
      "                        \"t\": 349.75054931640625,\n",
      "                        \"r\": 480.5900573730469,\n",
      "                        \"b\": 329.0058898925781,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        102\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"16. Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of Toronto (2013)\",\n",
      "            \"text\": \"16. Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of Toronto (2013)\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/118\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/4\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 11,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.64859008789062,\n",
      "                        \"t\": 327.69183349609375,\n",
      "                        \"r\": 480.5948181152344,\n",
      "                        \"b\": 296.1289367675781,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        199\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"17. Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with convolutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings. IEEE. pp. 224-229. IEEE (2005)\",\n",
      "            \"text\": \"17. Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with convolutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings. IEEE. pp. 224-229. IEEE (2005)\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/119\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/4\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 11,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.7650146484375,\n",
      "                        \"t\": 295.0496826171875,\n",
      "                        \"r\": 480.59002685546875,\n",
      "                        \"b\": 274.2109680175781,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        158\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"18. Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In: Advances in Neural Information Processing Systems. pp. 2553-2561 (2013)\",\n",
      "            \"text\": \"18. Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In: Advances in Neural Information Processing Systems. pp. 2553-2561 (2013)\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/120\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/4\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 11,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.6785888671875,\n",
      "                        \"t\": 273.25,\n",
      "                        \"r\": 480.60009765625,\n",
      "                        \"b\": 241.1690673828125,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        249\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"19. Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks (siconnets) and their application of face detection. In: Neural Networks, 2003. Proceedings of the International Joint Conference on. vol. 3, pp. 2157-2162. IEEE (2003)\",\n",
      "            \"text\": \"19. Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks (siconnets) and their application of face detection. In: Neural Networks, 2003. Proceedings of the International Joint Conference on. vol. 3, pp. 2157-2162. IEEE (2003)\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/121\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/4\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 11,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.3703155517578,\n",
      "                        \"t\": 240.287353515625,\n",
      "                        \"r\": 480.590087890625,\n",
      "                        \"b\": 219.4169921875,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        144\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"20. Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional neural networks. arXiv preprint arXiv:1301.3557 (2013)\",\n",
      "            \"text\": \"20. Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional neural networks. arXiv preprint arXiv:1301.3557 (2013)\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/texts/122\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/groups/4\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"list_item\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 11,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 134.26535034179688,\n",
      "                        \"t\": 218.36279296875,\n",
      "                        \"r\": 480.59014892578125,\n",
      "                        \"b\": 197.0758056640625,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        143\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"orig\": \"21. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In: Computer Vision-ECCV 2014, pp. 818-833. Springer (2014)\",\n",
      "            \"text\": \"21. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In: Computer Vision-ECCV 2014, pp. 818-833. Springer (2014)\",\n",
      "            \"enumerated\": false,\n",
      "            \"marker\": \"-\"\n",
      "        }\n",
      "    ],\n",
      "    \"pictures\": [\n",
      "        {\n",
      "            \"self_ref\": \"#/pictures/0\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"picture\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 2,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 193.8206024169922,\n",
      "                        \"t\": 725.920654296875,\n",
      "                        \"r\": 420.3355407714844,\n",
      "                        \"b\": 574.5050659179688,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        335\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"captions\": [\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/14\"\n",
      "                }\n",
      "            ],\n",
      "            \"references\": [],\n",
      "            \"footnotes\": [],\n",
      "            \"annotations\": []\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/pictures/1\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"picture\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 4,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 194.3790283203125,\n",
      "                        \"t\": 452.28253173828125,\n",
      "                        \"r\": 421.985595703125,\n",
      "                        \"b\": 329.37823486328125,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        65\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"captions\": [\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/36\"\n",
      "                }\n",
      "            ],\n",
      "            \"references\": [],\n",
      "            \"footnotes\": [],\n",
      "            \"annotations\": []\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/pictures/2\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"picture\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 5,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 135.41848754882812,\n",
      "                        \"t\": 533.9510498046875,\n",
      "                        \"r\": 479.3868713378906,\n",
      "                        \"b\": 399.3819274902344,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        279\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"captions\": [\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/46\"\n",
      "                }\n",
      "            ],\n",
      "            \"references\": [],\n",
      "            \"footnotes\": [],\n",
      "            \"annotations\": []\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/pictures/3\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"picture\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 6,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 168.14492797851562,\n",
      "                        \"t\": 584.790283203125,\n",
      "                        \"r\": 446.1294250488281,\n",
      "                        \"b\": 497.7923889160156,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        217\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"captions\": [\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/54\"\n",
      "                }\n",
      "            ],\n",
      "            \"references\": [],\n",
      "            \"footnotes\": [],\n",
      "            \"annotations\": []\n",
      "        },\n",
      "        {\n",
      "            \"self_ref\": \"#/pictures/4\",\n",
      "            \"parent\": {\n",
      "                \"$ref\": \"#/body\"\n",
      "            },\n",
      "            \"children\": [],\n",
      "            \"label\": \"picture\",\n",
      "            \"prov\": [\n",
      "                {\n",
      "                    \"page_no\": 9,\n",
      "                    \"bbox\": {\n",
      "                        \"l\": 133.3032989501953,\n",
      "                        \"t\": 653.78076171875,\n",
      "                        \"r\": 481.8245544433594,\n",
      "                        \"b\": 564.6818237304688,\n",
      "                        \"coord_origin\": \"BOTTOMLEFT\"\n",
      "                    },\n",
      "                    \"charspan\": [\n",
      "                        0,\n",
      "                        213\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"captions\": [\n",
      "                {\n",
      "                    \"$ref\": \"#/texts/84\"\n",
      "                }\n",
      "            ],\n",
      "            \"references\": [],\n",
      "            \"footnotes\": [],\n",
      "            \"annotations\": []\n",
      "        }\n",
      "    ],\n",
      "    \"tables\": [],\n",
      "    \"key_value_items\": [],\n",
      "    \"pages\": {\n",
      "        \"1\": {\n",
      "            \"size\": {\n",
      "                \"width\": 595.2760009765625,\n",
      "                \"height\": 841.8900146484375\n",
      "            },\n",
      "            \"page_no\": 1\n",
      "        },\n",
      "        \"2\": {\n",
      "            \"size\": {\n",
      "                \"width\": 595.2760009765625,\n",
      "                \"height\": 841.8900146484375\n",
      "            },\n",
      "            \"page_no\": 2\n",
      "        },\n",
      "        \"3\": {\n",
      "            \"size\": {\n",
      "                \"width\": 595.2760009765625,\n",
      "                \"height\": 841.8900146484375\n",
      "            },\n",
      "            \"page_no\": 3\n",
      "        },\n",
      "        \"4\": {\n",
      "            \"size\": {\n",
      "                \"width\": 595.2760009765625,\n",
      "                \"height\": 841.8900146484375\n",
      "            },\n",
      "            \"page_no\": 4\n",
      "        },\n",
      "        \"5\": {\n",
      "            \"size\": {\n",
      "                \"width\": 595.2760009765625,\n",
      "                \"height\": 841.8900146484375\n",
      "            },\n",
      "            \"page_no\": 5\n",
      "        },\n",
      "        \"6\": {\n",
      "            \"size\": {\n",
      "                \"width\": 595.2760009765625,\n",
      "                \"height\": 841.8900146484375\n",
      "            },\n",
      "            \"page_no\": 6\n",
      "        },\n",
      "        \"7\": {\n",
      "            \"size\": {\n",
      "                \"width\": 595.2760009765625,\n",
      "                \"height\": 841.8900146484375\n",
      "            },\n",
      "            \"page_no\": 7\n",
      "        },\n",
      "        \"8\": {\n",
      "            \"size\": {\n",
      "                \"width\": 595.2760009765625,\n",
      "                \"height\": 841.8900146484375\n",
      "            },\n",
      "            \"page_no\": 8\n",
      "        },\n",
      "        \"9\": {\n",
      "            \"size\": {\n",
      "                \"width\": 595.2760009765625,\n",
      "                \"height\": 841.8900146484375\n",
      "            },\n",
      "            \"page_no\": 9\n",
      "        },\n",
      "        \"10\": {\n",
      "            \"size\": {\n",
      "                \"width\": 595.2760009765625,\n",
      "                \"height\": 841.8900146484375\n",
      "            },\n",
      "            \"page_no\": 10\n",
      "        },\n",
      "        \"11\": {\n",
      "            \"size\": {\n",
      "                \"width\": 595.2760009765625,\n",
      "                \"height\": 841.8900146484375\n",
      "            },\n",
      "            \"page_no\": 11\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "doc_json = json.dumps(conv_result.document.export_to_dict(), indent=4)\n",
    "print(\"JSON Output:\")\n",
    "print(doc_json)\n",
    "\n",
    "# Optionally save to a file\n",
    "with open(\"output.json\", \"w\") as json_file:\n",
    "    json_file.write(doc_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown Output:\n",
      "## An Introduction to Convolutional Neural Networks\n",
      "\n",
      "Keiron O'Shea 1 and Ryan Nash 2\n",
      "\n",
      "- 1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB keo7@aber.ac.uk\n",
      "- 2 School of Computing and Communications, Lancaster University, Lancashire, LA1 4YW\n",
      "\n",
      "nashrd@live.lancs.ac.uk\n",
      "\n",
      "Abstract. The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs.\n",
      "\n",
      "This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.\n",
      "\n",
      "Keywords: Pattern recognition, artificial neural networks, machine learning, image analysis\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Artificial Neural Networks (ANNs) are computational processing systems of which are heavily inspired by way biological nervous systems (such as the human brain) operate. ANNs are mainly comprised of a high number of interconnected computational nodes (referred to as neurons), of which work entwine in a distributed fashion to collectively learn from the input in order to optimise its final output.\n",
      "\n",
      "The basic structure of a ANN can be modelled as shown in Figure 1. We would load the input, usually in the form of a multidimensional vector to the input layer of which will distribute it to the hidden layers. The hidden layers will then make decisions from the previous layer and weigh up how a stochastic change within itself detriments or improves the final output, and this is referred to as the process of learning. Having multiple hidden layers stacked upon each-other is commonly called deep learning.\n",
      "\n",
      "Fig. 1: A simple three layered feedforward neural network (FNN), comprised of a input layer, a hidden layer and an output layer. This structure is the basis of a number of common ANN architectures, included but not limited to Feedforward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and Recurrent Neural Networks (RNNs).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "The two key learning paradigms in image processing tasks are supervised and unsupervised learning. Supervised learning is learning through pre-labelled inputs, which act as targets. For each training example there will be a set of input values (vectors) and one or more associated designated output values. The goal of this form of training is to reduce the models overall classification error, through correct calculation of the output value of training example by training.\n",
      "\n",
      "Unsupervised learning differs in that the training set does not include any labels. Success is usually determined by whether the network is able to reduce or increase an associated cost function. However, it is important to note that most image-focused pattern-recognition tasks usually depend on classification using supervised learning.\n",
      "\n",
      "Convolutional Neural Networks (CNNs) are analogous to traditional ANNs in that they are comprised of neurons that self-optimise through learning. Each neuron will still receive an input and perform a operation (such as a scalar product followed by a non-linear function) - the basis of countless ANNs. From the input raw image vectors to the final output of the class score, the entire of the network will still express a single perceptive score function (the weight). The last layer will contain loss functions associated with the classes, and all of the regular tips and tricks developed for traditional ANNs still apply.\n",
      "\n",
      "The only notable difference between CNNs and traditional ANNs is that CNNs are primarily used in the field of pattern recognition within images. This allows us to encode image-specific features into the architecture, making the network\n",
      "\n",
      "more suited for image-focused tasks - whilst further reducing the parameters required to set up the model.\n",
      "\n",
      "One of the largest limitations of traditional forms of ANN is that they tend to struggle with the computational complexity required to compute image data. Common machine learning benchmarking datasets such as the MNIST database of handwritten digits are suitable for most forms of ANN, due to its relatively small image dimensionality of just 28 × 28 . With this dataset a single neuron in the first hidden layer will contain 784 weights ( 28 × 28 × 1 where 1 bare in mind that MNIST is normalised to just black and white values), which is manageable for most forms of ANN.\n",
      "\n",
      "If you consider a more substantial coloured image input of 64 × 64 , the number of weights on just a single neuron of the first layer increases substantially to 12 , 288 . Also take into account that to deal with this scale of input, the network will also need to be a lot larger than one used to classify colour-normalised MNIST digits, then you will understand the drawbacks of using such models.\n",
      "\n",
      "## 1.1 Overfitting\n",
      "\n",
      "But why does it matter? Surely we could just increase the number of hidden layers in our network, and perhaps increase the number of neurons within them? The simple answer to this question is no. This is down to two reasons, one being the simple problem of not having unlimited computational power and time to train these huge ANNs.\n",
      "\n",
      "The second reason is stopping or reducing the effects of overfitting. Overfitting is basically when a network is unable to learn effectively due to a number of reasons. It is an important concept of most, if not all machine learning algorithms and it is important that every precaution is taken as to reduce its effects. If our models were to exhibit signs of overfitting then we may see a reduced ability to pinpoint generalised features for not only our training dataset, but also our test and prediction sets.\n",
      "\n",
      "This is the main reason behind reducing the complexity of our ANNs. The less parameters required to train, the less likely the network will overfit - and of course, improve the predictive performance of the model.\n",
      "\n",
      "## 2 CNN architecture\n",
      "\n",
      "As noted earlier, CNNs primarily focus on the basis that the input will be comprised of images. This focuses the architecture to be set up in way to best suit the need for dealing with the specific type of data.\n",
      "\n",
      "Keiron O'Shea et al.\n",
      "\n",
      "One of the key differences is that the neurons that the layers within the CNN are comprised of neurons organised into three dimensions, the spatial dimensionality of the input ( height and the width ) and the depth . The depth does not refer to the total number of layers within the ANN, but the third dimension of a activation volume. Unlike standard ANNS, the neurons within any given layer will only connect to a small region of the layer preceding it.\n",
      "\n",
      "In practice this would mean that for the example given earlier, the input 'volume' will have a dimensionality of 64 × 64 × 3 (height, width and depth), leading to a final output layer comprised of a dimensionality of 1 × 1 × n (where n represents the possible number of classes) as we would have condensed the full input dimensionality into a smaller volume of class scores filed across the depth dimension.\n",
      "\n",
      "## 2.1 Overall architecture\n",
      "\n",
      "CNNs are comprised of three types of layers. These are convolutional layers, pooling layers and fully-connected layers . When these layers are stacked, a CNN architecture has been formed. A simplified CNN architecture for MNIST classification is illustrated in Figure 2.\n",
      "\n",
      "Fig. 2: An simple CNN architecture, comprised of just five layers\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "The basic functionality of the example CNN above can be broken down into four key areas.\n",
      "\n",
      "- 1. As found in other forms of ANN, the input layer will hold the pixel values of the image.\n",
      "- 2. The convolutional layer will determine the output of neurons of which are connected to local regions of the input through the calculation of the scalar product between their weights and the region connected to the input volume. The rectified linear unit (commonly shortened to ReLu) aims to apply\n",
      "\n",
      "- an 'elementwise' activation function such as sigmoid to the output of the activation produced by the previous layer.\n",
      "- 3. The pooling layer will then simply perform downsampling along the spatial dimensionality of the given input, further reducing the number of parameters within that activation.\n",
      "- 4. The fully-connected layers will then perform the same duties found in standard ANNs and attempt to produce class scores from the activations, to be used for classification. It is also suggested that ReLu may be used between these layers, as to improve performance.\n",
      "\n",
      "Through this simple method of transformation, CNNs are able to transform the original input layer by layer using convolutional and downsampling techniques to produce class scores for classification and regression purposes.\n",
      "\n",
      "Fig. 3: Activations taken from the first convolutional layer of a simplistic deep CNN, after training on the MNIST database of handwritten digits. If you look carefully, you can see that the network has successfully picked up on characteristics unique to specific numeric digits.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "However, it is important to note that simply understanding the overall architecture of a CNN architecture will not suffice. The creation and optimisation of these models can take quite some time, and can be quite confusing. We will now explore in detail the individual layers, detailing their hyperparameters and connectivities.\n",
      "\n",
      "## 2.2 Convolutional layer\n",
      "\n",
      "As the name implies, the convolutional layer plays a vital role in how CNNs operate. The layers parameters focus around the use of learnable kernels .\n",
      "\n",
      "These kernels are usually small in spatial dimensionality, but spreads along the entirety of the depth of the input. When the data hits a convolutional layer, the layer convolves each filter across the spatial dimensionality of the input to produce a 2D activation map. These activation maps can be visualised, as seen in Figure 3.\n",
      "\n",
      "As we glide through the input, the scalar product is calculated for each value in that kernel. (Figure 4) From this the network will learn kernels that 'fire' when they see a specific feature at a given spatial position of the input. These are commonly known as activations .\n",
      "\n",
      "Fig. 4: A visual representation of a convolutional layer. The centre element of the kernel is placed over the input vector, of which is then calculated and replaced with a weighted sum of itself and any nearby pixels.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Every kernel will have a corresponding activation map, of which will be stacked along the depth dimension to form the full output volume from the convolutional layer.\n",
      "\n",
      "As we alluded to earlier, training ANNs on inputs such as images results in models of which are too big to train effectively. This comes down to the fullyconnected manner of standard ANN neurons, so to mitigate against this every neuron in a convolutional layer is only connected to small region of the input volume. The dimensionality of this region is commonly referred to as the receptive field size of the neuron. The magnitude of the connectivity through the depth is nearly always equal to the depth of the input.\n",
      "\n",
      "For example, if the input to the network is an image of size 64 × 64 × 3 (a RGBcoloured image with a dimensionality of 64 × 64 ) and we set the receptive field size as 6 × 6 , we would have a total of 108 weights on each neuron within the convolutional layer. ( 6 × 6 × 3 where 3 is the magnitude of connectivity across the depth of the volume) To put this into perspective, a standard neuron seen in other forms of ANN would contain 12 , 288 weights each.\n",
      "\n",
      "Convolutional layers are also able to significantly reduce the complexity of the model through the optimisation of its output. These are optimised through three hyperparameters, the depth , the stride and setting zero-padding .\n",
      "\n",
      "The depth of the output volume produced by the convolutional layers can be manually set through the number of neurons within the layer to a the same region of the input. This can be seen with other forms of ANNs, where the all of the neurons in the hidden layer are directly connected to every single neuron beforehand. Reducing this hyperparameter can significantly minimise the total number of neurons of the network, but it can also significantly reduce the pattern recognition capabilities of the model.\n",
      "\n",
      "We are also able to define the stride in which we set the depth around the spatial dimensionality of the input in order to place the receptive field. For example if we were to set a stride as 1, then we would have a heavily overlapped receptive field producing extremely large activations. Alternatively, setting the stride to a greater number will reduce the amount of overlapping and produce an output of lower spatial dimensions.\n",
      "\n",
      "Zero-padding is the simple process of padding the border of the input, and is an effective method to give further control as to the dimensionality of the output volumes.\n",
      "\n",
      "It is important to understand that through using these techniques, we will alter the spatial dimensionality of the convolutional layers output. To calculate this, you can make use of the following formula:\n",
      "\n",
      "( V - R ) + 2 Z S + 1\n",
      "\n",
      "Where V represents the input volume size ( height × width × depth ), R represents the receptive field size, Z is the amount of zero padding set and S referring to the stride. If the calculated result from this equation is not equal to a whole integer then the stride has been incorrectly set, as the neurons will be unable to fit neatly across the given input.\n",
      "\n",
      "Despite our best efforts so far we will still find that our models are still enormous if we use an image input of any real dimensionality. However, methods have been developed as to greatly curtail the overall number of parameters within the convolutional layer.\n",
      "\n",
      "Parameter sharing works on the assumption that if one region feature is useful to compute at a set spatial region, then it is likely to be useful in another region. If we constrain each individual activation map within the output volume to the same weights and bias, then we will see a massive reduction in the number of parameters being produced by the convolutional layer.\n",
      "\n",
      "As a result of this as the backpropagation stage occurs, each neuron in the output will represent the overall gradient of which can be totalled across the depth - thus only updating a single set of weights, as opposed to every single one.\n",
      "\n",
      "## 2.3 Pooling layer\n",
      "\n",
      "Pooling layers aim to gradually reduce the dimensionality of the representation, and thus further reduce the number of parameters and the computational complexity of the model.\n",
      "\n",
      "The pooling layer operates over each activation map in the input, and scales its dimensionality using the \"MAX\" function. In most CNNs, these come in the form of max-pooling layers with kernels of a dimensionality of 2 × 2 applied with a stride of 2 along the spatial dimensions of the input. This scales the activation map down to 25% of the original size - whilst maintaining the depth volume to its standard size.\n",
      "\n",
      "Due to the destructive nature of the pooling layer, there are only two generally observed methods of max-pooling. Usually, the stride and filters of the pooling layers are both set to 2 × 2 , which will allow the layer to extend through the entirety of the spatial dimensionality of the input. Furthermore overlapping pooling may be utilised, where the stride is set to 2 with a kernel size set to 3 . Due to the destructive nature of pooling, having a kernel size above 3 will usually greatly decrease the performance of the model.\n",
      "\n",
      "It is also important to understand that beyond max-pooling, CNN architectures may contain general-pooling. General pooling layers are comprised of pooling neurons that are able to perform a multitude of common operations including L1/L2-normalisation, and average pooling. However, this tutorial will primarily focus on the use of max-pooling.\n",
      "\n",
      "## 2.4 Fully-connected layer\n",
      "\n",
      "The fully-connected layer contains neurons of which are directly connected to the neurons in the two adjacent layers, without being connected to any layers within them. This is analogous to way that neurons are arranged in traditional forms of ANN. (Figure 1)\n",
      "\n",
      "## 3 Recipes\n",
      "\n",
      "Despite the relatively small number of layers required to form a CNN, there is no set way of formulating a CNN architecture. That being said, it would be idiotic to simply throw a few of layers together and expect it to work. Through reading of related literature it is obvious that much like other forms of ANNs, CNNs tend to follow a common architecture. This common architecture is illustrated in Figure 2, where convolutional layers are stacked, followed by pooling layers in a repeated manner before feeding forward to fully-connected layers.\n",
      "\n",
      "Another common CNN architecture is to stack two convolutional layers before each pooling layer, as illustrated in Figure 5. This is strongly recommended as stacking multiple convolutional layers allows for more complex features of the input vector to be selected.\n",
      "\n",
      "Fig. 5: A common form of CNN architecture in which convolutional layers are stacked between ReLus continuously before being passed through the pooling layer, before going between one or many fully connected ReLus.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "It is also advised to split large convolutional layers up into many smaller sized convolutional layers. This is to reduce the amount of computational complexity within a given convolutional layer. For example, if you were to stack three convolutional layers on top of each other with a receptive field of 3 × 3 . Each neuron of the first convolutional layer will have a 3 × 3 view of the input vector. A neuron on the second convolutional layer will then have a 5 × 5 view of the input vector. A neuron on the third convolutional layer will then have a 7 × 7 view of the input vector. As these stacks feature non-linearities which in turn allows us to express stronger features of the input with fewer parameters. However, it is important to understand that this does come with a distinct memory allocation problem - especially when making use of the backpropagation algorithm.\n",
      "\n",
      "The input layer should be recursively divisible by two. Common numbers include 32 × 32 , 64 × 64 , 96 × 96 , 128 × 128 and 224 × 224 .\n",
      "\n",
      "Whilst using small filters, set stride to one and make use of zero-padding as to ensure that the convolutional layers do not reconfigure any of the dimensionality of the input. The amount of zero-padding to be used should be calculated by taking one away from the receptive field size and dividing by two.activation\n",
      "\n",
      "CNNs are extremely powerful machine learning algorithms, however they can be horrendously resource-heavy. An example of this problem could be in filtering a large image (anything over 128 × 128 could be considered large), so if the input is 227 × 227 (as seen with ImageNet) and we're filtering with 64 kernels each with a zero padding of then the result will be three activation vectors of size 227 × 227 × 64 - which calculates to roughly 10 million activations - or an enormous 70 megabytes of memory per image. In this case you have two options. Firstly, you can reduce the spatial dimensionality of the input images by\n",
      "\n",
      "resizing the raw images to something a little less heavy. Alternatively, you can go against everything we stated earlier in this document and opt for larger filter sizes with a larger stride (2, as opposed to 1).\n",
      "\n",
      "In addition to the few rules-of-thumb outlined above, it is also important to acknowledge a few 'tricks' about generalised ANN training techniques. The authors suggest a read of Geoffrey Hinton's excellent \"Practical Guide to Training Restricted Boltzmann Machines\".\n",
      "\n",
      "## 4 Conclusion\n",
      "\n",
      "Convolutional Neural Networks differ to other forms of Artifical Neural Network in that instead of focusing on the entirety of the problem domain, knowledge about the specific type of input is exploited. This in turn allows for a much simpler network architecture to be set up.\n",
      "\n",
      "This paper has outlined the basic concepts of Convolutional Neural Networks, explaining the layers required to build one and detailing how best to structure the network in most image analysis tasks.\n",
      "\n",
      "Research in the field of image analysis using neural networks has somewhat slowed in recent times. This is partly due to the incorrect belief surrounding the level of complexity and knowledge required to begin modelling these superbly powerful machine learning algorithms. The authors hope that this paper has in some way reduced this confusion, and made the field more accessible to beginners.\n",
      "\n",
      "## Acknowledgements\n",
      "\n",
      "The authors would like to thank Dr. Chuan Lu and Nicholas Dimonaco for useful discussion and suggestions.\n",
      "\n",
      "## References\n",
      "\n",
      "- 1. Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for image classification. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. pp. 3642-3649. IEEE (2012)\n",
      "- 2. Cires¸an, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in breast cancer histology images with deep neural networks. In: Medical Image Computing and Computer-Assisted Intervention-MICCAI 2013, pp. 411-418. Springer (2013)\n",
      "- 3. Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible, high performance convolutional neural networks for image classification. In: IJCAI Proceedings-International Joint Conference on Artificial Intelligence. vol. 22, p. 1237 (2011)\n",
      "\n",
      "- 4. Cires¸an, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural network committees for handwritten character classification. In: Document Analysis and Recognition (ICDAR), 2011 International Conference on. pp. 1135-1139. IEEE (2011)\n",
      "- 5. Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural networksa review. Pattern recognition 35(10), 2279-2301 (2002)\n",
      "- 6. Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware accelerated convolutional neural networks for synthetic vision systems. In: Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp. 257-260. IEEE (2010)\n",
      "- 7. Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum 9(1), 926 (2010)\n",
      "- 8. Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 (2012)\n",
      "- 9. Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1), 221-231 (2013)\n",
      "- 10. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Largescale video classification with convolutional neural networks. In: Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725-1732. IEEE (2014)\n",
      "- 11. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems. pp. 1097-1105 (2012)\n",
      "- 12. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural computation 1(4), 541-551 (1989)\n",
      "- 13. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998)\n",
      "- 14. Nebauer, C.: Evaluation of convolutional neural networks for visual recognition. Neural Networks, IEEE Transactions on 9(4), 685-696 (1998)\n",
      "- 15. Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural networks applied to visual document analysis. In: null. p. 958. IEEE (2003)\n",
      "- 16. Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of Toronto (2013)\n",
      "- 17. Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with convolutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings. IEEE. pp. 224-229. IEEE (2005)\n",
      "- 18. Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In: Advances in Neural Information Processing Systems. pp. 2553-2561 (2013)\n",
      "- 19. Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks (siconnets) and their application of face detection. In: Neural Networks, 2003. Proceedings of the International Joint Conference on. vol. 3, pp. 2157-2162. IEEE (2003)\n",
      "- 20. Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional neural networks. arXiv preprint arXiv:1301.3557 (2013)\n",
      "- 21. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In: Computer Vision-ECCV 2014, pp. 818-833. Springer (2014)\n"
     ]
    }
   ],
   "source": [
    "doc_markdown = conv_result.document.export_to_markdown()\n",
    "print(\"Markdown Output:\")\n",
    "print(doc_markdown)\n",
    "\n",
    "# Optionally save to a file\n",
    "with open(\"output.md\", \"w\") as md_file:\n",
    "    md_file.write(doc_markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks available: 88\n",
      "Chunk 0: arXiv:1511.08458v2 [cs.NE] 2 Dec 2015...\n",
      "Chunk 1: Keiron O'Shea 1 and Ryan Nash 2...\n",
      "Chunk 2: 1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB keo7@aber.ac.uk\n",
      "2 Sch...\n",
      "Chunk 3: nashrd@live.lancs.ac.uk...\n",
      "Chunk 4: Abstract. The field of machine learning has taken a dramatic twist in recent times, with the rise of...\n",
      "Chunk 5: This document provides a brief introduction to CNNs, discussing recently published papers and newly ...\n",
      "Chunk 6: Keywords: Pattern recognition, artificial neural networks, machine learning, image analysis...\n",
      "Chunk 7: Artificial Neural Networks (ANNs) are computational processing systems of which are heavily inspired...\n",
      "Chunk 8: The basic structure of a ANN can be modelled as shown in Figure 1. We would load the input, usually ...\n",
      "Chunk 9: 2...\n",
      "Chunk 10: Keiron O’Shea et al....\n",
      "Chunk 11: Fig. 1: A simple three layered feedforward neural network (FNN), comprised of a input layer, a hidde...\n",
      "Chunk 12: The two key learning paradigms in image processing tasks are supervised and unsupervised learning. S...\n",
      "Chunk 13: Unsupervised learning differs in that the training set does not include any labels. Success is usual...\n",
      "Chunk 14: Convolutional Neural Networks (CNNs) are analogous to traditional ANNs in that they are comprised of...\n",
      "Chunk 15: The only notable difference between CNNs and traditional ANNs is that CNNs are primarily used in the...\n",
      "Chunk 16: Introduction to Convolutional Neural Networks...\n",
      "Chunk 17: 3...\n",
      "Chunk 18: more suited for image-focused tasks - whilst further reducing the parameters required to set up the ...\n",
      "Chunk 19: One of the largest limitations of traditional forms of ANN is that they tend to struggle with the co...\n",
      "Chunk 20: If you consider a more substantial coloured image input of 64 × 64 , the number of weights on just a...\n",
      "Chunk 21: But why does it matter? Surely we could just increase the number of hidden layers in our network, an...\n",
      "Chunk 22: The second reason is stopping or reducing the effects of overfitting. Overfitting is basically when ...\n",
      "Chunk 23: This is the main reason behind reducing the complexity of our ANNs. The less parameters required to ...\n",
      "Chunk 24: As noted earlier, CNNs primarily focus on the basis that the input will be comprised of images. This...\n",
      "Chunk 25: 4...\n",
      "Chunk 26: Keiron O'Shea et al....\n",
      "Chunk 27: One of the key differences is that the neurons that the layers within the CNN are comprised of neuro...\n",
      "Chunk 28: In practice this would mean that for the example given earlier, the input 'volume' will have a dimen...\n",
      "Chunk 29: CNNs are comprised of three types of layers. These are convolutional layers, pooling layers and full...\n",
      "Chunk 30: Fig. 2: An simple CNN architecture, comprised of just five layers...\n",
      "Chunk 31: The basic functionality of the example CNN above can be broken down into four key areas....\n",
      "Chunk 32: 1. As found in other forms of ANN, the input layer will hold the pixel values of the image.\n",
      "2. The c...\n",
      "Chunk 33: Introduction to Convolutional Neural Networks...\n",
      "Chunk 34: 5...\n",
      "Chunk 35: an 'elementwise' activation function such as sigmoid to the output of the activation produced by the...\n",
      "Chunk 36: Through this simple method of transformation, CNNs are able to transform the original input layer by...\n",
      "Chunk 37: Fig. 3: Activations taken from the first convolutional layer of a simplistic deep CNN, after trainin...\n",
      "Chunk 38: However, it is important to note that simply understanding the overall architecture of a CNN archite...\n",
      "Chunk 39: As the name implies, the convolutional layer plays a vital role in how CNNs operate. The layers para...\n",
      "Chunk 40: 6...\n",
      "Chunk 41: Keiron O’Shea et al....\n",
      "Chunk 42: These kernels are usually small in spatial dimensionality, but spreads along the entirety of the dep...\n",
      "Chunk 43: As we glide through the input, the scalar product is calculated for each value in that kernel. (Figu...\n",
      "Chunk 44: Fig. 4: A visual representation of a convolutional layer. The centre element of the kernel is placed...\n",
      "Chunk 45: Every kernel will have a corresponding activation map, of which will be stacked along the depth dime...\n",
      "Chunk 46: As we alluded to earlier, training ANNs on inputs such as images results in models of which are too ...\n",
      "Chunk 47: For example, if the input to the network is an image of size 64 × 64 × 3 (a RGBcoloured image with a...\n",
      "Chunk 48: Convolutional layers are also able to significantly reduce the complexity of the model through the o...\n",
      "Chunk 49: Introduction to Convolutional Neural Networks...\n",
      "Chunk 50: 7...\n",
      "Chunk 51: The depth of the output volume produced by the convolutional layers can be manually set through the ...\n",
      "Chunk 52: We are also able to define the stride in which we set the depth around the spatial dimensionality of...\n",
      "Chunk 53: Zero-padding is the simple process of padding the border of the input, and is an effective method to...\n",
      "Chunk 54: It is important to understand that through using these techniques, we will alter the spatial dimensi...\n",
      "Chunk 55: ( V - R ) + 2 Z S + 1...\n",
      "Chunk 56: Where V represents the input volume size ( height × width × depth ), R represents the receptive fiel...\n",
      "Chunk 57: Despite our best efforts so far we will still find that our models are still enormous if we use an i...\n",
      "Chunk 58: Parameter sharing works on the assumption that if one region feature is useful to compute at a set s...\n",
      "Chunk 59: As a result of this as the backpropagation stage occurs, each neuron in the output will represent th...\n",
      "Chunk 60: 8...\n",
      "Chunk 61: Keiron O’Shea et al....\n",
      "Chunk 62: Pooling layers aim to gradually reduce the dimensionality of the representation, and thus further re...\n",
      "Chunk 63: The pooling layer operates over each activation map in the input, and scales its dimensionality usin...\n",
      "Chunk 64: Due to the destructive nature of the pooling layer, there are only two generally observed methods of...\n",
      "Chunk 65: It is also important to understand that beyond max-pooling, CNN architectures may contain general-po...\n",
      "Chunk 66: The fully-connected layer contains neurons of which are directly connected to the neurons in the two...\n",
      "Chunk 67: Despite the relatively small number of layers required to form a CNN, there is no set way of formula...\n",
      "Chunk 68: Introduction to Convolutional Neural Networks...\n",
      "Chunk 69: 9...\n",
      "Chunk 70: Another common CNN architecture is to stack two convolutional layers before each pooling layer, as i...\n",
      "Chunk 71: Fig. 5: A common form of CNN architecture in which convolutional layers are stacked between ReLus co...\n",
      "Chunk 72: It is also advised to split large convolutional layers up into many smaller sized convolutional laye...\n",
      "Chunk 73: The input layer should be recursively divisible by two. Common numbers include 32 × 32 , 64 × 64 , 9...\n",
      "Chunk 74: Whilst using small filters, set stride to one and make use of zero-padding as to ensure that the con...\n",
      "Chunk 75: CNNs are extremely powerful machine learning algorithms, however they can be horrendously resource-h...\n",
      "Chunk 76: 10...\n",
      "Chunk 77: Keiron O’Shea et al....\n",
      "Chunk 78: resizing the raw images to something a little less heavy. Alternatively, you can go against everythi...\n",
      "Chunk 79: In addition to the few rules-of-thumb outlined above, it is also important to acknowledge a few 'tri...\n",
      "Chunk 80: Convolutional Neural Networks differ to other forms of Artifical Neural Network in that instead of f...\n",
      "Chunk 81: This paper has outlined the basic concepts of Convolutional Neural Networks, explaining the layers r...\n",
      "Chunk 82: Research in the field of image analysis using neural networks has somewhat slowed in recent times. T...\n",
      "Chunk 83: The authors would like to thank Dr. Chuan Lu and Nicholas Dimonaco for useful discussion and suggest...\n",
      "Chunk 84: 1. Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for image classificati...\n",
      "Chunk 85: Introduction to Convolutional Neural Networks...\n",
      "Chunk 86: 11...\n",
      "Chunk 87: 4. Cires¸an, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural network commi...\n"
     ]
    }
   ],
   "source": [
    "# Import necessary classes from Docling\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling_core.transforms.chunker import HierarchicalChunker\n",
    "\n",
    "# # Step 1: Convert the document from a URL\n",
    "# conv_res = DocumentConverter().convert(\"/content/page_10.png\")\n",
    "\n",
    "# Step 2: Access the converted document\n",
    "doc = conv_result.document\n",
    "\n",
    "# Step 3: Initialize the HierarchicalChunker and chunk the document\n",
    "chunker = HierarchicalChunker()\n",
    "chunks = list(chunker.chunk(doc))\n",
    "\n",
    "# Step 4: Check the number of chunks and print all if they exist\n",
    "num_chunks = len(chunks)\n",
    "if num_chunks > 0:  # Ensure there are chunks available\n",
    "    print(f\"Total chunks available: {num_chunks}\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i}: {chunk.text[:100]}...\")  # Use dot notation to access text\n",
    "else:\n",
    "    print(\"No chunks available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
